import json
import pandas as pd
import os
from collections import defaultdict
from tqdm import tqdm

def load_recbole_interactions(inter_file):
    """
    åŠ è½½ RecBole çš„ .inter æ–‡ä»¶
    """
    print(f"ğŸ“– æ­£åœ¨è¯»å– RecBole äº¤äº’æ–‡ä»¶: {inter_file}")
    
    data = []
    with open(inter_file, 'r', encoding='utf-8') as f:
        # è·³è¿‡è¡¨å¤´
        header = f.readline().strip().split('\t')
        print(f"   è¡¨å¤´: {header}")
        
        # è¯»å–æ•°æ®
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 3:
                data.append({
                    'user_id': parts[0],
                    'item_id': parts[1],
                    'rating': float(parts[2]) if len(parts) > 2 else 1.0,
                    'timestamp': float(parts[3]) if len(parts) > 3 else 0.0
                })
    
    df = pd.DataFrame(data)
    print(f"âœ… è¯»å–äº† {len(df)} æ¡äº¤äº’")
    return df

def split_sequences_by_user(df):
    """
    æŒ‰ç”¨æˆ·åˆ’åˆ†æ•°æ®ï¼Œä½¿ç”¨ leave-one-out ç­–ç•¥
    æ¯ä¸ªç”¨æˆ·çš„äº¤äº’åºåˆ—ï¼š
    - è®­ç»ƒé›†ï¼šæ¯ä¸ªæ—¶é—´ç‚¹çš„å¢é‡åºåˆ—ï¼ˆå†å² -> ä¸‹ä¸€ä¸ªç‰©å“ï¼‰
    - éªŒè¯é›†ï¼šå‰ n-2 ä¸ª -> å€’æ•°ç¬¬2ä¸ªç‰©å“
    - æµ‹è¯•é›†ï¼šå‰ n-1 ä¸ª -> æœ€åä¸€ä¸ªç‰©å“
    """
    print(f"\nğŸ”ª æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
    print(f"   ç­–ç•¥: Leave-one-out (æ¯ä¸ªç”¨æˆ·æœ€å2ä¸ªäº¤äº’ä½œä¸ºéªŒè¯å’Œæµ‹è¯•)")
    
    # æŒ‰ç”¨æˆ·å’Œæ—¶é—´æ’åº
    df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)
    
    train_sequences = []
    valid_sequences = []
    test_sequences = []
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    user_groups = df.groupby('user_id')
    print(f"   ç”¨æˆ·æ•°: {len(user_groups)}")
    
    stats = {
        'total_users': 0,
        'train_users': 0,
        'valid_users': 0,
        'test_users': 0,
        'skipped_users': 0
    }
    
    for user_id, group in tqdm(user_groups, desc="å¤„ç†ç”¨æˆ·"):
        stats['total_users'] += 1
        interactions = group['item_id'].tolist()
        n = len(interactions)
        
        if n < 3:
            # äº¤äº’å¤ªå°‘ï¼ˆå°‘äº3ä¸ªï¼‰ï¼Œè·³è¿‡è¯¥ç”¨æˆ·
            stats['skipped_users'] += 1
            continue
        
        # ============ è®­ç»ƒé›†ï¼šå¢é‡åºåˆ— ============
        # ä»ç¬¬2ä¸ªäº¤äº’å¼€å§‹åˆ°å€’æ•°ç¬¬3ä¸ªäº¤äº’ï¼Œæ¯ä¸ªä½ç½®éƒ½ç”Ÿæˆä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        # ä¾‹å¦‚ï¼š[A, B, C, D, E] -> 
        #   {history: [A], target: B}
        #   {history: [A, B], target: C}
        #   {history: [A, B, C], target: D} (ä¸åŒ…æ‹¬æœ€åä¸¤ä¸ª)
        for i in range(1, n - 2):  # ä»ç´¢å¼•1åˆ°n-3
            train_sequences.append({
                'user_id': user_id,
                'inter_history': interactions[:i],
                'target_id': interactions[i]
            })
        
        if n > 3:  # è‡³å°‘æœ‰4ä¸ªäº¤äº’æ‰æœ‰è®­ç»ƒæ•°æ®
            stats['train_users'] += 1
        
        # ============ éªŒè¯é›† ============
        # ä½¿ç”¨å‰ n-2 ä¸ªä½œä¸ºå†å²ï¼Œå€’æ•°ç¬¬2ä¸ªä½œä¸ºç›®æ ‡
        # ä¾‹å¦‚ï¼š[A, B, C, D, E] -> {history: [A, B, C], target: D}
        valid_sequences.append({
            'user_id': user_id,
            'inter_history': interactions[:-2],
            'target_id': interactions[-2]
        })
        stats['valid_users'] += 1
        
        # ============ æµ‹è¯•é›† ============
        # ä½¿ç”¨å‰ n-1 ä¸ªä½œä¸ºå†å²ï¼Œæœ€åä¸€ä¸ªä½œä¸ºç›®æ ‡
        # ä¾‹å¦‚ï¼š[A, B, C, D, E] -> {history: [A, B, C, D], target: E}
        test_sequences.append({
            'user_id': user_id,
            'inter_history': interactions[:-1],
            'target_id': interactions[-1]
        })
        stats['test_users'] += 1
    
    print(f"\nâœ… æ•°æ®åˆ’åˆ†å®Œæˆ:")
    print(f"   æ€»ç”¨æˆ·æ•°: {stats['total_users']}")
    print(f"   è®­ç»ƒé›†åºåˆ—: {len(train_sequences)} (æ¥è‡ª {stats['train_users']} ä¸ªç”¨æˆ·)")
    print(f"   éªŒè¯é›†åºåˆ—: {len(valid_sequences)} (æ¥è‡ª {stats['valid_users']} ä¸ªç”¨æˆ·)")
    print(f"   æµ‹è¯•é›†åºåˆ—: {len(test_sequences)} (æ¥è‡ª {stats['test_users']} ä¸ªç”¨æˆ·)")
    print(f"   è·³è¿‡ç”¨æˆ·: {stats['skipped_users']} (äº¤äº’å°‘äº3æ¬¡)")
    
    return train_sequences, valid_sequences, test_sequences

def save_jsonl(data, output_file):
    """
    ä¿å­˜ä¸º JSONL æ ¼å¼
    æ ¼å¼ï¼š{"user_id": "xxx", "target_id": "xxx", "inter_history": [...]}
    """
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            # æŒ‰ç…§ä½œè€…çš„æ ¼å¼ï¼šuser_id, target_id, inter_history çš„é¡ºåº
            json_obj = {
                'user_id': item['user_id'],
                'target_id': item['target_id'],
                'inter_history': item['inter_history']
            }
            f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')
    
    print(f"âœ… å·²ä¿å­˜ {len(data)} æ¡è®°å½•")

def load_item2id_mapping(map_file):
    """
    åŠ è½½ item2id æ˜ å°„
    """
    print(f"\nğŸ“– æ­£åœ¨è¯»å– item2id æ˜ å°„: {map_file}")
    
    with open(map_file, 'r', encoding='utf-8') as f:
        item2id = json.load(f)
    
    print(f"âœ… è¯»å–äº† {len(item2id)} ä¸ªç‰©å“çš„æ˜ å°„")
    return item2id

def verify_consistency(train_seqs, valid_seqs, test_seqs, item2id):
    """
    éªŒè¯æ•°æ®ä¸€è‡´æ€§
    """
    print(f"\nğŸ” éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
    
    # æ”¶é›†æ‰€æœ‰å‡ºç°çš„ç‰©å“
    all_items = set()
    for seq in train_seqs + valid_seqs + test_seqs:
        all_items.update(seq['inter_history'])
        all_items.add(seq['target_id'])
    
    print(f"   æ•°æ®ä¸­çš„ç‰©å“æ•°: {len(all_items)}")
    print(f"   æ˜ å°„ä¸­çš„ç‰©å“æ•°: {len(item2id)}")
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç‰©å“éƒ½åœ¨æ˜ å°„ä¸­
    missing_items = all_items - set(item2id.keys())
    if missing_items:
        print(f"âš ï¸  è­¦å‘Š: æœ‰ {len(missing_items)} ä¸ªç‰©å“ä¸åœ¨æ˜ å°„ä¸­")
        print(f"   ç¤ºä¾‹: {list(missing_items)[:5]}")
    else:
        print(f"âœ… æ‰€æœ‰ç‰©å“éƒ½åœ¨æ˜ å°„ä¸­")
    
    return len(missing_items) == 0

def print_statistics(sequences, dataset_name):
    """
    æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    """
    if len(sequences) == 0:
        print(f"\n{dataset_name}:")
        print(f"  åºåˆ—æ•°é‡: 0")
        return
    
    hist_lens = [len(seq['inter_history']) for seq in sequences]
    unique_users = len(set(seq['user_id'] for seq in sequences))
    
    print(f"\n{dataset_name}:")
    print(f"  åºåˆ—æ•°é‡: {len(sequences)}")
    print(f"  å”¯ä¸€ç”¨æˆ·æ•°: {unique_users}")
    print(f"  å¹³å‡å†å²é•¿åº¦: {sum(hist_lens)/len(hist_lens):.2f}")
    print(f"  æœ€å°å†å²é•¿åº¦: {min(hist_lens)}")
    print(f"  æœ€å¤§å†å²é•¿åº¦: {max(hist_lens)}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("=" * 70)
    print("ğŸµ ETEGRec æ•°æ®å‡†å¤‡å·¥å…· - Musical Instruments 2023")
    print("=" * 70)
    print(f"å½“å‰æ—¶é—´: 2025-11-14 08:28:01 UTC")
    print(f"ç”¨æˆ·: YYYYXL1004")
    print("=" * 70)
    
    # ============ é…ç½® ============
    BASE_DIR = './dataset/Instruments2023'
    INTER_FILE = os.path.join(BASE_DIR, 'Instruments2023.inter')
    MAP_FILE = os.path.join(BASE_DIR, 'Instruments2023.emb_map.json')
    OUTPUT_DIR = BASE_DIR
    DATASET_NAME = 'Instruments2023'
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(INTER_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°äº¤äº’æ–‡ä»¶ {INTER_FILE}")
        return
    
    if not os.path.exists(MAP_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ˜ å°„æ–‡ä»¶ {MAP_FILE}")
        print(f"   è¯·å…ˆè¿è¡Œ train_sasrec_instruments.py ç”Ÿæˆæ˜ å°„æ–‡ä»¶")
        return
    
    # ============ æ­¥éª¤ 1: åŠ è½½æ•°æ® ============
    df = load_recbole_interactions(INTER_FILE)
    item2id = load_item2id_mapping(MAP_FILE)
    
    # ============ æ­¥éª¤ 2: åˆ’åˆ†æ•°æ®é›†å¹¶æ„å»ºåºåˆ— ============
    train_sequences, valid_sequences, test_sequences = split_sequences_by_user(df)
    
    # ============ æ­¥éª¤ 3: éªŒè¯ä¸€è‡´æ€§ ============
    verify_consistency(train_sequences, valid_sequences, test_sequences, item2id)
    
    # ============ æ­¥éª¤ 4: ä¿å­˜æ–‡ä»¶ ============
    print("\n" + "=" * 70)
    print("ä¿å­˜ JSONL æ–‡ä»¶...")
    
    train_file = os.path.join(OUTPUT_DIR, f'{DATASET_NAME}.train.jsonl')
    valid_file = os.path.join(OUTPUT_DIR, f'{DATASET_NAME}.valid.jsonl')
    test_file = os.path.join(OUTPUT_DIR, f'{DATASET_NAME}.test.jsonl')
    
    save_jsonl(train_sequences, train_file)
    save_jsonl(valid_sequences, valid_file)
    save_jsonl(test_sequences, test_file)
    
    # ============ æ­¥éª¤ 5: æ˜¾ç¤ºæ ·ä¾‹ ============
    print("\n" + "=" * 70)
    print("ğŸ“Š æ•°æ®æ ·ä¾‹:")
    print("=" * 70)
    
    if len(train_sequences) > 0:
        print("\nè®­ç»ƒé›†æ ·ä¾‹:")
        for i, seq in enumerate(train_sequences[:3]):
            print(f"  æ ·ä¾‹ {i+1}:")
            print(f"    User ID: {seq['user_id']}")
            print(f"    å†å²é•¿åº¦: {len(seq['inter_history'])}")
            hist_display = seq['inter_history'][:5]
            if len(seq['inter_history']) > 5:
                print(f"    å†å²: {hist_display}...")
            else:
                print(f"    å†å²: {seq['inter_history']}")
            print(f"    ç›®æ ‡: {seq['target_id']}")
            # æ˜¾ç¤ºå®Œæ•´çš„ JSON æ ¼å¼
            json_str = json.dumps({
                'user_id': seq['user_id'],
                'target_id': seq['target_id'],
                'inter_history': seq['inter_history'][:3] + (['...'] if len(seq['inter_history']) > 3 else [])
            }, ensure_ascii=False)
            print(f"    JSON: {json_str}")
    
    if len(valid_sequences) > 0:
        print("\néªŒè¯é›†æ ·ä¾‹:")
        for i, seq in enumerate(valid_sequences[:3]):
            print(f"  æ ·ä¾‹ {i+1}:")
            print(f"    User ID: {seq['user_id']}")
            print(f"    å†å²é•¿åº¦: {len(seq['inter_history'])}")
            hist_display = seq['inter_history'][:5]
            if len(seq['inter_history']) > 5:
                print(f"    å†å²: {hist_display}...")
            else:
                print(f"    å†å²: {seq['inter_history']}")
            print(f"    ç›®æ ‡: {seq['target_id']}")
            # æ˜¾ç¤ºå®Œæ•´çš„ JSON æ ¼å¼
            json_str = json.dumps({
                'user_id': seq['user_id'],
                'target_id': seq['target_id'],
                'inter_history': seq['inter_history'][:3] + (['...'] if len(seq['inter_history']) > 3 else [])
            }, ensure_ascii=False)
            print(f"    JSON: {json_str}")
    
    if len(test_sequences) > 0:
        print("\næµ‹è¯•é›†æ ·ä¾‹:")
        for i, seq in enumerate(test_sequences[:3]):
            print(f"  æ ·ä¾‹ {i+1}:")
            print(f"    User ID: {seq['user_id']}")
            print(f"    å†å²é•¿åº¦: {len(seq['inter_history'])}")
            hist_display = seq['inter_history'][:5]
            if len(seq['inter_history']) > 5:
                print(f"    å†å²: {hist_display}...")
            else:
                print(f"    å†å²: {seq['inter_history']}")
            print(f"    ç›®æ ‡: {seq['target_id']}")
            # æ˜¾ç¤ºå®Œæ•´çš„ JSON æ ¼å¼
            json_str = json.dumps({
                'user_id': seq['user_id'],
                'target_id': seq['target_id'],
                'inter_history': seq['inter_history'][:3] + (['...'] if len(seq['inter_history']) > 3 else [])
            }, ensure_ascii=False)
            print(f"    JSON: {json_str}")
    
    # ============ æ­¥éª¤ 6: ç»Ÿè®¡ä¿¡æ¯ ============
    print("\n" + "=" * 70)
    print("ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print("=" * 70)
    
    print_statistics(train_sequences, "è®­ç»ƒé›†")
    print_statistics(valid_sequences, "éªŒè¯é›†")
    print_statistics(test_sequences, "æµ‹è¯•é›†")
    
    # ============ æ­¥éª¤ 7: éªŒè¯æ–‡ä»¶æ ¼å¼ ============
    print("\n" + "=" * 70)
    print("ğŸ” éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶æ ¼å¼...")
    print("=" * 70)
    
    # è¯»å–ç¬¬ä¸€è¡ŒéªŒè¯
    for name, file_path in [('è®­ç»ƒé›†', train_file), ('éªŒè¯é›†', valid_file), ('æµ‹è¯•é›†', test_file)]:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                if first_line:
                    obj = json.loads(first_line)
                    print(f"\n{name}ç¬¬ä¸€è¡Œ:")
                    print(f"  é”®: {list(obj.keys())}")
                    print(f"  å®Œæ•´å†…å®¹: {json.dumps(obj, ensure_ascii=False)[:200]}...")
    
    # ============ æ€»ç»“ ============
    print("\n" + "=" * 70)
    print("ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆ!")
    print("=" * 70)
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   1. {train_file}")
    print(f"      - åºåˆ—æ•°: {len(train_sequences)}")
    print(f"   2. {valid_file}")
    print(f"      - åºåˆ—æ•°: {len(valid_sequences)}")
    print(f"   3. {test_file}")
    print(f"      - åºåˆ—æ•°: {len(test_sequences)}")
    
    print(f"\nğŸ“ å·²æœ‰çš„æ–‡ä»¶:")
    print(f"   4. {MAP_FILE}")
    print(f"   5. {os.path.join(OUTPUT_DIR, f'{DATASET_NAME}_emb_256.npy')}")
    
    print(f"\nâœ¨ ä¸‹ä¸€æ­¥: è®­ç»ƒ ETEGRec!")
    print(f"\n1. åˆ›å»ºé…ç½®æ–‡ä»¶ config/instruments.yaml")
    print(f"2. ä¿®æ”¹ run.sh ä¸­çš„ DATASET=Instruments2023")
    print(f"3. è¿è¡Œ: bash run.sh")
    
    print("\n" + "=" * 70)

if __name__ == '__main__':
    main()