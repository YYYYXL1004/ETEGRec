"""
ä½¿ç”¨ç»Ÿä¸€åˆ’åˆ†ç”Ÿæˆ ETEGRec è®­ç»ƒæ•°æ®
å…³é”®ï¼šä½¿ç”¨æ­¥éª¤1ç”Ÿæˆçš„ split æ ‡ç­¾ï¼Œç¡®ä¿ä¸ SASRec å®Œå…¨ä¸€è‡´
"""

import json
import pandas as pd
import os
from collections import defaultdict
from tqdm import tqdm

def load_unified_data(inter_file):
    """åŠ è½½å¸¦ split æ ‡ç­¾çš„æ•°æ®"""
    print(f"ğŸ“– æ­£åœ¨è¯»å–ç»Ÿä¸€æ ¼å¼æ•°æ®: {inter_file}")
    
    df = pd.read_csv(inter_file, sep='\t', dtype=str, keep_default_na=False)
    
    # è§„èŒƒåŒ–åˆ—åï¼ˆå– ':' å‰é¢çš„éƒ¨åˆ†ï¼‰
    new_cols = []
    for c in df.columns.tolist():
        if isinstance(c, str) and ':' in c:
            new_cols.append(c.split(':')[0])
        else:
            new_cols.append(c)
    df.columns = new_cols
    
    print(f"âœ… è¯»å–äº† {len(df)} æ¡äº¤äº’")
    print(f"   è®­ç»ƒé›†: {len(df[df['split'] == 'train']):,} æ¡")
    print(f"   éªŒè¯é›†: {len(df[df['split'] == 'valid']):,} æ¡")
    print(f"   æµ‹è¯•é›†: {len(df[df['split'] == 'test']):,} æ¡")
    
    return df

def build_sequences_from_split(df, max_seq_length=50):
    """
    ğŸ”‘ æ ¸å¿ƒï¼šæ ¹æ® split æ ‡ç­¾æ„å»ºåºåˆ—
    ç¡®ä¿ä¸ SASRec çš„åˆ’åˆ†å®Œå…¨ä¸€è‡´
    """
    print(f"\nğŸ”¨ æ­£åœ¨æ„å»ºåºåˆ—...")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_length}")
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    df = df.sort_values(['user_id', 'timestamp'])
    user_groups = df.groupby('user_id')
    
    train_sequences = []
    valid_sequences = []
    test_sequences = []
    
    stats = {'truncated': 0}
    
    for user_id, group in tqdm(user_groups, desc="æ„å»ºåºåˆ—"):
        interactions = group['item_id'].tolist()
        splits = group['split'].tolist()
        n = len(interactions)
        
        if n < 3:
            continue
        
        # æ‰¾åˆ° valid å’Œ test çš„ä½ç½®
        valid_idx = next((i for i, s in enumerate(splits) if s == 'valid'), None)
        test_idx = next((i for i, s in enumerate(splits) if s == 'test'), None)
        
        if valid_idx is None or test_idx is None:
            continue
        
        # ============ è®­ç»ƒé›†ï¼šæ‰€æœ‰ train æ ‡è®°çš„äº¤äº’ ============
        # æ„å»ºå¢é‡åºåˆ—
        for i in range(1, valid_idx):
            if splits[i] == 'train' or i < valid_idx:
                history = interactions[:i]
                if len(history) > max_seq_length:
                    history = history[-max_seq_length:]
                    stats['truncated'] += 1
                
                train_sequences.append({
                    'user_id': user_id,
                    'inter_history': history,
                    'target_id': interactions[i]
                })
        
        # ============ éªŒè¯é›†ï¼švalid æ ‡è®°çš„äº¤äº’ ============
        valid_history = interactions[:valid_idx]
        if len(valid_history) > max_seq_length:
            valid_history = valid_history[-max_seq_length:]
            stats['truncated'] += 1
        
        valid_sequences.append({
            'user_id': user_id,
            'inter_history': valid_history,
            'target_id': interactions[valid_idx]
        })
        
        # ============ æµ‹è¯•é›†ï¼štest æ ‡è®°çš„äº¤äº’ ============
        test_history = interactions[:test_idx]
        if len(test_history) > max_seq_length:
            test_history = test_history[-max_seq_length:]
            stats['truncated'] += 1
        
        test_sequences.append({
            'user_id': user_id,
            'inter_history': test_history,
            'target_id': interactions[test_idx]
        })
    
    print(f"\nâœ… åºåˆ—æ„å»ºå®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_sequences):,} æ¡åºåˆ—")
    print(f"   éªŒè¯é›†: {len(valid_sequences):,} æ¡åºåˆ—")
    print(f"   æµ‹è¯•é›†: {len(test_sequences):,} æ¡åºåˆ—")
    print(f"   æˆªæ–­åºåˆ—: {stats['truncated']:,} æ¡")
    
    return train_sequences, valid_sequences, test_sequences

def save_jsonl(data, output_file):
    """ä¿å­˜ä¸º JSONL æ ¼å¼"""
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            json_obj = {
                'user_id': item['user_id'],
                'target_id': item['target_id'],
                'inter_history': item['inter_history']
            }
            f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')
    
    print(f"âœ… å·²ä¿å­˜ {len(data)} æ¡è®°å½•")

def verify_consistency(train_seqs, valid_seqs, test_seqs):
    """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
    print(f"\nğŸ” éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
    
    # æ£€æŸ¥ç”¨æˆ·é‡å 
    train_users = set(s['user_id'] for s in train_seqs)
    valid_users = set(s['user_id'] for s in valid_seqs)
    test_users = set(s['user_id'] for s in test_seqs)
    
    print(f"\nç”¨æˆ·åˆ†å¸ƒ:")
    print(f"   è®­ç»ƒé›†å”¯ä¸€ç”¨æˆ·: {len(train_users):,}")
    print(f"   éªŒè¯é›†å”¯ä¸€ç”¨æˆ·: {len(valid_users):,}")
    print(f"   æµ‹è¯•é›†å”¯ä¸€ç”¨æˆ·: {len(test_users):,}")
    print(f"   éªŒè¯âˆ©æµ‹è¯•: {len(valid_users & test_users):,} ({len(valid_users & test_users)/len(valid_users)*100:.1f}%)")
    
    # æ£€æŸ¥åºåˆ—é•¿åº¦
    train_lens = [len(s['inter_history']) for s in train_seqs]
    valid_lens = [len(s['inter_history']) for s in valid_seqs]
    test_lens = [len(s['inter_history']) for s in test_seqs]
    
    print(f"\nåºåˆ—é•¿åº¦:")
    print(f"   è®­ç»ƒé›†æœ€å¤§: {max(train_lens)}")
    print(f"   éªŒè¯é›†æœ€å¤§: {max(valid_lens)}")
    print(f"   æµ‹è¯•é›†æœ€å¤§: {max(test_lens)}")
    
    if max(train_lens) <= 50 and max(valid_lens) <= 50 and max(test_lens) <= 50:
        print(f"   âœ… æ‰€æœ‰åºåˆ—é•¿åº¦ â‰¤ 50")
    else:
        print(f"   âŒ å‘ç°è¶…é•¿åºåˆ—ï¼")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸµ ETEGRec æ•°æ®å‡†å¤‡ - ä½¿ç”¨ç»Ÿä¸€åˆ’åˆ†")
    print("=" * 70)
    
    # é…ç½®
    BASE_DIR = './dataset/Instruments2023'
    INTER_FILE = os.path.join(BASE_DIR, 'Instruments2023.inter')
    OUTPUT_DIR = BASE_DIR
    DATASET_NAME = 'Instruments2023'
    MAX_SEQ_LENGTH = 50
    
    if not os.path.exists(INTER_FILE):
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {INTER_FILE}")
        print(f"   è¯·å…ˆè¿è¡Œ prepare_amazon_data_unified.py")
        return
    
    # æ­¥éª¤ 1: åŠ è½½ç»Ÿä¸€æ ¼å¼æ•°æ®
    df = load_unified_data(INTER_FILE)
    
    # æ­¥éª¤ 2: æ ¹æ® split æ ‡ç­¾æ„å»ºåºåˆ—
    train_seqs, valid_seqs, test_seqs = build_sequences_from_split(df, MAX_SEQ_LENGTH)
    
    # æ­¥éª¤ 3: éªŒè¯ä¸€è‡´æ€§
    verify_consistency(train_seqs, valid_seqs, test_seqs)
    
    # æ­¥éª¤ 4: ä¿å­˜æ–‡ä»¶
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶...")
    print(f"{'='*70}")
    
    train_file = os.path.join(OUTPUT_DIR, f'{DATASET_NAME}.train.jsonl')
    valid_file = os.path.join(OUTPUT_DIR, f'{DATASET_NAME}.valid.jsonl')
    test_file = os.path.join(OUTPUT_DIR, f'{DATASET_NAME}.test.jsonl')
    
    save_jsonl(train_seqs, train_file)
    save_jsonl(valid_seqs, valid_file)
    save_jsonl(test_seqs, test_file)
    
    # æ€»ç»“
    print(f"\n{'='*70}")
    print(f"ğŸ‰ ETEGRec æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print(f"{'='*70}")
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   1. {train_file}")
    print(f"   2. {valid_file}")
    print(f"   3. {test_file}")
    
    print(f"\nâœ… æ•°æ®åˆ’åˆ†ä¸ SASRec å®Œå…¨ä¸€è‡´ï¼")
    print(f"   âœ… ä½¿ç”¨ç›¸åŒçš„ split æ ‡ç­¾")
    print(f"   âœ… åºåˆ—é•¿åº¦é™åˆ¶ä¸º {MAX_SEQ_LENGTH}")
    print(f"   âœ… æ— æ•°æ®æ³„éœ²")
    
    print(f"\nâœ¨ ä¸‹ä¸€æ­¥: è®­ç»ƒ ETEGRec")
    print(f"   bash run.sh")
    print("=" * 70)

if __name__ == '__main__':
    main()