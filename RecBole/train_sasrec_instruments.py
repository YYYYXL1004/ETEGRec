from recbole.quick_start import run_recbole
from recbole.config import Config
from recbole.data import create_dataset, data_preparation
from recbole.model.sequential_recommender import SASRec
from recbole.trainer import Trainer
import torch
import numpy as np
import json
import os

def train_and_extract_embeddings():
    """
    è®­ç»ƒ SASRec å¹¶æå–ç‰©å“åµŒå…¥
    """
    print("=" * 70)
    print("ğŸµ SASRec è®­ç»ƒ - Musical Instruments 2023 (ä¼˜åŒ–ç‰ˆ)")
    print("=" * 70)
    
    # ============ é…ç½® ============
    config_dict = {
        # æ•°æ®é›†é…ç½®
        'model': 'SASRec',
        'dataset': 'Instruments2023',
        'data_path': './dataset/',
        'USER_ID_FIELD': 'user_id',
        'ITEM_ID_FIELD': 'item_id',
        'RATING_FIELD': 'rating',
        'TIME_FIELD': 'timestamp',
        'load_col': {
            'inter': ['user_id', 'item_id', 'rating', 'timestamp']
        },
        
        # æ•°æ®åˆ’åˆ†ç­–ç•¥
        'eval_args': {
            'split': {'LS': 'valid_and_test'},
            'order': 'TO',
            'group_by': 'user',
            'mode': 'full'
        },
        
        # ğŸ”§ SASRec æ¨¡å‹å‚æ•°ï¼ˆä¸ä½œè€…å¯¹é½ï¼‰
        'hidden_size': 256,          # åµŒå…¥ç»´åº¦
        'inner_size': 256,
        'n_layers': 2,
        'n_heads': 2,
        'hidden_dropout_prob': 0.5,
        'attn_dropout_prob': 0.5,
        'hidden_act': 'gelu',
        'layer_norm_eps': 1e-12,
        'initializer_range': 0.02,
        'loss_type': 'CE',
        'max_seq_length': 50,        # ğŸ”§ é™åˆ¶ä¸º50ï¼ˆä¸ä½œè€…ä¸€è‡´ï¼‰
        
        # ğŸ”§ ä¿®å¤ï¼šç¦ç”¨è´Ÿé‡‡æ ·
        'train_neg_sample_args': None,
        
        # è®­ç»ƒå‚æ•°
        'epochs': 300,
        'train_batch_size': 2048,
        'eval_batch_size': 2048,
        'learner': 'adam',
        'learning_rate': 0.001,
        
        # è¯„ä¼°å‚æ•°
        'eval_step': 1,
        'stopping_step': 10,
        'metrics': ['Recall', 'NDCG', 'Hit', 'MRR'],
        'topk': [5, 10, 20],
        'valid_metric': 'NDCG@10',
        'metric_decimal_place': 4,
        
        # GPU é…ç½®
        'gpu_id': '3',
        'use_gpu': True,
        
        # ä¿å­˜é…ç½®
        'checkpoint_dir': './saved/SASRec',
        'show_progress': True,
    }
    
    try:
        # ============ åŠ è½½æ•°æ® ============
        print("\nğŸ”§ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
        config = Config(model='SASRec', dataset='Instruments2023', config_dict=config_dict)
        dataset = create_dataset(config)
        
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
        print(f"   ç”¨æˆ·æ•°: {dataset.user_num:,}")
        print(f"   ç‰©å“æ•°: {dataset.item_num:,}")
        print(f"   äº¤äº’æ•°: {dataset.inter_num:,}")
        
        train_data, valid_data, test_data = data_preparation(config, dataset)
        
        # ============ åˆ›å»ºæ¨¡å‹ ============
        print("\nğŸ¤– æ­£åœ¨åˆ›å»º SASRec æ¨¡å‹...")
        model = SASRec(config, train_data.dataset).to(config['device'])
        print(f"   è®¾å¤‡: {config['device']}")
        print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # ============ è®­ç»ƒ ============
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"   æ€»è½®æ•°: {config['epochs']}")
        print(f"   Batch Size: {config['train_batch_size']}")
        print(f"   å­¦ä¹ ç‡: {config['learning_rate']}")
        print(f"   æ—©åœæ­¥æ•°: {config['stopping_step']}")
        print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {config['max_seq_length']}")
        
        trainer = Trainer(config, model)
        best_valid_score, best_valid_result = trainer.fit(
            train_data, valid_data,
            saved=True,
            show_progress=config['show_progress']
        )
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³éªŒè¯ {config['valid_metric']}: {best_valid_score:.4f}")
        
        # ============ æµ‹è¯• ============
        print("\nğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        test_result = trainer.evaluate(test_data, load_best_model=True, show_progress=True)
        print(f"   æµ‹è¯•ç»“æœ:")
        for metric, value in test_result.items():
            print(f"      {metric}: {value:.4f}")
        
        # ============ æå–åµŒå…¥ ============
        print("\nğŸ’¾ æ­£åœ¨æå–ç‰©å“åµŒå…¥...")
        
        # è·å–è®­ç»ƒå¥½çš„ item embedding
        item_embedding = model.item_embedding.weight.data.cpu().numpy()
        print(f"   åŸå§‹åµŒå…¥å½¢çŠ¶: {item_embedding.shape}")
        
        # å»æ‰ padding token (ç´¢å¼• 0)
        item_embedding_no_pad = item_embedding[1:]
        print(f"   å»é™¤ padding å: {item_embedding_no_pad.shape}")
        
        # ============ ä¿å­˜æ–‡ä»¶ ============
        output_dir = './dataset/Instruments2023'
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ .npy åµŒå…¥æ–‡ä»¶
        npy_path = os.path.join(output_dir, 'Instruments2023_emb_256.npy')
        np.save(npy_path, item_embedding_no_pad)
        print(f"\nâœ… åµŒå…¥æ–‡ä»¶å·²ä¿å­˜: {npy_path}")
        print(f"   å½¢çŠ¶: {item_embedding_no_pad.shape}")
        print(f"   å¤§å°: {item_embedding_no_pad.nbytes / 1024 / 1024:.2f} MB")
        
        # 2. ç”Ÿæˆ item2id æ˜ å°„ (ETEGRec æ ¼å¼)
        # ğŸ”§ ä¸ä½œè€…æ ¼å¼ä¸€è‡´ï¼šåŒ…å« [PAD] token
        item_token2id = dataset.field2token_id['item_id']
        
        # åˆ›å»ºæ˜ å°„ï¼ˆåŒ…å« [PAD]ï¼‰
        item2id_etegrec = {}
        item2id_etegrec['[PAD]'] = 0  # ğŸ”§ æ·»åŠ  PAD token
        
        for token, idx in item_token2id.items():
            if idx > 0:  # è·³è¿‡ RecBole çš„ padding (idx=0)
                item2id_etegrec[str(token)] = int(idx)
        
        # ä¿å­˜ä¸º .emb_map.json
        map_path = os.path.join(output_dir, 'Instruments2023.emb_map.json')
        with open(map_path, 'w') as f:
            json.dump(item2id_etegrec, f, indent=2)
        print(f"âœ… Item2ID æ˜ å°„å·²ä¿å­˜: {map_path}")
        print(f"   æ˜ å°„æ¡ç›®æ•°: {len(item2id_etegrec)} (åŒ…å« [PAD])")
        print(f"   ç‰©å“æ•°: {len(item2id_etegrec) - 1} (ä¸å« [PAD])")
        
        # ============ éªŒè¯ ============
        print("\nğŸ” éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶...")
        
        # éªŒè¯åµŒå…¥æ–‡ä»¶
        loaded_emb = np.load(npy_path)
        assert loaded_emb.shape == item_embedding_no_pad.shape, "åµŒå…¥å½¢çŠ¶ä¸åŒ¹é…!"
        
        # éªŒè¯æ˜ å°„æ–‡ä»¶
        with open(map_path, 'r') as f:
            loaded_map = json.load(f)
        
        # ğŸ”§ æ˜ å°„æ•°é‡åº”è¯¥æ˜¯åµŒå…¥æ•°é‡ + 1 ([PAD])
        expected_map_size = loaded_emb.shape[0] + 1
        assert len(loaded_map) == expected_map_size, \
            f"æ˜ å°„æ•°é‡ ({len(loaded_map)}) åº”è¯¥æ˜¯ {expected_map_size} (åµŒå…¥æ•° + PAD)!"
        
        assert '[PAD]' in loaded_map and loaded_map['[PAD]'] == 0, \
            "æ˜ å°„åº”åŒ…å« [PAD] tokenï¼Œä¸”ç´¢å¼•ä¸º 0!"
        
        print("âœ… æ‰€æœ‰éªŒè¯é€šè¿‡!")
        print(f"   æ˜ å°„æ ¼å¼: {{'[PAD]': 0, ...}}")
        print(f"   æ˜ å°„æ•°é‡ä¸åµŒå…¥åŒ¹é…")
        
        # ============ æ€»ç»“ ============
        print("\n" + "=" * 70)
        print("ğŸ‰ è®­ç»ƒå’Œæå–å®Œæˆ!")
        print("=" * 70)
        print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   1. {npy_path}")
        print(f"      - å½¢çŠ¶: {loaded_emb.shape}")
        print(f"      - ç”¨é€”: ETEGRec çš„ semantic_emb_path")
        print(f"\n   2. {map_path}")
        print(f"      - æ¡ç›®æ•°: {len(loaded_map)} (å« [PAD])")
        print(f"      - ç‰©å“æ•°: {len(loaded_map) - 1}")
        print(f"      - ç”¨é€”: ETEGRec çš„ map_path")
        
        print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½:")
        print(f"   éªŒè¯é›† {config['valid_metric']}: {best_valid_score:.4f}")
        for metric, value in test_result.items():
            print(f"   æµ‹è¯•é›† {metric}: {value:.4f}")
        
        print(f"\nâœ¨ ä¸ä½œè€…æ•°æ®é›†å¯¹é½:")
        print(f"   âœ… æœ€å¤§åºåˆ—é•¿åº¦: {config['max_seq_length']}")
        print(f"   âœ… æ˜ å°„åŒ…å« [PAD] token")
        print(f"   âœ… åµŒå…¥ç»´åº¦: 256")
        
        print("\n" + "=" * 70)
        
        return model, dataset, item_embedding_no_pad, test_result
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

if __name__ == '__main__':
    model, dataset, embeddings, test_result = train_and_extract_embeddings()
    
    if model is not None:
        print("\nâœ¨ ä¸‹ä¸€æ­¥: å‡†å¤‡ ETEGRec çš„è®­ç»ƒæ•°æ®!")
        print("   è¿è¡Œ: python prepare_etegrec_data.py")