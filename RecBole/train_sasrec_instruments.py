#!/usr/bin/env python3
"""
train_sasrec_unified.py

æ›´ç¨³å¥çš„ SASRec è®­ç»ƒè„šæœ¬ï¼ˆä½¿ç”¨ç»Ÿä¸€ split æ ‡ç­¾ï¼‰ï¼Œä¿®å¤åˆ—åè§£æã€è¯„ä¼°è®¾ç½®é—®é¢˜ï¼Œ
å¹¶åœ¨è®­ç»ƒå‰åšé¢å¤–çš„æ£€æŸ¥ä»¥é¿å…è¯„ä¼°é˜¶æ®µè¿”å› None çš„æƒ…å†µã€‚
"""
import os
import json
import numpy as np
import pandas as pd
from recbole.config import Config
from recbole.data import create_dataset, data_preparation
from recbole.model.sequential_recommender import SASRec
from recbole.trainer import Trainer

def read_inter_file_normalized(inter_file):
    """
    è¯»å– .inter æ–‡ä»¶å¹¶è§„èŒƒåŒ–åˆ—åï¼š
    å°† 'user_id:token' -> 'user_id'ï¼Œ'split:token' -> 'split' ç­‰ã€‚
    è¿”å› pandas.DataFrame
    """
    print(f"ğŸ“– è¯»å–äº¤äº’æ–‡ä»¶: {inter_file}")
    # ä½¿ç”¨ header=0 è¯»å–ï¼Œä¿ç•™åŸå§‹åˆ—å
    df = pd.read_csv(inter_file, sep='\t', header=0, dtype=str, keep_default_na=False)
    # è§„èŒƒåŒ–åˆ—åï¼ˆå– ':' å‰é¢çš„éƒ¨åˆ†ï¼‰
    new_cols = []
    for c in df.columns.tolist():
        if isinstance(c, str) and ':' in c:
            new_cols.append(c.split(':')[0])
        else:
            new_cols.append(c)
    df.columns = new_cols
    # æŠŠç©ºå­—ç¬¦ä¸²è½¬ä¸º NaN ä»¥åˆ©äºåç»­ç±»å‹è½¬æ¢
    df = df.replace({'': pd.NA})
    return df

def quick_checks_df(df):
    # å¿…éœ€åˆ—æ£€æŸ¥
    for col in ['user_id', 'item_id', 'rating', 'timestamp', 'split']:
        if col not in df.columns:
            raise KeyError(f"ç¼ºå°‘å¿…éœ€åˆ—: {col}ã€‚è¯·ç¡®è®¤ Instruments2023.inter åŒ…å«è¯¥åˆ—ï¼ˆå¯èƒ½åä¸º 'split:token'ï¼‰")
    # æ£€æŸ¥ split å–å€¼
    uniques = set(df['split'].dropna().unique())
    if not {'train','valid','test'}.issubset({u.lower() for u in uniques}):
        raise ValueError(f"split åˆ—å€¼åº”åŒ…å« 'train','valid','test' ä¸‰ç±»ï¼Œç›®å‰å‘ç°: {sorted(list(uniques))}")
    # æ£€æŸ¥ timestamp/rating ç±»å‹å¯è½¬ä¸ºæ•°å€¼
    df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')
    df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
    if df['timestamp'].isna().any():
        raise ValueError("timestamp åˆ—åŒ…å«æ— æ³•è§£æä¸ºæ•°å€¼çš„å€¼ï¼Œè¯·æ£€æŸ¥ .inter æ–‡ä»¶ä¸­çš„æ—¶é—´æˆ³")
    if df['rating'].isna().any():
        # å…è®¸ç¼ºå¤± ratingï¼ˆå¯é»˜è®¤ä¸º1.0ï¼‰ï¼Œä½†æé†’ç”¨æˆ·
        print("âš ï¸ warning: rating åˆ—åŒ…å«æ— æ³•è§£æä¸ºæ•°å€¼çš„å€¼ï¼Œä¼šç”¨ 1.0 å¡«å……")
        df['rating'] = df['rating'].fillna(1.0)
    return df

def save_split_files(df, output_dir):
    """
    ä¿å­˜ä¸‰ä¸ªåˆ†å‰²æ–‡ä»¶ï¼ˆRecBole å¯è¯»ï¼‰ï¼Œå¹¶è¿”å›æ¯ä¸ªæ–‡ä»¶è·¯å¾„
    """
    train_df = df[df['split'] == 'train'][['user_id','item_id','rating','timestamp']]
    valid_df = df[df['split'] == 'valid'][['user_id','item_id','rating','timestamp']]
    test_df  = df[df['split'] == 'test'][['user_id','item_id','rating','timestamp']]

    # å†™æ–‡ä»¶ï¼Œheader éœ€è¦å¸¦ recbole çš„åˆ—ç±»å‹æ ‡æ³¨
    train_file = os.path.join(output_dir, 'Instruments2023_train.inter')
    valid_file = os.path.join(output_dir, 'Instruments2023_valid.inter')
    test_file  = os.path.join(output_dir, 'Instruments2023_test.inter')

    train_df.to_csv(train_file, sep='\t', index=False,
                    header=['user_id:token','item_id:token','rating:float','timestamp:float'])
    valid_df.to_csv(valid_file, sep='\t', index=False,
                    header=['user_id:token','item_id:token','rating:float','timestamp:float'])
    test_df.to_csv(test_file, sep='\t', index=False,
                    header=['user_id:token','item_id:token','rating:float','timestamp:float'])

    print(f"âœ… å·²ä¿å­˜åˆ†å‰²æ–‡ä»¶: {train_file}, {valid_file}, {test_file}")
    return train_file, valid_file, test_file

def train_and_extract_embeddings():
    print("=" * 70)
    print("ğŸµ SASRec è®­ç»ƒ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åˆ’åˆ†ï¼ˆç¨³å¥ç‰ˆï¼‰")
    print("=" * 70)

    config_dict = {
        'model': 'SASRec',
        'dataset': 'Instruments2023',
        'data_path': './dataset/',
        'USER_ID_FIELD': 'user_id',
        'ITEM_ID_FIELD': 'item_id',
        'RATING_FIELD': 'rating',
        'TIME_FIELD': 'timestamp',
        'load_col': {
            'inter': ['user_id', 'item_id', 'rating', 'timestamp']
        },
        # ä½¿ç”¨ Leave-one-out splitï¼ˆLSï¼‰ï¼Œä¸æˆ‘ä»¬æŒ‰ç”¨æˆ·æœ€åä¸¤ä¸ªäº¤äº’åˆ’åˆ†ä¸€è‡´ã€‚
        'eval_args': {
            'split': {'LS': 'valid_and_test'},
            'order': 'TO',
            'group_by': 'user',
            'mode': 'full'   # ä½¿ç”¨ full æ¨¡å¼ï¼ˆä¸ä¾èµ–å¤–éƒ¨è´Ÿé‡‡æ ·è¡¨ï¼‰
        },
        'hidden_size': 256,
        'inner_size': 256,
        'n_layers': 2,
        'n_heads': 2,
        'hidden_dropout_prob': 0.5,
        'attn_dropout_prob': 0.5,
        'hidden_act': 'gelu',
        'loss_type': 'CE',
        'max_seq_length': 50,
        'train_neg_sample_args': None,
        'epochs': 200,
        'train_batch_size': 2048,
        'eval_batch_size': 2048,
        'learner': 'adam',
        'learning_rate': 0.001,
        'eval_step': 1,
        'stopping_step': 10,
        'metrics': ['Recall', 'NDCG', 'Hit', 'MRR'],
        'topk': [5, 10, 20],
        'valid_metric': 'NDCG@10',
        'gpu_id': '0',
        'use_gpu': True,
        'checkpoint_dir': './saved/SASRec_unified',
        'show_progress': True,
    }

    try:
        inter_file = './dataset/Instruments2023/Instruments2023.inter'
        if not os.path.exists(inter_file):
            raise FileNotFoundError(f"{inter_file} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ prepare_amazon_data_unified.py")

        # è¯»å–å¹¶è§„èŒƒåŒ–åˆ—å
        df = read_inter_file_normalized(inter_file)

        # quick checks: ensure columns and types are OK
        df = quick_checks_df(df)

        # æŠŠ split åˆ—å€¼æ ‡å‡†åŒ–å°å†™å¹¶å»é™¤ç©ºç™½
        df['split'] = df['split'].astype(str).str.strip().str.lower()

        # æ‰“å°åˆ†å¸ƒï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        print(f"split å€¼åˆ†å¸ƒ:\n{df['split'].value_counts()}")

        # ä¿å­˜ä¸º RecBole å¯è¯»çš„åˆ†å‰²æ–‡ä»¶ï¼ˆRecBole ä¼šåŸºäº dataset name å»è¯»å–ï¼‰
        output_dir = './dataset/Instruments2023'
        os.makedirs(output_dir, exist_ok=True)
        train_file, valid_file, test_file = save_split_files(df, output_dir)

        # ä½¿ç”¨ RecBole çš„é…ç½®åŠ è½½æ•°æ®
        # RecBole ä¼šåœ¨ dataset/Instruments2023/ ä¸‹æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        config = Config(model='SASRec', dataset='Instruments2023', config_dict=config_dict)
        dataset = create_dataset(config)
        train_data, valid_data, test_data = data_preparation(config, dataset)

        # quick runtime checks: ensure dataloaders non-empty
        if len(train_data) == 0:
            raise RuntimeError("è®­ç»ƒ DataLoader ä¸ºç©ºï¼æ£€æŸ¥ Instruments2023_train.inter æ˜¯å¦æ­£ç¡®")
        if len(valid_data) == 0:
            raise RuntimeError("éªŒè¯ DataLoader ä¸ºç©ºï¼æ£€æŸ¥ Instruments2023_valid.inter æ˜¯å¦æ­£ç¡®")
        if len(test_data) == 0:
            raise RuntimeError("æµ‹è¯• DataLoader ä¸ºç©ºï¼æ£€æŸ¥ Instruments2023_test.inter æ˜¯å¦æ­£ç¡®")

        # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        model = SASRec(config, train_data.dataset).to(config['device'])
        trainer = Trainer(config, model)
        best_valid_score, best_valid_result = trainer.fit(train_data, valid_data, saved=True, show_progress=config['show_progress'])

        print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯ NDCG@10: {best_valid_score:.4f}")

        # è¯„ä¼°
        test_result = trainer.evaluate(test_data, load_best_model=True, show_progress=True)
        print("\nğŸ“Š æµ‹è¯•ç»“æœ:")
        for metric, value in test_result.items():
            print(f"   {metric}: {value:.4f}")

        # æå–å¹¶ä¿å­˜åµŒå…¥
        print("\nğŸ’¾ æ­£åœ¨æå–ç‰©å“åµŒå…¥...")
        item_embedding = model.item_embedding.weight.data.cpu().numpy()
        item_embedding_no_pad = item_embedding[1:]
        npy_path = os.path.join(output_dir, 'Instruments2023_emb_256.npy')
        np.save(npy_path, item_embedding_no_pad)
        print(f"âœ… åµŒå…¥æ–‡ä»¶å·²ä¿å­˜: {npy_path} (shape={item_embedding_no_pad.shape})")

        # ä¿å­˜æ˜ å°„ï¼ˆåŒ…å« [PAD]ï¼‰
        item_token2id = dataset.field2token_id['item_id']
        item2id_etegrec = {'[PAD]': 0}
        for token, idx in item_token2id.items():
            if idx > 0:
                item2id_etegrec[str(token)] = int(idx)
        map_path = os.path.join(output_dir, 'Instruments2023.emb_map.json')
        with open(map_path, 'w', encoding='utf-8') as f:
            json.dump(item2id_etegrec, f, indent=2, ensure_ascii=False)
        print(f"âœ… Item2ID æ˜ å°„å·²ä¿å­˜: {map_path} (å« [PAD])")

        # Validate mapping size vs embeddings
        loaded_emb = np.load(npy_path)
        with open(map_path, 'r', encoding='utf-8') as f:
            loaded_map = json.load(f)
        expected_map_size = loaded_emb.shape[0] + 1
        if len(loaded_map) != expected_map_size:
            raise AssertionError(f"æ˜ å°„æ•°é‡ ({len(loaded_map)}) != åµŒå…¥æ•°é‡+1 ({expected_map_size})")

        print("âœ… æ˜ å°„æ•°é‡ä¸åµŒå…¥æ•°é‡ä¸€è‡´ (å« [PAD])")
        return model, dataset, item_embedding_no_pad, test_result

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback; traceback.print_exc()
        return None, None, None, None

if __name__ == '__main__':
    train_and_extract_embeddings()