#!/usr/bin/env python3
"""
å¤šæ¨¡æ€æ•°æ®å‡†å¤‡è„šæœ¬ - åœ¨ prepare_data_2018.py åŸºç¡€ä¸Šå¢åŠ å›¾ç‰‡è¿‡æ»¤

ä¸ prepare_data_2018.py çš„åŒºåˆ«:
    1. å®é™…ä¸‹è½½å›¾ç‰‡å¹¶éªŒè¯å®Œæ•´æ€§ï¼Œè¿‡æ»¤æ‰æ— æ³•è·å–å›¾ç‰‡çš„item
       (å‚è€ƒ MACRec load_all_figures.py çš„åšæ³•ï¼Œè€Œéä»…æ£€æŸ¥URLæ˜¯å¦å­˜åœ¨)
    2. å»é™¤åŒä¸€ç”¨æˆ·å¯¹åŒä¸€itemçš„é‡å¤äº¤äº’ (å‚è€ƒ MACRec make_inters_in_order)

è¾“å…¥:
    - Musical_Instruments.json (Amazon 2018 è¯„è®ºæ•°æ®)
    - meta_Musical_Instruments.json (å…ƒæ•°æ®ï¼Œå«å›¾ç‰‡URL)

è¾“å‡º (ä¿å­˜åˆ° dataset/Instrument2018_MM/):
    - Instrument2018_MM.inter
    - Instrument2018_MM.train.jsonl
    - Instrument2018_MM.valid.jsonl
    - Instrument2018_MM.test.jsonl
    - dataset_stats.json
    - images/ (ä¸‹è½½çš„å›¾ç‰‡ç›®å½•)
"""

import json
import pandas as pd
import os
from collections import defaultdict
from tqdm import tqdm
import requests


def download_image(url, save_path, timeout=10):
    """ä¸‹è½½å›¾ç‰‡ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
    try:
        response = requests.get(url, stream=True, timeout=timeout)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        # éªŒè¯ JPG å®Œæ•´æ€§ (å‚è€ƒ MACRec load_all_figures.py is_valid_jpg)
        with open(save_path, 'rb') as f:
            file_size = os.path.getsize(save_path)
            if file_size < 2:
                return False
            f.seek(file_size - 2)
            if f.read() != b'\xff\xd9':
                os.remove(save_path)
                return False
        return True
    except Exception:
        if os.path.exists(save_path):
            os.remove(save_path)
        return False


def load_image_items(meta_file, image_dir):
    """ä»metaæ•°æ®ä¸­æå–æœ‰å›¾ç‰‡çš„itemé›†åˆï¼Œå®é™…ä¸‹è½½éªŒè¯å›¾ç‰‡å¯ç”¨æ€§
    
    å‚è€ƒ MACRec/data_process/load_all_figures.py:
    ä¸ä»…æ£€æŸ¥ imageURLHighRes å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œè¿˜å®é™…ä¸‹è½½å›¾ç‰‡å¹¶éªŒè¯å®Œæ•´æ€§ã€‚
    åªæœ‰ä¸‹è½½æˆåŠŸä¸”æ–‡ä»¶å®Œæ•´çš„ item æ‰ä¼šè¢«ä¿ç•™ã€‚
    
    Args:
        meta_file: meta JSON æ–‡ä»¶è·¯å¾„
        image_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
    Returns:
        items_with_image: å®é™…æœ‰å¯ç”¨å›¾ç‰‡çš„ item asin é›†åˆ
    """
    print(f"ğŸ“· è¯»å–å…ƒæ•°æ®ï¼Œä¸‹è½½å¹¶éªŒè¯å›¾ç‰‡: {meta_file}")
    os.makedirs(image_dir, exist_ok=True)
    
    # å…ˆè¯»å–æ‰€æœ‰ metaï¼Œæ”¶é›†æœ‰ URL çš„ item
    asin2url = {}
    total = 0
    with open(meta_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                asin = item.get('asin', '')
                image_urls = item.get('imageURLHighRes', [])
                total += 1
                if image_urls and len(image_urls) > 0:
                    asin2url[asin] = image_urls[0]  # å–ç¬¬ä¸€å¼ é«˜æ¸…å›¾
            except json.JSONDecodeError:
                continue
    
    print(f"  - å…ƒæ•°æ®æ€»itemæ•°: {total}")
    print(f"  - æœ‰å›¾ç‰‡URLçš„itemæ•°: {len(asin2url)}")
    print(f"  - æ— å›¾ç‰‡URLçš„itemæ•°: {total - len(asin2url)}")
    
    # å®é™…ä¸‹è½½éªŒè¯
    items_with_image = set()
    download_ok = 0
    download_fail = 0
    already_exist = 0
    
    for asin, url in tqdm(asin2url.items(), desc="ä¸‹è½½éªŒè¯å›¾ç‰‡"):
        save_path = os.path.join(image_dir, f"{asin}.jpg")
        
        # å·²ä¸‹è½½è¿‡ä¸”æ–‡ä»¶æœ‰æ•ˆï¼Œè·³è¿‡
        if os.path.exists(save_path) and os.path.getsize(save_path) > 2:
            items_with_image.add(asin)
            already_exist += 1
            continue
        
        if download_image(url, save_path):
            items_with_image.add(asin)
            download_ok += 1
        else:
            download_fail += 1
    
    print(f"  - ä¸‹è½½ç»“æœ: æ–°ä¸‹è½½={download_ok}, å·²å­˜åœ¨={already_exist}, å¤±è´¥={download_fail}")
    print(f"  - å®é™…å¯ç”¨å›¾ç‰‡çš„itemæ•°: {len(items_with_image)}")
    return items_with_image


def load_and_preprocess(review_file, items_with_image, min_interactions=5):
    """åŠ è½½å¹¶é¢„å¤„ç†Amazon 2018è¯„è®ºæ•°æ®ï¼Œè¿‡æ»¤æ— å›¾ç‰‡item"""
    print(f"\nğŸ“– è¯»å–æ•°æ®: {review_file}")
    
    reviews = []
    with open(review_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                reviews.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    
    df = pd.DataFrame(reviews)
    print(f"åŸå§‹æ•°æ®: {len(df):,} æ¡äº¤äº’")
    
    df = df.rename(columns={
        'reviewerID': 'user_id',
        'asin': 'item_id',
        'overall': 'rating',
        'unixReviewTime': 'timestamp'
    })
    
    df = df.dropna(subset=['user_id', 'item_id', 'timestamp'])
    df['timestamp'] = df['timestamp'].astype(float)
    
    # å…³é”®æ­¥éª¤: è¿‡æ»¤æ‰æ²¡æœ‰å›¾ç‰‡çš„item (é€šè¿‡å®é™…ä¸‹è½½éªŒè¯ï¼Œå‚è€ƒMACRec)
    before_filter = len(df)
    df = df[df['item_id'].isin(items_with_image)]
    print(f"ğŸ” å›¾ç‰‡è¿‡æ»¤: {before_filter:,} â†’ {len(df):,} æ¡äº¤äº’ "
          f"(ç§»é™¤ {before_filter - len(df):,} æ¡æ— å›¾ç‰‡äº¤äº’)")
    
    # # å»é‡: åŒä¸€ç”¨æˆ·å¯¹åŒä¸€itemåªä¿ç•™ç¬¬ä¸€æ¬¡äº¤äº’ (å‚è€ƒMACRec make_inters_in_order)
    # before_dedup = len(df)
    # df = df.sort_values(['user_id', 'timestamp'])
    # df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')
    # if before_dedup != len(df):
    #     print(f"ğŸ”„ å»é‡: {before_dedup:,} â†’ {len(df):,} æ¡äº¤äº’ "
    #           f"(ç§»é™¤ {before_dedup - len(df):,} æ¡é‡å¤äº¤äº’)")
    
    # è¿­ä»£è¿‡æ»¤ (ä¿ç•™è‡³å°‘min_interactionsæ¬¡äº¤äº’çš„ç”¨æˆ·å’Œç‰©å“)
    print(f"ğŸ”„ è¿­ä»£è¿‡æ»¤ (æœ€å°‘{min_interactions}æ¬¡äº¤äº’)...")
    prev_len = -1
    iteration = 0
    while len(df) != prev_len:
        iteration += 1
        prev_len = len(df)
        user_counts = df['user_id'].value_counts()
        df = df[df['user_id'].isin(user_counts[user_counts >= min_interactions].index)]
        item_counts = df['item_id'].value_counts()
        df = df[df['item_id'].isin(item_counts[item_counts >= min_interactions].index)]
        print(f"  è¿­ä»£{iteration}: {len(df):,} æ¡, {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“")
    
    df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)
    print(f"âœ… é¢„å¤„ç†å®Œæˆ: {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“, {len(df):,} äº¤äº’\n")
    return df


def split_data(df):
    """ç»Ÿä¸€åˆ’åˆ†train/valid/test (leave-one-out)"""
    print("ğŸ”ª åˆ’åˆ†æ•°æ®é›† (leave-one-out)...")
    df['split'] = 'train'
    
    for user_id, group in df.groupby('user_id'):
        indices = group.index.tolist()
        if len(indices) >= 3:
            df.loc[indices[-1], 'split'] = 'test'
            df.loc[indices[-2], 'split'] = 'valid'
    
    train_cnt = len(df[df['split'] == 'train'])
    valid_cnt = len(df[df['split'] == 'valid'])
    test_cnt = len(df[df['split'] == 'test'])
    print(f"âœ… åˆ’åˆ†å®Œæˆ: train={train_cnt:,}, valid={valid_cnt:,}, test={test_cnt:,}\n")
    return df


def save_inter_file(df, output_dir, dataset_name):
    """ä¿å­˜RecBoleæ ¼å¼çš„.interæ–‡ä»¶ (å¸¦splitæ ‡ç­¾)"""
    os.makedirs(output_dir, exist_ok=True)
    inter_file = os.path.join(output_dir, f'{dataset_name}.inter')
    
    with open(inter_file, 'w', encoding='utf-8') as f:
        f.write('user_id:token\titem_id:token\trating:float\ttimestamp:float\tsplit:token\n')
        for _, row in df.iterrows():
            f.write(f"{row['user_id']}\t{row['item_id']}\t{row['rating']}\t{row['timestamp']}\t{row['split']}\n")
    
    print(f"âœ… å·²ä¿å­˜: {inter_file}")
    return inter_file


def build_sequences(df, max_seq_length=50):
    """æ ¹æ®splitæ ‡ç­¾æ„å»ºåºåˆ—æ•°æ®"""
    print(f"ğŸ”¨ æ„å»ºåºåˆ— (max_length={max_seq_length})...")
    
    df = df.sort_values(['user_id', 'timestamp'])
    train_seqs, valid_seqs, test_seqs = [], [], []
    
    for user_id, group in tqdm(df.groupby('user_id'), desc="æ„å»ºåºåˆ—"):
        items = group['item_id'].tolist()
        splits = group['split'].tolist()
        
        if len(items) < 3:
            continue
        
        valid_idx = next((i for i, s in enumerate(splits) if s == 'valid'), None)
        test_idx = next((i for i, s in enumerate(splits) if s == 'test'), None)
        
        if valid_idx is None or test_idx is None:
            continue
        
        for i in range(1, valid_idx):
            history = items[:i][-max_seq_length:]
            train_seqs.append({
                'user_id': user_id,
                'target_id': items[i],
                'inter_history': history
            })
        
        valid_history = items[:valid_idx][-max_seq_length:]
        valid_seqs.append({
            'user_id': user_id,
            'target_id': items[valid_idx],
            'inter_history': valid_history
        })
        
        test_history = items[:test_idx][-max_seq_length:]
        test_seqs.append({
            'user_id': user_id,
            'target_id': items[test_idx],
            'inter_history': test_history
        })
    
    print(f"âœ… åºåˆ—æ„å»ºå®Œæˆ: train={len(train_seqs):,}, valid={len(valid_seqs):,}, test={len(test_seqs):,}\n")
    return train_seqs, valid_seqs, test_seqs


def save_jsonl(data, output_file):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"âœ… å·²ä¿å­˜: {output_file}")


def main():
    print("=" * 70)
    print("ğŸµ å¤šæ¨¡æ€æ•°æ®å‡†å¤‡ - Amazon Musical Instruments 2018")
    print("   (è¿‡æ»¤æ— å›¾ç‰‡itemï¼Œå‚è€ƒMACRec)")
    print("=" * 70)
    
    # é…ç½® - æºæ•°æ®æ¥è‡ªåŸå§‹æ•°æ®é›†ç›®å½•
    SRC_DIR = './dataset/Instrument2018_5090'
    REVIEW_FILE = os.path.join(SRC_DIR, 'Musical_Instruments.json')
    META_FILE = os.path.join(SRC_DIR, 'meta_Musical_Instruments.json')
    
    # è¾“å‡ºåˆ°æ–°ç›®å½•
    OUT_DIR = './dataset/Instrument2018_MM'
    DATASET_NAME = 'Instrument2018_MM'
    MIN_INTERACTIONS = 5
    MAX_SEQ_LENGTH = 50
    
    if not os.path.exists(REVIEW_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {REVIEW_FILE}")
        return
    if not os.path.exists(META_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {META_FILE}")
        return
    
    os.makedirs(OUT_DIR, exist_ok=True)
    
    # å¤åˆ¶metaæ–‡ä»¶åˆ°æ–°ç›®å½• (åç»­è„šæœ¬éœ€è¦)
    import shutil
    meta_dst = os.path.join(OUT_DIR, 'meta_Musical_Instruments.json')
    if not os.path.exists(meta_dst):
        shutil.copy2(META_FILE, meta_dst)
        print(f"ğŸ“‹ å·²å¤åˆ¶å…ƒæ•°æ®åˆ°: {meta_dst}")
    
    # æ­¥éª¤1: ä¸‹è½½å›¾ç‰‡å¹¶è·å–æœ‰å¯ç”¨å›¾ç‰‡çš„itemé›†åˆ
    IMAGE_DIR = os.path.join(OUT_DIR, 'images')
    items_with_image = load_image_items(META_FILE, IMAGE_DIR)
    
    # æ­¥éª¤2: åŠ è½½å’Œé¢„å¤„ç† (å«å›¾ç‰‡è¿‡æ»¤)
    df = load_and_preprocess(REVIEW_FILE, items_with_image, MIN_INTERACTIONS)
    
    # æ­¥éª¤3: åˆ’åˆ†æ•°æ®é›†
    df = split_data(df)
    
    # æ­¥éª¤4: ä¿å­˜.interæ–‡ä»¶
    save_inter_file(df, OUT_DIR, DATASET_NAME)
    
    # æ­¥éª¤5: æ„å»ºåºåˆ—
    train_seqs, valid_seqs, test_seqs = build_sequences(df, MAX_SEQ_LENGTH)
    
    # æ­¥éª¤6: ä¿å­˜JSONLæ–‡ä»¶
    print("ğŸ’¾ ä¿å­˜JSONLæ–‡ä»¶...")
    save_jsonl(train_seqs, os.path.join(OUT_DIR, f'{DATASET_NAME}.train.jsonl'))
    save_jsonl(valid_seqs, os.path.join(OUT_DIR, f'{DATASET_NAME}.valid.jsonl'))
    save_jsonl(test_seqs, os.path.join(OUT_DIR, f'{DATASET_NAME}.test.jsonl'))
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'num_users': int(df['user_id'].nunique()),
        'num_items': int(df['item_id'].nunique()),
        'num_interactions': int(len(df)),
        'train_interactions': int(len(df[df['split'] == 'train'])),
        'valid_interactions': int(len(df[df['split'] == 'valid'])),
        'test_interactions': int(len(df[df['split'] == 'test'])),
        'train_sequences': len(train_seqs),
        'valid_sequences': len(valid_seqs),
        'test_sequences': len(test_seqs),
    }
    stats_file = os.path.join(OUT_DIR, 'dataset_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ… å·²ä¿å­˜: {stats_file}")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ å¤šæ¨¡æ€æ•°æ®å‡†å¤‡å®Œæˆ!")
    print("=" * 70)
    print(f"\nè¾“å‡ºç›®å½•: {OUT_DIR}")
    print(f"åç»­æ­¥éª¤:")
    print(f"  1. python get_collab_emb.py       (ä¿®æ”¹è·¯å¾„æŒ‡å‘ {DATASET_NAME})")
    print(f"  2. python get_text_emb.py         (ä¿®æ”¹è·¯å¾„æŒ‡å‘ {DATASET_NAME})")
    print(f"  3. python get_image_emb.py --dataset {DATASET_NAME}")


if __name__ == '__main__':
    main()
