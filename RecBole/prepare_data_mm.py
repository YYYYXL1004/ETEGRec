#!/usr/bin/env python3
"""
å¤šæ¨¡æ€æ•°æ®å‡†å¤‡è„šæœ¬ - åœ¨ prepare_data_2018.py åŸºç¡€ä¸Šå¢åŠ å›¾ç‰‡è¿‡æ»¤

ä¸ prepare_data_2018.py çš„åŒºåˆ«:
    1. å…ˆç”¨URLå­˜åœ¨æ€§åˆç­› + 5-coreè¿‡æ»¤ï¼Œå¾—åˆ°æœ€ç»ˆitemé›†åˆ
    2. åªä¸‹è½½æœ€ç»ˆitemçš„å›¾ç‰‡å¹¶éªŒè¯å®Œæ•´æ€§ (é¿å…ä¸‹è½½å¤§é‡æ— ç”¨å›¾ç‰‡)
    3. è¸¢æ‰ä¸‹è½½å¤±è´¥çš„itemï¼Œå¿…è¦æ—¶è¡¥ä¸€è½®5-core
    (å‚è€ƒ MACRec load_all_figures.py çš„å›¾ç‰‡éªŒè¯é€»è¾‘)

æµç¨‹:
    URLåˆç­› â†’ 5-core â†’ ä¸‹è½½éªŒè¯å›¾ç‰‡ â†’ (è¡¥å……5-core) â†’ åˆ’åˆ† â†’ è¾“å‡º

è¾“å…¥:
    - Musical_Instruments.json (Amazon 2018 è¯„è®ºæ•°æ®)
    - meta_Musical_Instruments.json (å…ƒæ•°æ®ï¼Œå«å›¾ç‰‡URL)

è¾“å‡º (ä¿å­˜åˆ° dataset/Instrument2018_MM/):
    - Instrument2018_MM.inter
    - Instrument2018_MM.train.jsonl
    - Instrument2018_MM.valid.jsonl
    - Instrument2018_MM.test.jsonl
    - dataset_stats.json
    - images/ (ä¸‹è½½çš„å›¾ç‰‡ç›®å½•)
"""

import json
import pandas as pd
import os
from collections import defaultdict
from tqdm import tqdm
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed


def download_image(url, save_path, timeout=10):
    """ä¸‹è½½å›¾ç‰‡ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
    try:
        response = requests.get(url, stream=True, timeout=timeout)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        # éªŒè¯ JPG å®Œæ•´æ€§ (å‚è€ƒ MACRec load_all_figures.py is_valid_jpg)
        with open(save_path, 'rb') as f:
            file_size = os.path.getsize(save_path)
            if file_size < 2:
                return False
            f.seek(file_size - 2)
            if f.read() != b'\xff\xd9':
                os.remove(save_path)
                return False
        return True
    except Exception:
        if os.path.exists(save_path):
            os.remove(save_path)
        return False


def load_image_items_url(meta_file):
    """ä»metaæ•°æ®ä¸­æå–æœ‰å›¾ç‰‡URLçš„itemé›†åˆ (ä»…æ£€æŸ¥URLå­˜åœ¨æ€§ï¼Œä¸ä¸‹è½½)"""
    print(f"ğŸ“· è¯»å–å…ƒæ•°æ®ï¼Œç­›é€‰æœ‰å›¾ç‰‡URLçš„item: {meta_file}")
    asin2url = {}
    total = 0
    with open(meta_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                asin = item.get('asin', '')
                image_urls = item.get('imageURLHighRes', [])
                total += 1
                if image_urls and len(image_urls) > 0:
                    asin2url[asin] = image_urls[0]
            except json.JSONDecodeError:
                continue
    
    print(f"  - å…ƒæ•°æ®æ€»itemæ•°: {total}")
    print(f"  - æœ‰å›¾ç‰‡URLçš„itemæ•°: {len(asin2url)}")
    print(f"  - æ— å›¾ç‰‡URLçš„itemæ•°: {total - len(asin2url)}")
    return asin2url


def download_and_verify(asin2url, target_asins, image_dir, max_workers=32):
    """åªä¸‹è½½ target_asins ä¸­çš„å›¾ç‰‡ï¼Œè¿”å›å®é™…ä¸‹è½½æˆåŠŸçš„ asin é›†åˆ
    
    Args:
        asin2url: asin -> å›¾ç‰‡URL çš„å®Œæ•´æ˜ å°„
        target_asins: éœ€è¦ä¸‹è½½çš„ asin é›†åˆ (5-core è¿‡æ»¤åçš„æœ€ç»ˆ item)
        image_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
        max_workers: å¹¶å‘çº¿ç¨‹æ•°
    Returns:
        verified_asins: å®é™…æœ‰å¯ç”¨å›¾ç‰‡çš„ asin é›†åˆ
    """
    os.makedirs(image_dir, exist_ok=True)
    
    verified = set()
    to_download = {}
    already_exist = 0
    no_url = 0
    
    for asin in target_asins:
        if asin not in asin2url:
            no_url += 1
            continue
        save_path = os.path.join(image_dir, f"{asin}.jpg")
        if os.path.exists(save_path) and os.path.getsize(save_path) > 2:
            verified.add(asin)
            already_exist += 1
        else:
            to_download[asin] = asin2url[asin]
    
    print(f"\nğŸ“¥ ä¸‹è½½å›¾ç‰‡ (ä»… 5-core åçš„ {len(target_asins)} ä¸ªitem)")
    print(f"  - å·²å­˜åœ¨: {already_exist}, å¾…ä¸‹è½½: {len(to_download)}, æ— URL: {no_url}")
    
    if to_download:
        download_ok = 0
        download_fail = 0
        
        def _download_one(asin_url):
            asin, url = asin_url
            save_path = os.path.join(image_dir, f"{asin}.jpg")
            return asin, download_image(url, save_path)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(_download_one, item): item 
                       for item in to_download.items()}
            with tqdm(total=len(futures), desc="ä¸‹è½½éªŒè¯å›¾ç‰‡") as pbar:
                for future in as_completed(futures):
                    asin, success = future.result()
                    if success:
                        verified.add(asin)
                        download_ok += 1
                    else:
                        download_fail += 1
                    pbar.update(1)
        
        print(f"  - ä¸‹è½½ç»“æœ: æˆåŠŸ={download_ok}, å¤±è´¥={download_fail}")
    
    print(f"  - å®é™…å¯ç”¨å›¾ç‰‡: {len(verified)} / {len(target_asins)}")
    return verified


def load_and_preprocess(review_file, items_with_image, min_interactions=5):
    """åŠ è½½å¹¶é¢„å¤„ç†Amazon 2018è¯„è®ºæ•°æ®ï¼Œè¿‡æ»¤æ— å›¾ç‰‡item"""
    print(f"\nğŸ“– è¯»å–æ•°æ®: {review_file}")
    
    reviews = []
    with open(review_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                reviews.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    
    df = pd.DataFrame(reviews)
    print(f"åŸå§‹æ•°æ®: {len(df):,} æ¡äº¤äº’")
    
    df = df.rename(columns={
        'reviewerID': 'user_id',
        'asin': 'item_id',
        'overall': 'rating',
        'unixReviewTime': 'timestamp'
    })
    
    df = df.dropna(subset=['user_id', 'item_id', 'timestamp'])
    df['timestamp'] = df['timestamp'].astype(float)
    
    # å…³é”®æ­¥éª¤: è¿‡æ»¤æ‰æ²¡æœ‰å›¾ç‰‡çš„item (é€šè¿‡å®é™…ä¸‹è½½éªŒè¯ï¼Œå‚è€ƒMACRec)
    before_filter = len(df)
    df = df[df['item_id'].isin(items_with_image)]
    print(f"ğŸ” å›¾ç‰‡è¿‡æ»¤: {before_filter:,} â†’ {len(df):,} æ¡äº¤äº’ "
          f"(ç§»é™¤ {before_filter - len(df):,} æ¡æ— å›¾ç‰‡äº¤äº’)")
    
    # # å»é‡: åŒä¸€ç”¨æˆ·å¯¹åŒä¸€itemåªä¿ç•™ç¬¬ä¸€æ¬¡äº¤äº’ (å‚è€ƒMACRec make_inters_in_order)
    # before_dedup = len(df)
    # df = df.sort_values(['user_id', 'timestamp'])
    # df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')
    # if before_dedup != len(df):
    #     print(f"ğŸ”„ å»é‡: {before_dedup:,} â†’ {len(df):,} æ¡äº¤äº’ "
    #           f"(ç§»é™¤ {before_dedup - len(df):,} æ¡é‡å¤äº¤äº’)")
    
    # è¿­ä»£è¿‡æ»¤ (ä¿ç•™è‡³å°‘min_interactionsæ¬¡äº¤äº’çš„ç”¨æˆ·å’Œç‰©å“)
    print(f"ğŸ”„ è¿­ä»£è¿‡æ»¤ (æœ€å°‘{min_interactions}æ¬¡äº¤äº’)...")
    prev_len = -1
    iteration = 0
    while len(df) != prev_len:
        iteration += 1
        prev_len = len(df)
        user_counts = df['user_id'].value_counts()
        df = df[df['user_id'].isin(user_counts[user_counts >= min_interactions].index)]
        item_counts = df['item_id'].value_counts()
        df = df[df['item_id'].isin(item_counts[item_counts >= min_interactions].index)]
        print(f"  è¿­ä»£{iteration}: {len(df):,} æ¡, {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“")
    
    df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)
    print(f"âœ… é¢„å¤„ç†å®Œæˆ: {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“, {len(df):,} äº¤äº’\n")
    return df


def split_data(df):
    """ç»Ÿä¸€åˆ’åˆ†train/valid/test (leave-one-out)"""
    print("ğŸ”ª åˆ’åˆ†æ•°æ®é›† (leave-one-out)...")
    df['split'] = 'train'
    
    for user_id, group in df.groupby('user_id'):
        indices = group.index.tolist()
        if len(indices) >= 3:
            df.loc[indices[-1], 'split'] = 'test'
            df.loc[indices[-2], 'split'] = 'valid'
    
    train_cnt = len(df[df['split'] == 'train'])
    valid_cnt = len(df[df['split'] == 'valid'])
    test_cnt = len(df[df['split'] == 'test'])
    print(f"âœ… åˆ’åˆ†å®Œæˆ: train={train_cnt:,}, valid={valid_cnt:,}, test={test_cnt:,}\n")
    return df


def save_inter_file(df, output_dir, dataset_name):
    """ä¿å­˜RecBoleæ ¼å¼çš„.interæ–‡ä»¶ (å¸¦splitæ ‡ç­¾)"""
    os.makedirs(output_dir, exist_ok=True)
    inter_file = os.path.join(output_dir, f'{dataset_name}.inter')
    
    with open(inter_file, 'w', encoding='utf-8') as f:
        f.write('user_id:token\titem_id:token\trating:float\ttimestamp:float\tsplit:token\n')
        for _, row in df.iterrows():
            f.write(f"{row['user_id']}\t{row['item_id']}\t{row['rating']}\t{row['timestamp']}\t{row['split']}\n")
    
    print(f"âœ… å·²ä¿å­˜: {inter_file}")
    return inter_file


def build_sequences(df, max_seq_length=50):
    """æ ¹æ®splitæ ‡ç­¾æ„å»ºåºåˆ—æ•°æ®"""
    print(f"ğŸ”¨ æ„å»ºåºåˆ— (max_length={max_seq_length})...")
    
    df = df.sort_values(['user_id', 'timestamp'])
    train_seqs, valid_seqs, test_seqs = [], [], []
    
    for user_id, group in tqdm(df.groupby('user_id'), desc="æ„å»ºåºåˆ—"):
        items = group['item_id'].tolist()
        splits = group['split'].tolist()
        
        if len(items) < 3:
            continue
        
        valid_idx = next((i for i, s in enumerate(splits) if s == 'valid'), None)
        test_idx = next((i for i, s in enumerate(splits) if s == 'test'), None)
        
        if valid_idx is None or test_idx is None:
            continue
        
        for i in range(1, valid_idx):
            history = items[:i][-max_seq_length:]
            train_seqs.append({
                'user_id': user_id,
                'target_id': items[i],
                'inter_history': history
            })
        
        valid_history = items[:valid_idx][-max_seq_length:]
        valid_seqs.append({
            'user_id': user_id,
            'target_id': items[valid_idx],
            'inter_history': valid_history
        })
        
        test_history = items[:test_idx][-max_seq_length:]
        test_seqs.append({
            'user_id': user_id,
            'target_id': items[test_idx],
            'inter_history': test_history
        })
    
    print(f"âœ… åºåˆ—æ„å»ºå®Œæˆ: train={len(train_seqs):,}, valid={len(valid_seqs):,}, test={len(test_seqs):,}\n")
    return train_seqs, valid_seqs, test_seqs


def save_jsonl(data, output_file):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"âœ… å·²ä¿å­˜: {output_file}")


def main():
    print("=" * 70)
    print("ğŸµ å¤šæ¨¡æ€æ•°æ®å‡†å¤‡ - Amazon Musical Instruments 2018")
    print("   (è¿‡æ»¤æ— å›¾ç‰‡itemï¼Œå‚è€ƒMACRec)")
    print("=" * 70)
    
    # é…ç½® - æºæ•°æ®æ¥è‡ªåŸå§‹æ•°æ®é›†ç›®å½•
    SRC_DIR = './dataset/Instrument2018_5090'
    REVIEW_FILE = os.path.join(SRC_DIR, 'Musical_Instruments.json')
    META_FILE = os.path.join(SRC_DIR, 'meta_Musical_Instruments.json')
    
    # è¾“å‡ºåˆ°æ–°ç›®å½•
    OUT_DIR = './dataset/Instrument2018_MM'
    DATASET_NAME = 'Instrument2018_MM'
    MIN_INTERACTIONS = 5
    MAX_SEQ_LENGTH = 50
    
    if not os.path.exists(REVIEW_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {REVIEW_FILE}")
        return
    if not os.path.exists(META_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {META_FILE}")
        return
    
    os.makedirs(OUT_DIR, exist_ok=True)
    
    # å¤åˆ¶metaæ–‡ä»¶åˆ°æ–°ç›®å½• (åç»­è„šæœ¬éœ€è¦)
    import shutil
    meta_dst = os.path.join(OUT_DIR, 'meta_Musical_Instruments.json')
    if not os.path.exists(meta_dst):
        shutil.copy2(META_FILE, meta_dst)
        print(f"ğŸ“‹ å·²å¤åˆ¶å…ƒæ•°æ®åˆ°: {meta_dst}")
    
    # æ­¥éª¤1: ç”¨URLå­˜åœ¨æ€§åˆç­› + 5-coreè¿‡æ»¤ (ä¸ä¸‹è½½å›¾ç‰‡ï¼Œé€Ÿåº¦å¿«)
    asin2url = load_image_items_url(META_FILE)
    items_with_url = set(asin2url.keys())
    df = load_and_preprocess(REVIEW_FILE, items_with_url, MIN_INTERACTIONS)
    
    # æ­¥éª¤2: åªä¸‹è½½ 5-core åæœ€ç»ˆ item çš„å›¾ç‰‡å¹¶éªŒè¯
    IMAGE_DIR = os.path.join(OUT_DIR, 'images')
    final_items = set(df['item_id'].unique())
    verified_items = download_and_verify(asin2url, final_items, IMAGE_DIR)
    
    # æ­¥éª¤3: è¸¢æ‰ä¸‹è½½å¤±è´¥çš„itemï¼Œå¦‚æœæœ‰çš„è¯å†è¡¥ä¸€è½® 5-core
    failed_items = final_items - verified_items
    if failed_items:
        print(f"\nâš ï¸  {len(failed_items)} ä¸ªitemå›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œé‡æ–°è¿‡æ»¤...")
        df = df[df['item_id'].isin(verified_items)]
        # è¡¥ä¸€è½® 5-core (ä¸‹è½½å¤±è´¥å¯èƒ½å¯¼è‡´æŸäº›ç”¨æˆ·/itemä¸æ»¡è¶³5æ¬¡)
        prev_len = -1
        iteration = 0
        while len(df) != prev_len:
            iteration += 1
            prev_len = len(df)
            user_counts = df['user_id'].value_counts()
            df = df[df['user_id'].isin(user_counts[user_counts >= MIN_INTERACTIONS].index)]
            item_counts = df['item_id'].value_counts()
            df = df[df['item_id'].isin(item_counts[item_counts >= MIN_INTERACTIONS].index)]
        print(f"  è¡¥å……è¿‡æ»¤å: {len(df):,} æ¡, {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“")
    else:
        print(f"\nâœ… æ‰€æœ‰ {len(final_items)} ä¸ªitemå›¾ç‰‡å‡å¯ç”¨ï¼Œæ— éœ€è¡¥å……è¿‡æ»¤")
    
    # æ­¥éª¤4: åˆ’åˆ†æ•°æ®é›†
    df = split_data(df)
    
    # æ­¥éª¤5: ä¿å­˜.interæ–‡ä»¶
    save_inter_file(df, OUT_DIR, DATASET_NAME)
    
    # æ­¥éª¤6: æ„å»ºåºåˆ—
    train_seqs, valid_seqs, test_seqs = build_sequences(df, MAX_SEQ_LENGTH)
    
    # æ­¥éª¤7: ä¿å­˜JSONLæ–‡ä»¶
    print("ğŸ’¾ ä¿å­˜JSONLæ–‡ä»¶...")
    save_jsonl(train_seqs, os.path.join(OUT_DIR, f'{DATASET_NAME}.train.jsonl'))
    save_jsonl(valid_seqs, os.path.join(OUT_DIR, f'{DATASET_NAME}.valid.jsonl'))
    save_jsonl(test_seqs, os.path.join(OUT_DIR, f'{DATASET_NAME}.test.jsonl'))
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'num_users': int(df['user_id'].nunique()),
        'num_items': int(df['item_id'].nunique()),
        'num_interactions': int(len(df)),
        'train_interactions': int(len(df[df['split'] == 'train'])),
        'valid_interactions': int(len(df[df['split'] == 'valid'])),
        'test_interactions': int(len(df[df['split'] == 'test'])),
        'train_sequences': len(train_seqs),
        'valid_sequences': len(valid_seqs),
        'test_sequences': len(test_seqs),
    }
    stats_file = os.path.join(OUT_DIR, 'dataset_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ… å·²ä¿å­˜: {stats_file}")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ å¤šæ¨¡æ€æ•°æ®å‡†å¤‡å®Œæˆ!")
    print("=" * 70)
    print(f"\nè¾“å‡ºç›®å½•: {OUT_DIR}")
    print(f"åç»­æ­¥éª¤:")
    print(f"  1. python get_collab_emb.py --dataset {DATASET_NAME}")
    print(f"  2. python get_text_emb.py --dataset {DATASET_NAME}")
    print(f"  3. python get_image_emb.py --dataset {DATASET_NAME}")


if __name__ == '__main__':
    main()
