#!/usr/bin/env python3
"""
SASRecè®­ç»ƒè„šæœ¬ - ç”Ÿæˆç‰©å“åµŒå…¥å’Œæ˜ å°„æ–‡ä»¶

åŠŸèƒ½:
1. è¯»å–å¸¦splitæ ‡ç­¾çš„.interæ–‡ä»¶
2. åˆ†å‰²train/valid/testæ•°æ®
3. åªåœ¨trainä¸Šè®­ç»ƒSASRec (æ¢¯åº¦æ›´æ–°)
4. ä½¿ç”¨validåšæ—©åœ (é€‰æ‹©æœ€ä½³checkpoint)
5. åœ¨testä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½
6. ç”Ÿæˆç‰©å“åµŒå…¥ .npy æ–‡ä»¶å’Œ item2id æ˜ å°„æ–‡ä»¶

å…³é”®:
- æ— æ•°æ®æ³„éœ²: åªæœ‰trainå‚ä¸æ¢¯åº¦æ›´æ–°
- æ—©åœæœºåˆ¶: ä½¿ç”¨validé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- æœ€ç»ˆè¯„ä¼°: åœ¨testä¸ŠæŠ¥å‘Šæ€§èƒ½
- ä¸ETEGRecæ•°æ®åˆ’åˆ†å®Œå…¨ä¸€è‡´
"""

import os
import json
import numpy as np
import pandas as pd
from recbole.config import Config
from recbole.data import create_dataset, data_preparation
from recbole.model.sequential_recommender import SASRec
from recbole.trainer import Trainer


def load_and_split_data(inter_file):
    """åŠ è½½.interæ–‡ä»¶å¹¶æŒ‰splitæ ‡ç­¾åˆ†å‰²æ•°æ®"""
    print(f"ğŸ“– è¯»å–æ•°æ®: {inter_file}")
    
    # è¯»å–å¹¶è§„èŒƒåŒ–åˆ—å
    df = pd.read_csv(inter_file, sep='\t', header=0, dtype=str, keep_default_na=False)
    df.columns = [c.split(':')[0] for c in df.columns]
    df = df.replace({'': pd.NA})
    
    # è½¬æ¢æ•°æ®ç±»å‹
    df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')
    df['rating'] = pd.to_numeric(df['rating'], errors='coerce').fillna(1.0)
    df['split'] = df['split'].str.strip().str.lower()
    
    print(f"æ•°æ®åˆ†å¸ƒ: train={len(df[df['split']=='train']):,}, "
          f"valid={len(df[df['split']=='valid']):,}, "
          f"test={len(df[df['split']=='test']):,}")
    
    # åˆ†å‰²æ•°æ®
    df_train = df[df['split'] == 'train'].copy()
    df_valid = df[df['split'] == 'valid'].copy()
    df_test = df[df['split'] == 'test'].copy()
    
    # è¿‡æ»¤: ä¿ç•™åœ¨æ‰€æœ‰é›†åˆä¸­éƒ½å‡ºç°çš„ç”¨æˆ·
    valid_users = set(df_train['user_id'].unique()) & set(df_valid['user_id'].unique()) & set(df_test['user_id'].unique())
    df_train = df_train[df_train['user_id'].isin(valid_users)]
    df_valid = df_valid[df_valid['user_id'].isin(valid_users)]
    df_test = df_test[df_test['user_id'].isin(valid_users)]
    
    # æ’åº
    df_train = df_train.sort_values(['user_id', 'timestamp'])
    df_valid = df_valid.sort_values(['user_id', 'timestamp'])
    df_test = df_test.sort_values(['user_id', 'timestamp'])
    
    print(f"âœ… è®­ç»ƒé›†: {df_train['user_id'].nunique():,} ç”¨æˆ·, {len(df_train):,} äº¤äº’")
    print(f"âœ… éªŒè¯é›†: {df_valid['user_id'].nunique():,} ç”¨æˆ·, {len(df_valid):,} äº¤äº’")
    print(f"âœ… æµ‹è¯•é›†: {df_test['user_id'].nunique():,} ç”¨æˆ·, {len(df_test):,} äº¤äº’\n")
    
    return df_train, df_valid, df_test


def save_recbole_inter_file(df_train, df_valid, df_test, output_dir, dataset_name):
    """ä¿å­˜åˆå¹¶çš„.interæ–‡ä»¶ä¾›RecBoleä½¿ç”¨ (è®©RecBoleè‡ªåŠ¨åšleave-one-outåˆ’åˆ†)"""
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå¹¶æ‰€æœ‰æ•°æ® (RecBoleä¼šè‡ªåŠ¨æŒ‰æ—¶é—´æˆ³åšleave-one-outåˆ’åˆ†)
    df_all = pd.concat([df_train, df_valid, df_test], ignore_index=True)
    df_all = df_all.sort_values(['user_id', 'timestamp'])
    
    inter_file = os.path.join(output_dir, f'{dataset_name}.inter')
    df_all[['user_id', 'item_id', 'rating', 'timestamp']].to_csv(
        inter_file, sep='\t', index=False,
        header=['user_id:token', 'item_id:token', 'rating:float', 'timestamp:float']
    )
    
    print(f"ğŸ’¾ å·²ä¿å­˜æ•°æ®æ–‡ä»¶:")
    print(f"   - {inter_file} (æ‰€æœ‰æ•°æ®ï¼ŒRecBoleå°†è‡ªåŠ¨åˆ’åˆ†)")
    print(f"   åŸå§‹åˆ†å¸ƒ: train={len(df_train)}, valid={len(df_valid)}, test={len(df_test)}")
    print(f"   RecBoleå°†æŒ‰æ—¶é—´æˆ³è‡ªåŠ¨åšleave-one-outåˆ’åˆ†\n")
    
    return inter_file


def train_sasrec(dataset_name, data_path):
    """è®­ç»ƒSASRecæ¨¡å‹ (ä½¿ç”¨çœŸå®valid/teståšæ—©åœå’Œè¯„ä¼°)"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒSASRec...")
    
    config_dict = {
        'model': 'SASRec',
        'dataset': dataset_name,
        'data_path': data_path,
        'USER_ID_FIELD': 'user_id',
        'ITEM_ID_FIELD': 'item_id',
        'RATING_FIELD': 'rating',
        'TIME_FIELD': 'timestamp',
        'load_col': {'inter': ['user_id', 'item_id', 'rating', 'timestamp']},
        
        # è¯„ä¼°é…ç½® - RecBoleä¼šè‡ªåŠ¨åšleave-one-out
        'eval_args': {
            'split': {'LS': 'valid_and_test'},  # è‡ªåŠ¨åˆ’åˆ†æœ€å2ä¸ªäº¤äº’ä¸ºvalidå’Œtest
            'order': 'TO',  # æŒ‰æ—¶é—´æ’åº
            'group_by': 'user',  # æŒ‰ç”¨æˆ·åˆ†ç»„
            'mode': 'full'  # å…¨æ’åºæ¨¡å¼
        },
        
        # æ¨¡å‹å‚æ•°
        'hidden_size': 256,
        'inner_size': 256,
        'n_layers': 2,
        'n_heads': 2,
        'hidden_dropout_prob': 0.5,
        'attn_dropout_prob': 0.5,
        'hidden_act': 'gelu',
        'loss_type': 'CE',
        'max_seq_length': 50,
        
        # è®­ç»ƒå‚æ•°
        'train_neg_sample_args': None,
        'epochs': 50,  # å¢åŠ æœ€å¤§epochsï¼Œä¾èµ–early stopping
        'train_batch_size': 2048,
        'eval_batch_size': 2048,
        'learner': 'adam',
        'learning_rate': 0.001,
        'eval_step': 1,  # æ¯ä¸ªepochè¯„ä¼°ä¸€æ¬¡
        'stopping_step': 10,  # 10ä¸ªepochæ— æå‡åˆ™åœæ­¢
        
        # è¯„ä¼°æŒ‡æ ‡
        'metrics': ['Recall', 'NDCG', 'Hit', 'MRR'],
        'topk': [5, 10, 20],
        'valid_metric': 'NDCG@10',
        
        # è®¾å¤‡é…ç½®
        'gpu_id': '0',
        'use_gpu': True,
        'checkpoint_dir': f'./saved/SASRec_{dataset_name}',
        'show_progress': True,
    }
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ¨¡å‹
    config = Config(model='SASRec', dataset=dataset_name, config_dict=config_dict)
    dataset = create_dataset(config)
    train_data, valid_data, test_data = data_preparation(config, dataset)
    
    model = SASRec(config, train_data.dataset).to(config['device'])
    trainer = Trainer(config, model)
    
    # è®­ç»ƒ (ä½¿ç”¨validåšæ—©åœ)
    print("\nâš™ï¸  è®­ç»ƒé…ç½®:")
    print(f"   - åªåœ¨trainæ•°æ®ä¸Šè®­ç»ƒ (æ¢¯åº¦æ›´æ–°)")
    print(f"   - ä½¿ç”¨validæ•°æ®åšæ—©åœ (é€‰æ‹©æœ€ä½³checkpoint)")
    print(f"   - æœ€å¤§åºåˆ—é•¿åº¦: {config['max_seq_length']}")
    print(f"   - æœ€å¤§epochs: {config['epochs']}, æ—©åœpatience: {config['stopping_step']}\n")
    
    best_valid_score, best_valid_result = trainer.fit(
        train_data, 
        valid_data=valid_data,  # ä½¿ç”¨çœŸå®éªŒè¯é›†
        saved=True,  # ä¿å­˜æœ€ä½³æ¨¡å‹
        show_progress=True
    )
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³éªŒè¯é›† {config['valid_metric']}: {best_valid_score:.4f}")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
    print("\nğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
    test_result = trainer.evaluate(test_data, load_best_model=True, show_progress=True)
    
    print("\næµ‹è¯•é›†ç»“æœ:")
    for metric, value in test_result.items():
        print(f"   {metric}: {value:.4f}")
    
    return model, dataset, test_result


def extract_embeddings(model, dataset, output_dir, dataset_name):
    """æå–å¹¶ä¿å­˜ç‰©å“åµŒå…¥å’Œæ˜ å°„"""
    print("ğŸ’¾ æå–ç‰©å“åµŒå…¥...")
    
    # æå–åµŒå…¥ (å»é™¤padding)
    item_embedding = model.item_embedding.weight.data.cpu().numpy()
    item_embedding_no_pad = item_embedding[1:]  # å»é™¤ç¬¬0ä¸ª (padding)
    
    # ä¿å­˜åµŒå…¥
    npy_path = os.path.join(output_dir, f'{dataset_name}_emb_256.npy')
    np.save(npy_path, item_embedding_no_pad)
    print(f"âœ… å·²ä¿å­˜: {npy_path} (shape={item_embedding_no_pad.shape})")
    
    # æ„å»ºæ˜ å°„ (åŒ…å«[PAD])
    item_token2id = dataset.field2token_id['item_id']
    item2id_map = {'[PAD]': 0}
    for token, idx in item_token2id.items():
        if idx > 0:
            item2id_map[str(token)] = int(idx)
    
    # ä¿å­˜æ˜ å°„
    map_path = os.path.join(output_dir, f'{dataset_name}.emb_map.json')
    with open(map_path, 'w', encoding='utf-8') as f:
        json.dump(item2id_map, f, indent=2, ensure_ascii=False)
    print(f"âœ… å·²ä¿å­˜: {map_path} (å«[PAD], å…±{len(item2id_map)}ä¸ªç‰©å“)")
    
    # éªŒè¯ä¸€è‡´æ€§
    if len(item2id_map) != item_embedding_no_pad.shape[0] + 1:
        raise ValueError(f"æ˜ å°„æ•°é‡ ({len(item2id_map)}) != åµŒå…¥æ•°é‡+1 ({item_embedding_no_pad.shape[0]+1})")
    
    print("âœ… æ˜ å°„ä¸åµŒå…¥ä¸€è‡´æ€§éªŒè¯é€šè¿‡\n")
    return npy_path, map_path


def main():
    print("=" * 70)
    print("ğŸµ SASRecè®­ç»ƒ - Amazon Musical Instruments 2023")
    print("=" * 70)
    
    # é…ç½®
    BASE_DIR = './dataset/Instruments2023'
    INTER_FILE = os.path.join(BASE_DIR, 'Instruments2023.inter')
    DATASET_NAME = 'Instruments2023'
    
    if not os.path.exists(INTER_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {INTER_FILE}")
        print("è¯·å…ˆè¿è¡Œ: python prepare_data.py")
        return
    
    # æ­¥éª¤1: åŠ è½½å¹¶åˆ†å‰²æ•°æ®
    df_train, df_valid, df_test = load_and_split_data(INTER_FILE)
    
    # æ­¥éª¤2: ä¿å­˜åˆå¹¶çš„.interæ–‡ä»¶ (ä¾›RecBoleä½¿ç”¨)
    recbole_dir = './dataset/Instruments2023_recbole'
    recbole_dataset_name = 'Instruments2023_recbole'
    save_recbole_inter_file(df_train, df_valid, df_test, recbole_dir, recbole_dataset_name)
    
    # æ­¥éª¤3: è®­ç»ƒSASRec (ä½¿ç”¨validåšæ—©åœï¼Œteståšæœ€ç»ˆè¯„ä¼°)
    model, dataset, test_result = train_sasrec(recbole_dataset_name, './dataset/')
    
    # æ­¥éª¤4: æå–å¹¶ä¿å­˜åµŒå…¥ (ä¿å­˜åˆ°åŸå§‹æ•°æ®ç›®å½•)
    npy_path, map_path = extract_embeddings(model, dataset, BASE_DIR, DATASET_NAME)
    
    print("=" * 70)
    print("ğŸ‰ SASRecè®­ç»ƒå®Œæˆ!")
    print("=" * 70)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. {npy_path} - ç‰©å“åµŒå…¥")
    print(f"  2. {map_path} - item2idæ˜ å°„")
    print(f"\nâœ… è®­ç»ƒç­–ç•¥:")
    print(f"   - åªåœ¨trainæ•°æ®ä¸Šè®­ç»ƒ (æ— æ•°æ®æ³„éœ²)")
    print(f"   - ä½¿ç”¨validæ•°æ®åšæ—©åœ (é€‰æ‹©æœ€ä½³æ¨¡å‹)")
    print(f"   - åœ¨testæ•°æ®ä¸Šè¯„ä¼° (æœ€ç»ˆæ€§èƒ½)")
    print(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½: NDCG@10 = {test_result.get('ndcg@10', 0):.4f}")


if __name__ == '__main__':
    main()
