"""
ç»Ÿä¸€æ•°æ®é¢„å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼š
1. è¯»å–AmazonåŸå§‹æ•°æ®
2. è¿‡æ»¤å’Œæ¸…æ´—
3. ç»Ÿä¸€åˆ’åˆ† train/valid/test (è®°å½•æ¯æ¡æ•°æ®å±äºå“ªä¸ªé›†åˆ)
4. ä¿å­˜ RecBole æ ¼å¼ + åˆ’åˆ†ä¿¡æ¯
"""

import json
import pandas as pd
import os
from collections import defaultdict
from datetime import datetime
import numpy as np

def load_reviews(review_file):
    """åŠ è½½è¯„è®ºæ•°æ®"""
    print("ğŸ“– æ­£åœ¨è¯»å–è¯„è®ºæ•°æ®...")
    reviews = []
    
    with open(review_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                review = json.loads(line.strip())
                reviews.append(review)
            except json.JSONDecodeError as e:
                if line_num % 10000 == 0:
                    print(f"âš ï¸  è­¦å‘Š: ç¬¬ {line_num} è¡Œè§£æå¤±è´¥")
                continue
    
    print(f"âœ… è¯»å–äº† {len(reviews)} æ¡è¯„è®º")
    return reviews

def preprocess_reviews(reviews, min_user_interactions=5, min_item_interactions=5):
    """é¢„å¤„ç†è¯„è®ºæ•°æ®"""
    print("\nğŸ”§ å¼€å§‹é¢„å¤„ç†è¯„è®ºæ•°æ®...")
    
    df = pd.DataFrame(reviews)
    print(f"åŸå§‹æ•°æ®: {len(df)} æ¡äº¤äº’")
    
    # ä½¿ç”¨ parent_asin ä½œä¸º item_id
    df['item_id'] = df['parent_asin']
    
    # å¤„ç†æ—¶é—´æˆ³
    if 'timestamp' in df.columns:
        df['timestamp'] = df['timestamp'] / 1000
    else:
        df['timestamp'] = range(len(df))
    
    # è¿‡æ»¤ç¼ºå¤±å€¼
    df = df.dropna(subset=['user_id', 'item_id', 'timestamp'])
    print(f"å»é™¤ç¼ºå¤±å€¼å: {len(df)} æ¡äº¤äº’")
    
    # è¿­ä»£è¿‡æ»¤
    print(f"\nğŸ”„ å¼€å§‹è¿­ä»£è¿‡æ»¤...")
    prev_len = -1
    iteration = 0
    
    while len(df) != prev_len:
        iteration += 1
        prev_len = len(df)
        
        user_counts = df['user_id'].value_counts()
        valid_users = user_counts[user_counts >= min_user_interactions].index
        df = df[df['user_id'].isin(valid_users)]
        
        item_counts = df['item_id'].value_counts()
        valid_items = item_counts[item_counts >= min_item_interactions].index
        df = df[df['item_id'].isin(valid_items)]
        
        print(f"   è¿­ä»£ {iteration}: {len(df):,} æ¡äº¤äº’, {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“")
    
    # æŒ‰ç”¨æˆ·å’Œæ—¶é—´æ’åº
    df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)
    
    print(f"\nâœ… é¢„å¤„ç†å®Œæˆï¼")
    print(f"   æœ€ç»ˆç”¨æˆ·æ•°: {df['user_id'].nunique():,}")
    print(f"   æœ€ç»ˆç‰©å“æ•°: {df['item_id'].nunique():,}")
    print(f"   æœ€ç»ˆäº¤äº’æ•°: {len(df):,}")
    
    return df

def split_data_by_user(df, max_seq_length=50):
    """
    ğŸ”‘ æ ¸å¿ƒå‡½æ•°ï¼šç»Ÿä¸€åˆ’åˆ†æ•°æ®
    ä¸ºæ¯ä¸ªç”¨æˆ·çš„äº¤äº’æ‰“ä¸Šæ ‡ç­¾ï¼štrain/valid/test
    """
    print(f"\nğŸ”ª æ­£åœ¨ç»Ÿä¸€åˆ’åˆ†æ•°æ®é›†...")
    print(f"   ç­–ç•¥: Leave-one-out per user")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_length}")
    
    # ä¸ºæ¯æ¡äº¤äº’æ·»åŠ  split æ ‡ç­¾
    df['split'] = 'train'  # é»˜è®¤éƒ½æ˜¯è®­ç»ƒé›†
    
    user_groups = df.groupby('user_id')
    print(f"   ç”¨æˆ·æ•°: {len(user_groups)}")
    
    stats = {
        'total_users': 0,
        'valid_users': 0,
        'test_users': 0,
        'skipped_users': 0,
    }
    
    for user_id, group in user_groups:
        stats['total_users'] += 1
        indices = group.index.tolist()
        n = len(indices)
        
        if n < 3:
            # äº¤äº’å¤ªå°‘ï¼Œå…¨éƒ¨æ ‡è®°ä¸ºè®­ç»ƒé›†ï¼ˆä½†ä¼šè¢«åç»­è¿‡æ»¤æ‰ï¼‰
            stats['skipped_users'] += 1
            continue
        
        # ğŸ”‘ ç»Ÿä¸€æ ‡è®°ç­–ç•¥ï¼š
        # - æœ€åä¸€ä¸ªäº¤äº’ â†’ test
        # - å€’æ•°ç¬¬äºŒä¸ªäº¤äº’ â†’ valid
        # - å…¶ä½™ â†’ train
        
        df.loc[indices[-1], 'split'] = 'test'
        df.loc[indices[-2], 'split'] = 'valid'
        # indices[:-2] è‡ªåŠ¨ä¿æŒä¸º 'train'
        
        stats['valid_users'] += 1
        stats['test_users'] += 1
    
    print(f"\nâœ… æ•°æ®åˆ’åˆ†å®Œæˆ:")
    print(f"   æ€»ç”¨æˆ·æ•°: {stats['total_users']:,}")
    print(f"   æœ‰æ•ˆç”¨æˆ·æ•°: {stats['valid_users']:,}")
    print(f"   è·³è¿‡ç”¨æˆ·: {stats['skipped_users']:,}")
    
    # ç»Ÿè®¡å„ä¸ªé›†åˆçš„å¤§å°
    train_count = len(df[df['split'] == 'train'])
    valid_count = len(df[df['split'] == 'valid'])
    test_count = len(df[df['split'] == 'test'])
    
    print(f"\n   äº¤äº’åˆ†å¸ƒ:")
    print(f"   è®­ç»ƒé›†: {train_count:,} æ¡")
    print(f"   éªŒè¯é›†: {valid_count:,} æ¡")
    print(f"   æµ‹è¯•é›†: {test_count:,} æ¡")
    
    return df

def save_unified_data(df, output_dir, dataset_name='Instruments2023'):
    """
    ä¿å­˜ç»Ÿä¸€åˆ’åˆ†çš„æ•°æ®
    1. RecBole æ ¼å¼çš„ .inter æ–‡ä»¶ï¼ˆå¸¦ split æ ‡ç­¾ï¼‰
    2. åˆ’åˆ†ä¿¡æ¯ JSON
    """
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»Ÿä¸€æ ¼å¼æ•°æ®...")
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ä¿å­˜å®Œæ•´çš„ .inter æ–‡ä»¶ï¼ˆåŒ…å« split åˆ—ï¼‰
    inter_file = os.path.join(output_dir, f'{dataset_name}.inter')
    
    with open(inter_file, 'w', encoding='utf-8') as f:
        # è¡¨å¤´ï¼ˆå¢åŠ  split å­—æ®µï¼‰
        f.write('user_id:token\titem_id:token\trating:float\ttimestamp:float\tsplit:token\n')
        
        for _, row in df.iterrows():
            user_id = str(row['user_id'])
            item_id = str(row['item_id'])
            rating = row.get('rating', 5.0)
            timestamp = row['timestamp']
            split = row['split']
            f.write(f"{user_id}\t{item_id}\t{rating}\t{timestamp}\t{split}\n")
    
    print(f"âœ… å·²ä¿å­˜äº¤äº’æ–‡ä»¶: {inter_file}")
    
    # 2. ä¿å­˜åˆ’åˆ†ä¿¡æ¯ï¼ˆæ–¹ä¾¿åç»­ä½¿ç”¨ï¼‰
    split_info = {
        'train': df[df['split'] == 'train'].index.tolist(),
        'valid': df[df['split'] == 'valid'].index.tolist(),
        'test': df[df['split'] == 'test'].index.tolist(),
    }
    
    split_file = os.path.join(output_dir, f'{dataset_name}.split.json')
    with open(split_file, 'w', encoding='utf-8') as f:
        json.dump(split_info, f)
    print(f"âœ… å·²ä¿å­˜åˆ’åˆ†ä¿¡æ¯: {split_file}")
    
    # 3. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'dataset_name': dataset_name,
        'num_users': int(df['user_id'].nunique()),
        'num_items': int(df['item_id'].nunique()),
        'num_interactions': int(len(df)),
        'train_interactions': int(len(df[df['split'] == 'train'])),
        'valid_interactions': int(len(df[df['split'] == 'valid'])),
        'test_interactions': int(len(df[df['split'] == 'test'])),
        'sparsity': float(1 - len(df) / (df['user_id'].nunique() * df['item_id'].nunique())),
    }
    
    stats_file = os.path.join(output_dir, 'dataset_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ… å·²ä¿å­˜ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
    
    return inter_file, split_file

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸµ ç»Ÿä¸€æ•°æ®é¢„å¤„ç†å·¥å…· - Musical Instruments 2023")
    print("=" * 70)
    print(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç”¨æˆ·: YYYYXL1004")
    print("=" * 70)
    
    # é…ç½®
    BASE_DIR = './dataset/Instruments2023'
    REVIEW_FILE = os.path.join(BASE_DIR, 'Musical_Instruments.jsonl')
    OUTPUT_DIR = BASE_DIR
    DATASET_NAME = 'Instruments2023'
    
    MIN_USER_INTERACTIONS = 5
    MIN_ITEM_INTERACTIONS = 5
    MAX_SEQ_LENGTH = 50
    
    print(f"\nğŸ“‚ è¾“å…¥æ–‡ä»¶: {REVIEW_FILE}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"\nâš™ï¸  å‚æ•°:")
    print(f"   æœ€å°‘ç”¨æˆ·äº¤äº’: {MIN_USER_INTERACTIONS}")
    print(f"   æœ€å°‘ç‰©å“äº¤äº’: {MIN_ITEM_INTERACTIONS}")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {MAX_SEQ_LENGTH}")
    
    if not os.path.exists(REVIEW_FILE):
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {REVIEW_FILE}")
        return
    
    # æ­¥éª¤ 1: åŠ è½½æ•°æ®
    reviews = load_reviews(REVIEW_FILE)
    
    # æ­¥éª¤ 2: é¢„å¤„ç†
    df = preprocess_reviews(reviews, MIN_USER_INTERACTIONS, MIN_ITEM_INTERACTIONS)
    
    # æ­¥éª¤ 3: ç»Ÿä¸€åˆ’åˆ†
    df = split_data_by_user(df, MAX_SEQ_LENGTH)
    
    # æ­¥éª¤ 4: ä¿å­˜
    inter_file, split_file = save_unified_data(df, OUTPUT_DIR, DATASET_NAME)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ ç»Ÿä¸€æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   1. {inter_file}")
    print(f"      - RecBole æ ¼å¼äº¤äº’æ–‡ä»¶ï¼ˆå« split æ ‡ç­¾ï¼‰")
    print(f"   2. {split_file}")
    print(f"      - æ•°æ®åˆ’åˆ†ä¿¡æ¯ï¼ˆtrain/valid/test ç´¢å¼•ï¼‰")
    print(f"   3. {OUTPUT_DIR}/dataset_stats.json")
    print(f"      - æ•°æ®ç»Ÿè®¡ä¿¡æ¯")
    
    print(f"\nâœ¨ ä¸‹ä¸€æ­¥:")
    print(f"   è¿è¡Œ: python train_sasrec_unified.py")
    print("=" * 70)

if __name__ == '__main__':
    main()