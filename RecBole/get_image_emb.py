#!/usr/bin/env python3
"""
å›¾åƒEmbeddingæå–è„šæœ¬ - ç”¨CLIPæå–å›¾åƒembedding

å‰ç½®æ¡ä»¶: å›¾ç‰‡åº”å·²ç”± prepare_data_mm.py ä¸‹è½½åˆ° dataset/{dataset}/images/ ç›®å½•

æµç¨‹:
1. è¯»å– emb_map.json è·å– item é¡ºåº (index 0=[PAD], 1~N=çœŸå®item)
2. æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å·²ä¸‹è½½ (ç¼ºå¤±æ—¶å°è¯•ä» meta è¡¥å……ä¸‹è½½)
3. ç”¨ CLIP ViT-L/14 æå–å›¾åƒembedding
4. ä¿å­˜ä¸º .npy æ–‡ä»¶, shape=(N_items, 768)

ç”¨æ³•:
    python get_image_emb.py --dataset Instrument2018_MM

ä¾èµ–:
    pip install clip-by-openai pillow tqdm
    æˆ–è€…: pip install git+https://github.com/openai/CLIP.git
"""

import os
import json
import argparse
import numpy as np
import torch
from PIL import Image
from tqdm import tqdm
import requests
from io import BytesIO


def download_image(url, save_path, timeout=10):
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
    try:
        response = requests.get(url, stream=True, timeout=timeout)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except Exception as e:
        return False


def main(args):
    dataset_dir = os.path.join(args.data_root, args.dataset)
    image_dir = os.path.join(dataset_dir, "images")
    os.makedirs(image_dir, exist_ok=True)

    # 1. åŠ è½½ item é¡ºåº
    emb_map_path = os.path.join(dataset_dir, f"{args.dataset}.emb_map.json")
    with open(emb_map_path, 'r') as f:
        emb_map = json.load(f)

    # id2asin: index -> asin (è·³è¿‡ [PAD])
    id2asin = {}
    for asin, idx in emb_map.items():
        if asin != "[PAD]":
            id2asin[idx] = asin
    n_items = len(id2asin)
    print(f"å…± {n_items} ä¸ª item (ä¸å«PAD)")

    # 2. æ£€æŸ¥å›¾ç‰‡ (å›¾ç‰‡åº”å·²ç”± prepare_data_mm.py ä¸‹è½½åˆ° images/ ç›®å½•)
    print(f"\nğŸ“‚ æ£€æŸ¥å›¾ç‰‡ç›®å½•: {image_dir}")
    found = 0
    missing = 0
    for idx in range(1, n_items + 1):
        asin = id2asin[idx]
        if os.path.exists(os.path.join(image_dir, f"{asin}.jpg")):
            found += 1
        else:
            missing += 1
    print(f"  å·²æœ‰å›¾ç‰‡: {found}, ç¼ºå¤±: {missing}")
    if missing > 0:
        print(f"  âš ï¸  å¦‚æœä½¿ç”¨ prepare_data_mm.py ç”Ÿæˆçš„æ•°æ®é›†ï¼Œä¸åº”æœ‰ç¼ºå¤±ã€‚")
        print(f"     å°è¯•ä» meta è¡¥å……ä¸‹è½½...")
        # fallback: ä» meta è¡¥ä¸‹è½½
        meta_path = os.path.join(dataset_dir, args.meta_file)
        asin2url = {}
        with open(meta_path, 'r') as f:
            for line in f:
                d = json.loads(line.strip())
                asin = d.get('asin', '')
                urls = d.get('imageURLHighRes', [])
                if urls:
                    asin2url[asin] = urls[0]
        for idx in tqdm(range(1, n_items + 1), desc="è¡¥å……ä¸‹è½½"):
            asin = id2asin[idx]
            save_path = os.path.join(image_dir, f"{asin}.jpg")
            if not os.path.exists(save_path) and asin in asin2url:
                download_image(asin2url[asin], save_path)

    # 3. ç”¨ CLIP æå– embedding
    print(f"\nğŸ”§ åŠ è½½ CLIP æ¨¡å‹: {args.clip_model}")
    try:
        from clip import clip
    except ImportError:
        print("è¯·å®‰è£… CLIP: pip install git+https://github.com/openai/CLIP.git")
        return

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load(args.clip_model, device=device,
                                   download_root=args.clip_cache_dir)
    model.eval()

    print(f"\nğŸ–¼ï¸  æå–å›¾åƒembedding...")
    embeddings = []
    missing_items = []

    with torch.no_grad():
        for idx in tqdm(range(1, n_items + 1), desc="æå–embedding"):
            asin = id2asin[idx]
            image_path = os.path.join(image_dir, f"{asin}.jpg")

            if os.path.exists(image_path):
                try:
                    image = Image.open(image_path).convert("RGB")
                    image_input = preprocess(image).unsqueeze(0).to(device)
                    feat = model.encode_image(image_input)
                    embeddings.append(feat[0].cpu().float())
                    continue
                except Exception as e:
                    pass

            # æ²¡æœ‰å›¾ç‰‡æˆ–åŠ è½½å¤±è´¥ â†’ æŠ¥é”™ (æ–°æ•°æ®é›†åº”ä¿è¯æ‰€æœ‰iteméƒ½æœ‰å›¾ç‰‡)
            missing_items.append((idx, asin))
            embeddings.append(torch.zeros(768))

    embeddings = torch.stack(embeddings, dim=0).numpy()

    if missing_items:
        print(f"âš ï¸  è­¦å‘Š: {len(missing_items)} ä¸ªitemç¼ºå°‘å›¾ç‰‡!")
        print(f"   å¦‚æœä½¿ç”¨ prepare_data_mm.py ç”Ÿæˆçš„æ•°æ®é›†ï¼Œä¸åº”å‡ºç°æ­¤æƒ…å†µã€‚")
        print(f"   ç¼ºå¤±item: {missing_items[:10]}...")  # åªæ‰“å°å‰10ä¸ª

    print(f"\nğŸ“Š Embedding shape: {embeddings.shape}")

    # 4. ä¿å­˜
    save_name = f"{args.dataset}_clip_image_{embeddings.shape[1]}.npy"
    save_path = os.path.join(dataset_dir, save_name)
    np.save(save_path, embeddings)
    print(f"âœ… å·²ä¿å­˜: {save_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æå–å›¾åƒEmbedding (CLIP)")
    parser.add_argument("--dataset", type=str, default="Instrument2018_5090")
    parser.add_argument("--data_root", type=str, default="./dataset")
    parser.add_argument("--meta_file", type=str, default="meta_Musical_Instruments.json",
                        help="meta JSON æ–‡ä»¶å")
    parser.add_argument("--clip_model", type=str, default="ViT-L/14",
                        help="CLIPæ¨¡å‹åç§°")
    parser.add_argument("--clip_cache_dir", type=str, default=None,
                        help="CLIPæ¨¡å‹ç¼“å­˜ç›®å½•")
    args = parser.parse_args()
    main(args)
