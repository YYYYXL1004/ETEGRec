#!/usr/bin/env python3
"""
æ•°æ®å‡†å¤‡è„šæœ¬ - ä»Amazon 2014åŸå§‹æ•°æ®ç”ŸæˆRecBole .interæ–‡ä»¶å’ŒETEGRecæ‰€éœ€çš„train/valid/test.jsonl

åŠŸèƒ½:
1. è¯»å–Amazon 2014åŸå§‹JSONæ•°æ®
2. æ•°æ®æ¸…æ´—å’Œè¿‡æ»¤ (æœ€å°‘5æ¬¡äº¤äº’)
3. ç»Ÿä¸€åˆ’åˆ†train/valid/test (leave-one-out)
4. ç”ŸæˆRecBoleæ ¼å¼çš„.interæ–‡ä»¶ (å¸¦splitæ ‡ç­¾)
5. ç”ŸæˆETEGRecæ ¼å¼çš„train/valid/test.jsonlæ–‡ä»¶

æ•°æ®åˆ’åˆ†ç­–ç•¥:
- æ¯ä¸ªç”¨æˆ·æœ€åä¸€ä¸ªäº¤äº’ â†’ test
- æ¯ä¸ªç”¨æˆ·å€’æ•°ç¬¬äºŒä¸ªäº¤äº’ â†’ valid
- å…¶ä½™äº¤äº’ â†’ train
"""

import json
import pandas as pd
import os
from collections import defaultdict
from tqdm import tqdm


def load_and_preprocess(review_file, min_interactions=5):
    """åŠ è½½å¹¶é¢„å¤„ç†Amazon 2014è¯„è®ºæ•°æ®"""
    print(f"ğŸ“– è¯»å–æ•°æ®: {review_file}")
    
    # è¯»å–JSON (2014ç‰ˆæœ¬æ˜¯ä¸€è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼Œä½†ä¸æ˜¯æ ‡å‡†JSONL)
    reviews = []
    with open(review_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                reviews.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    
    df = pd.DataFrame(reviews)
    print(f"åŸå§‹æ•°æ®: {len(df):,} æ¡äº¤äº’")
    
    # 2014ç‰ˆæœ¬å­—æ®µæ˜ å°„
    df = df.rename(columns={
        'reviewerID': 'user_id',
        'asin': 'item_id',
        'overall': 'rating',
        'unixReviewTime': 'timestamp'
    })
    
    # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
    df = df.dropna(subset=['user_id', 'item_id', 'timestamp'])
    
    # 2014ç‰ˆæœ¬çš„timestampå·²ç»æ˜¯ç§’çº§ï¼Œä¸éœ€è¦é™¤ä»¥1000
    df['timestamp'] = df['timestamp'].astype(float)
    
    # è¿­ä»£è¿‡æ»¤ (ä¿ç•™è‡³å°‘min_interactionsæ¬¡äº¤äº’çš„ç”¨æˆ·å’Œç‰©å“)
    print(f"ğŸ”„ è¿­ä»£è¿‡æ»¤ (æœ€å°‘{min_interactions}æ¬¡äº¤äº’)...")
    prev_len = -1
    iteration = 0
    while len(df) != prev_len:
        iteration += 1
        prev_len = len(df)
        user_counts = df['user_id'].value_counts()
        df = df[df['user_id'].isin(user_counts[user_counts >= min_interactions].index)]
        item_counts = df['item_id'].value_counts()
        df = df[df['item_id'].isin(item_counts[item_counts >= min_interactions].index)]
        print(f"  è¿­ä»£{iteration}: {len(df):,} æ¡, {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“")
    
    df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)
    print(f"âœ… é¢„å¤„ç†å®Œæˆ: {df['user_id'].nunique():,} ç”¨æˆ·, {df['item_id'].nunique():,} ç‰©å“, {len(df):,} äº¤äº’\n")
    return df


def split_data(df):
    """ç»Ÿä¸€åˆ’åˆ†train/valid/test (leave-one-out)"""
    print("ğŸ”ª åˆ’åˆ†æ•°æ®é›† (leave-one-out)...")
    df['split'] = 'train'
    
    for user_id, group in df.groupby('user_id'):
        indices = group.index.tolist()
        if len(indices) >= 3:
            df.loc[indices[-1], 'split'] = 'test'
            df.loc[indices[-2], 'split'] = 'valid'
    
    train_cnt = len(df[df['split'] == 'train'])
    valid_cnt = len(df[df['split'] == 'valid'])
    test_cnt = len(df[df['split'] == 'test'])
    print(f"âœ… åˆ’åˆ†å®Œæˆ: train={train_cnt:,}, valid={valid_cnt:,}, test={test_cnt:,}\n")
    return df


def save_inter_file(df, output_dir, dataset_name):
    """ä¿å­˜RecBoleæ ¼å¼çš„.interæ–‡ä»¶ (å¸¦splitæ ‡ç­¾)"""
    os.makedirs(output_dir, exist_ok=True)
    inter_file = os.path.join(output_dir, f'{dataset_name}.inter')
    
    with open(inter_file, 'w', encoding='utf-8') as f:
        f.write('user_id:token\titem_id:token\trating:float\ttimestamp:float\tsplit:token\n')
        for _, row in df.iterrows():
            f.write(f"{row['user_id']}\t{row['item_id']}\t{row['rating']}\t{row['timestamp']}\t{row['split']}\n")
    
    print(f"âœ… å·²ä¿å­˜: {inter_file}")
    return inter_file


def build_sequences(df, max_seq_length=50):
    """æ ¹æ®splitæ ‡ç­¾æ„å»ºåºåˆ—æ•°æ®"""
    print(f"ğŸ”¨ æ„å»ºåºåˆ— (max_length={max_seq_length})...")
    
    df = df.sort_values(['user_id', 'timestamp'])
    train_seqs, valid_seqs, test_seqs = [], [], []
    
    for user_id, group in tqdm(df.groupby('user_id'), desc="æ„å»ºåºåˆ—"):
        items = group['item_id'].tolist()
        splits = group['split'].tolist()
        
        if len(items) < 3:
            continue
        
        # æ‰¾åˆ°validå’Œtestçš„ä½ç½®
        valid_idx = next((i for i, s in enumerate(splits) if s == 'valid'), None)
        test_idx = next((i for i, s in enumerate(splits) if s == 'test'), None)
        
        if valid_idx is None or test_idx is None:
            continue
        
        # è®­ç»ƒé›†: å¢é‡åºåˆ— (æ¯ä¸ªtrainä½ç½®ç”Ÿæˆä¸€ä¸ªæ ·æœ¬)
        for i in range(1, valid_idx):
            history = items[:i][-max_seq_length:]
            train_seqs.append({
                'user_id': user_id,
                'target_id': items[i],
                'inter_history': history
            })
        
        # éªŒè¯é›†: validä½ç½®çš„æ ·æœ¬
        valid_history = items[:valid_idx][-max_seq_length:]
        valid_seqs.append({
            'user_id': user_id,
            'target_id': items[valid_idx],
            'inter_history': valid_history
        })
        
        # æµ‹è¯•é›†: testä½ç½®çš„æ ·æœ¬
        test_history = items[:test_idx][-max_seq_length:]
        test_seqs.append({
            'user_id': user_id,
            'target_id': items[test_idx],
            'inter_history': test_history
        })
    
    print(f"âœ… åºåˆ—æ„å»ºå®Œæˆ: train={len(train_seqs):,}, valid={len(valid_seqs):,}, test={len(test_seqs):,}\n")
    return train_seqs, valid_seqs, test_seqs


def save_jsonl(data, output_file):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"âœ… å·²ä¿å­˜: {output_file}")


def main():
    print("=" * 70)
    print("ğŸµ æ•°æ®å‡†å¤‡ - Amazon Musical Instruments 2014")
    print("=" * 70)
    
    # é…ç½®
    BASE_DIR = './dataset/Instrument2014'
    REVIEW_FILE = os.path.join(BASE_DIR, 'reviews_Musical_Instruments.json')
    DATASET_NAME = 'Instrument2014'
    MIN_INTERACTIONS = 5
    MAX_SEQ_LENGTH = 50
    
    if not os.path.exists(REVIEW_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {REVIEW_FILE}")
        return
    
    # æ­¥éª¤1: åŠ è½½å’Œé¢„å¤„ç†
    df = load_and_preprocess(REVIEW_FILE, MIN_INTERACTIONS)
    
    # æ­¥éª¤2: åˆ’åˆ†æ•°æ®é›†
    df = split_data(df)
    
    # æ­¥éª¤3: ä¿å­˜.interæ–‡ä»¶
    save_inter_file(df, BASE_DIR, DATASET_NAME)
    
    # æ­¥éª¤4: æ„å»ºåºåˆ—
    train_seqs, valid_seqs, test_seqs = build_sequences(df, MAX_SEQ_LENGTH)
    
    # æ­¥éª¤5: ä¿å­˜JSONLæ–‡ä»¶
    print("ğŸ’¾ ä¿å­˜JSONLæ–‡ä»¶...")
    save_jsonl(train_seqs, os.path.join(BASE_DIR, f'{DATASET_NAME}.train.jsonl'))
    save_jsonl(valid_seqs, os.path.join(BASE_DIR, f'{DATASET_NAME}.valid.jsonl'))
    save_jsonl(test_seqs, os.path.join(BASE_DIR, f'{DATASET_NAME}.test.jsonl'))
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'num_users': int(df['user_id'].nunique()),
        'num_items': int(df['item_id'].nunique()),
        'num_interactions': int(len(df)),
        'train_interactions': int(len(df[df['split'] == 'train'])),
        'valid_interactions': int(len(df[df['split'] == 'valid'])),
        'test_interactions': int(len(df[df['split'] == 'test'])),
        'train_sequences': len(train_seqs),
        'valid_sequences': len(valid_seqs),
        'test_sequences': len(test_seqs),
    }
    stats_file = os.path.join(BASE_DIR, 'dataset_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ… å·²ä¿å­˜: {stats_file}")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆ!")
    print("=" * 70)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. {DATASET_NAME}.inter - RecBoleæ ¼å¼ (å¸¦splitæ ‡ç­¾)")
    print(f"  2. {DATASET_NAME}.train.jsonl - ETEGRecè®­ç»ƒé›†")
    print(f"  3. {DATASET_NAME}.valid.jsonl - ETEGRecéªŒè¯é›†")
    print(f"  4. {DATASET_NAME}.test.jsonl - ETEGRecæµ‹è¯•é›†")
    print(f"  5. dataset_stats.json - æ•°æ®ç»Ÿè®¡")


if __name__ == '__main__':
    main()
