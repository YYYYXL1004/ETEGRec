import json
import numpy as np
import os
from collections import defaultdict, Counter
from tqdm import tqdm

class DatasetAnalyzer:
    """
    æ•°æ®é›†åˆ†æå·¥å…·
    """
    
    def __init__(self, dataset_dir, dataset_name):
        self.dataset_dir = dataset_dir
        self.dataset_name = dataset_name
        self.train_file = os.path.join(dataset_dir, f'{dataset_name}.train.jsonl')
        self.valid_file = os.path.join(dataset_dir, f'{dataset_name}.valid.jsonl')
        self.test_file = os.path.join(dataset_dir, f'{dataset_name}.test.jsonl')
        self.map_file = os.path.join(dataset_dir, f'{dataset_name}.emb_map.json')
        self.emb_file = os.path.join(dataset_dir, f'{dataset_name}_emb_256.npy')
        
        self.train_data = []
        self.valid_data = []
        self.test_data = []
        self.item2id = {}
        self.embeddings = None
    
    def load_data(self):
        """
        åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {self.dataset_name}")
        print(f"{'='*70}")
        
        # åŠ è½½è®­ç»ƒé›†
        if os.path.exists(self.train_file):
            print(f"ğŸ“– åŠ è½½è®­ç»ƒé›†: {self.train_file}")
            with open(self.train_file, 'r', encoding='utf-8') as f:
                for line in f:
                    self.train_data.append(json.loads(line.strip()))
            print(f"   âœ… è®­ç»ƒé›†: {len(self.train_data)} æ¡åºåˆ—")
        else:
            print(f"   âŒ è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åŠ è½½éªŒè¯é›†
        if os.path.exists(self.valid_file):
            print(f"ğŸ“– åŠ è½½éªŒè¯é›†: {self.valid_file}")
            with open(self.valid_file, 'r', encoding='utf-8') as f:
                for line in f:
                    self.valid_data.append(json.loads(line.strip()))
            print(f"   âœ… éªŒè¯é›†: {len(self.valid_data)} æ¡åºåˆ—")
        else:
            print(f"   âŒ éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åŠ è½½æµ‹è¯•é›†
        if os.path.exists(self.test_file):
            print(f"ğŸ“– åŠ è½½æµ‹è¯•é›†: {self.test_file}")
            with open(self.test_file, 'r', encoding='utf-8') as f:
                for line in f:
                    self.test_data.append(json.loads(line.strip()))
            print(f"   âœ… æµ‹è¯•é›†: {len(self.test_data)} æ¡åºåˆ—")
        else:
            print(f"   âŒ æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åŠ è½½æ˜ å°„
        if os.path.exists(self.map_file):
            print(f"ğŸ“– åŠ è½½æ˜ å°„æ–‡ä»¶: {self.map_file}")
            with open(self.map_file, 'r', encoding='utf-8') as f:
                self.item2id = json.load(f)
            print(f"   âœ… æ˜ å°„: {len(self.item2id)} ä¸ªç‰©å“")
        else:
            print(f"   âŒ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åŠ è½½åµŒå…¥
        if os.path.exists(self.emb_file):
            print(f"ğŸ“– åŠ è½½åµŒå…¥æ–‡ä»¶: {self.emb_file}")
            self.embeddings = np.load(self.emb_file)
            print(f"   âœ… åµŒå…¥å½¢çŠ¶: {self.embeddings.shape}")
            print(f"   âœ… åµŒå…¥å¤§å°: {self.embeddings.nbytes / 1024 / 1024:.2f} MB")
        else:
            print(f"   âŒ åµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨")
    
    def analyze_basic_stats(self):
        """
        åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
        print(f"{'='*70}")
        
        all_data = {
            'è®­ç»ƒé›†': self.train_data,
            'éªŒè¯é›†': self.valid_data,
            'æµ‹è¯•é›†': self.test_data
        }
        
        for name, data in all_data.items():
            if len(data) == 0:
                print(f"\n{name}: æ— æ•°æ®")
                continue
            
            # ç»Ÿè®¡å†å²é•¿åº¦
            hist_lens = [len(seq['inter_history']) for seq in data]
            
            # ç»Ÿè®¡å”¯ä¸€ç”¨æˆ·
            unique_users = len(set(seq['user_id'] for seq in data))
            
            # ç»Ÿè®¡å”¯ä¸€ç‰©å“
            all_items = set()
            for seq in data:
                all_items.update(seq['inter_history'])
                all_items.add(seq['target_id'])
            
            print(f"\n{name}:")
            print(f"  åºåˆ—æ•°é‡: {len(data):,}")
            print(f"  å”¯ä¸€ç”¨æˆ·æ•°: {unique_users:,}")
            print(f"  å”¯ä¸€ç‰©å“æ•°: {len(all_items):,}")
            print(f"  å†å²é•¿åº¦:")
            print(f"    å¹³å‡: {np.mean(hist_lens):.2f}")
            print(f"    ä¸­ä½æ•°: {np.median(hist_lens):.2f}")
            print(f"    æœ€å°: {np.min(hist_lens)}")
            print(f"    æœ€å¤§: {np.max(hist_lens)}")
            print(f"    æ ‡å‡†å·®: {np.std(hist_lens):.2f}")
    
    def analyze_user_distribution(self):
        """
        ç”¨æˆ·åˆ†å¸ƒåˆ†æ
        """
        print(f"\n{'='*70}")
        print(f"ğŸ‘¥ ç”¨æˆ·åˆ†å¸ƒåˆ†æ")
        print(f"{'='*70}")
        
        all_data = self.train_data + self.valid_data + self.test_data
        
        if len(all_data) == 0:
            print("æ— æ•°æ®å¯åˆ†æ")
            return
        
        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„äº¤äº’æ¬¡æ•°
        user_interactions = defaultdict(int)
        for seq in all_data:
            user_interactions[seq['user_id']] += len(seq['inter_history']) + 1
        
        interaction_counts = list(user_interactions.values())
        
        print(f"\næ€»ç”¨æˆ·æ•°: {len(user_interactions):,}")
        print(f"æ€»äº¤äº’æ•°: {sum(interaction_counts):,}")
        print(f"æ¯ç”¨æˆ·å¹³å‡äº¤äº’æ•°: {np.mean(interaction_counts):.2f}")
        print(f"æ¯ç”¨æˆ·ä¸­ä½æ•°äº¤äº’æ•°: {np.median(interaction_counts):.2f}")
        print(f"æ¯ç”¨æˆ·æœ€å°‘äº¤äº’æ•°: {np.min(interaction_counts)}")
        print(f"æ¯ç”¨æˆ·æœ€å¤šäº¤äº’æ•°: {np.max(interaction_counts)}")
        
        # åˆ†å¸ƒç»Ÿè®¡
        print(f"\nç”¨æˆ·äº¤äº’æ•°åˆ†å¸ƒ:")
        bins = [0, 5, 10, 20, 50, 100, float('inf')]
        labels = ['1-5', '6-10', '11-20', '21-50', '51-100', '100+']
        
        for i, (low, high, label) in enumerate(zip(bins[:-1], bins[1:], labels)):
            count = sum(1 for c in interaction_counts if low < c <= high)
            pct = count / len(user_interactions) * 100
            print(f"  {label:>10}æ¬¡: {count:>6} ç”¨æˆ· ({pct:>5.2f}%)")
    
    def analyze_item_distribution(self):
        """
        ç‰©å“åˆ†å¸ƒåˆ†æ
        """
        print(f"\n{'='*70}")
        print(f"ğŸ¸ ç‰©å“åˆ†å¸ƒåˆ†æ")
        print(f"{'='*70}")
        
        all_data = self.train_data + self.valid_data + self.test_data
        
        if len(all_data) == 0:
            print("æ— æ•°æ®å¯åˆ†æ")
            return
        
        # ç»Ÿè®¡æ¯ä¸ªç‰©å“å‡ºç°çš„æ¬¡æ•°
        item_counts = defaultdict(int)
        for seq in all_data:
            for item in seq['inter_history']:
                item_counts[item] += 1
            item_counts[seq['target_id']] += 1
        
        counts = list(item_counts.values())
        
        print(f"\næ€»ç‰©å“æ•°: {len(item_counts):,}")
        print(f"æ€»å‡ºç°æ¬¡æ•°: {sum(counts):,}")
        print(f"æ¯ç‰©å“å¹³å‡å‡ºç°æ¬¡æ•°: {np.mean(counts):.2f}")
        print(f"æ¯ç‰©å“ä¸­ä½æ•°å‡ºç°æ¬¡æ•°: {np.median(counts):.2f}")
        print(f"æ¯ç‰©å“æœ€å°‘å‡ºç°æ¬¡æ•°: {np.min(counts)}")
        print(f"æ¯ç‰©å“æœ€å¤šå‡ºç°æ¬¡æ•°: {np.max(counts)}")
        
        # åˆ†å¸ƒç»Ÿè®¡
        print(f"\nç‰©å“æµè¡Œåº¦åˆ†å¸ƒ:")
        bins = [0, 5, 10, 20, 50, 100, 500, float('inf')]
        labels = ['1-5', '6-10', '11-20', '21-50', '51-100', '101-500', '500+']
        
        for i, (low, high, label) in enumerate(zip(bins[:-1], bins[1:], labels)):
            count = sum(1 for c in counts if low < c <= high)
            pct = count / len(item_counts) * 100
            print(f"  {label:>10}æ¬¡: {count:>6} ç‰©å“ ({pct:>5.2f}%)")
        
        # Top çƒ­é—¨ç‰©å“
        print(f"\nTop 10 çƒ­é—¨ç‰©å“:")
        top_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        for i, (item, count) in enumerate(top_items, 1):
            print(f"  {i:2d}. {item}: {count:,} æ¬¡")
    
    def analyze_data_format(self):
        """
        æ•°æ®æ ¼å¼åˆ†æ
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“ æ•°æ®æ ¼å¼åˆ†æ")
        print(f"{'='*70}")
        
        all_data = {
            'è®­ç»ƒé›†': self.train_data,
            'éªŒè¯é›†': self.valid_data,
            'æµ‹è¯•é›†': self.test_data
        }
        
        for name, data in all_data.items():
            if len(data) == 0:
                continue
            
            print(f"\n{name}:")
            
            # æ£€æŸ¥å­—æ®µ
            sample = data[0]
            print(f"  å­—æ®µ: {list(sample.keys())}")
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è®°å½•éƒ½æœ‰ç›¸åŒå­—æ®µ
            all_keys = set()
            for seq in data:
                all_keys.update(seq.keys())
            print(f"  æ‰€æœ‰å­—æ®µ: {all_keys}")
            
            # æ£€æŸ¥å­—æ®µç±»å‹
            print(f"  å­—æ®µç±»å‹:")
            print(f"    user_id: {type(sample.get('user_id')).__name__}")
            print(f"    target_id: {type(sample.get('target_id')).__name__}")
            print(f"    inter_history: {type(sample.get('inter_history')).__name__} (é•¿åº¦: {len(sample.get('inter_history', []))})")
            
            # æ˜¾ç¤ºæ ·ä¾‹
            print(f"  æ ·ä¾‹ (å‰3æ¡):")
            for i, seq in enumerate(data[:3], 1):
                hist_preview = seq['inter_history'][:3]
                if len(seq['inter_history']) > 3:
                    hist_preview_str = str(hist_preview)[:-1] + ', ...]'
                else:
                    hist_preview_str = str(seq['inter_history'])
                print(f"    {i}. user_id={seq['user_id']}, target={seq['target_id']}, history={hist_preview_str}")
    
    def check_data_consistency(self):
        """
        æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        """
        print(f"\n{'='*70}")
        print(f"ğŸ” æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥")
        print(f"{'='*70}")
        
        issues = []
        
        # 1. æ£€æŸ¥ç‰©å“æ˜¯å¦éƒ½åœ¨æ˜ å°„ä¸­
        all_items = set()
        for data in [self.train_data, self.valid_data, self.test_data]:
            for seq in data:
                all_items.update(seq['inter_history'])
                all_items.add(seq['target_id'])
        
        missing_in_map = all_items - set(self.item2id.keys())
        if missing_in_map:
            issues.append(f"âŒ æœ‰ {len(missing_in_map)} ä¸ªç‰©å“ä¸åœ¨æ˜ å°„ä¸­")
            print(f"  ç¤ºä¾‹: {list(missing_in_map)[:5]}")
        else:
            print(f"âœ… æ‰€æœ‰ç‰©å“éƒ½åœ¨æ˜ å°„ä¸­")
        
        # 2. æ£€æŸ¥æ˜ å°„å’ŒåµŒå…¥æ˜¯å¦åŒ¹é…
        if self.embeddings is not None:
            if len(self.item2id) == self.embeddings.shape[0]:
                print(f"âœ… æ˜ å°„æ•°é‡ ({len(self.item2id)}) ä¸åµŒå…¥æ•°é‡ ({self.embeddings.shape[0]}) ä¸€è‡´")
            else:
                issues.append(f"âŒ æ˜ å°„æ•°é‡ ({len(self.item2id)}) ä¸åµŒå…¥æ•°é‡ ({self.embeddings.shape[0]}) ä¸ä¸€è‡´")
        
        # 3. æ£€æŸ¥ç”¨æˆ·åœ¨ä¸åŒé›†åˆä¸­çš„åˆ†å¸ƒ
        train_users = set(seq['user_id'] for seq in self.train_data)
        valid_users = set(seq['user_id'] for seq in self.valid_data)
        test_users = set(seq['user_id'] for seq in self.test_data)
        
        print(f"\nç”¨æˆ·åˆ†å¸ƒ:")
        print(f"  è®­ç»ƒé›†ç”¨æˆ·: {len(train_users):,}")
        print(f"  éªŒè¯é›†ç”¨æˆ·: {len(valid_users):,}")
        print(f"  æµ‹è¯•é›†ç”¨æˆ·: {len(test_users):,}")
        print(f"  éªŒè¯âˆ©æµ‹è¯•: {len(valid_users & test_users):,}")
        print(f"  è®­ç»ƒâˆ©éªŒè¯: {len(train_users & valid_users):,}")
        print(f"  è®­ç»ƒâˆ©æµ‹è¯•: {len(train_users & test_users):,}")
        
        # 4. æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå†å²
        empty_history = 0
        for data in [self.train_data, self.valid_data, self.test_data]:
            for seq in data:
                if len(seq['inter_history']) == 0:
                    empty_history += 1
        
        if empty_history > 0:
            issues.append(f"âŒ æœ‰ {empty_history} ä¸ªåºåˆ—çš„å†å²ä¸ºç©º")
        else:
            print(f"âœ… æ²¡æœ‰ç©ºå†å²åºåˆ—")
        
        # æ€»ç»“
        print(f"\n{'='*40}")
        if len(issues) == 0:
            print(f"âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼æ•°æ®ä¸€è‡´æ€§è‰¯å¥½")
        else:
            print(f"âš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues:
                print(f"  {issue}")
        print(f"{'='*40}")
    
    def run_full_analysis(self):
        """
        è¿è¡Œå®Œæ•´åˆ†æï¼ˆä¸ä¿å­˜åˆ°æ–‡ä»¶ï¼Œåªæ‰“å°ï¼‰
        """
        self.analyze_basic_stats()
        self.analyze_user_distribution()
        self.analyze_item_distribution()
        self.analyze_data_format()
        self.check_data_consistency()
    
    def generate_report(self, output_file=None):
        """
        ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        """
        if output_file is None:
            output_file = os.path.join(self.dataset_dir, f'{self.dataset_name}_analysis_report.txt')
        
        print(f"\n{'='*70}")
        print(f"ğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print(f"{'='*70}")
        
        import sys
        from io import StringIO
        
        # é‡å®šå‘è¾“å‡ºåˆ°å­—ç¬¦ä¸²
        old_stdout = sys.stdout
        sys.stdout = report_buffer = StringIO()
        
        # è¿è¡Œæ‰€æœ‰åˆ†æ
        self.run_full_analysis()
        
        # æ¢å¤è¾“å‡º
        sys.stdout = old_stdout
        report_content = report_buffer.getvalue()
        
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"æ•°æ®é›†åˆ†ææŠ¥å‘Š\n")
            f.write(f"æ•°æ®é›†: {self.dataset_name}\n")
            f.write(f"æ—¶é—´: 2025-11-14 08:57:57 UTC\n")
            f.write(f"ç”¨æˆ·: YYYYXL1004\n")
            f.write(f"{'='*70}\n\n")
            f.write(report_content)
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
        print(report_content)


def compare_datasets(dataset1_dir, dataset1_name, dataset2_dir, dataset2_name):
    """
    å¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›†
    """
    print(f"\n{'='*70}")
    print(f"ğŸ”„ å¯¹æ¯”æ•°æ®é›†")
    print(f"{'='*70}")
    print(f"æ•°æ®é›†1: {dataset1_name} ({dataset1_dir})")
    print(f"æ•°æ®é›†2: {dataset2_name} ({dataset2_dir})")
    
    # åŠ è½½ä¸¤ä¸ªæ•°æ®é›†
    analyzer1 = DatasetAnalyzer(dataset1_dir, dataset1_name)
    analyzer1.load_data()
    
    analyzer2 = DatasetAnalyzer(dataset2_dir, dataset2_name)
    analyzer2.load_data()
    
    # å¯¹æ¯”ç»Ÿè®¡
    print(f"\n{'='*70}")
    print(f"ğŸ“Š æ•°æ®é›†å¯¹æ¯”")
    print(f"{'='*70}")
    
    stats = {
        'è®­ç»ƒé›†åºåˆ—æ•°': (len(analyzer1.train_data), len(analyzer2.train_data)),
        'éªŒè¯é›†åºåˆ—æ•°': (len(analyzer1.valid_data), len(analyzer2.valid_data)),
        'æµ‹è¯•é›†åºåˆ—æ•°': (len(analyzer1.test_data), len(analyzer2.test_data)),
        'ç‰©å“æ˜ å°„æ•°': (len(analyzer1.item2id), len(analyzer2.item2id)),
    }
    
    if analyzer1.embeddings is not None and analyzer2.embeddings is not None:
        stats['åµŒå…¥å½¢çŠ¶'] = (analyzer1.embeddings.shape, analyzer2.embeddings.shape)
    
    print(f"\n{'æŒ‡æ ‡':<20} {'æ•°æ®é›†1':>15} {'æ•°æ®é›†2':>15} {'å·®å¼‚':>15}")
    print(f"{'-'*70}")
    
    for metric, (val1, val2) in stats.items():
        if isinstance(val1, tuple):
            diff = "N/A"
            print(f"{metric:<20} {str(val1):>15} {str(val2):>15} {diff:>15}")
        else:
            diff = val2 - val1
            diff_pct = (diff / val1 * 100) if val1 > 0 else 0
            print(f"{metric:<20} {val1:>15,} {val2:>15,} {diff:>+15,} ({diff_pct:+.1f}%)")
    
    # å¯¹æ¯”ç”¨æˆ·å’Œç‰©å“
    print(f"\nç”¨æˆ·å’Œç‰©å“åˆ†æ:")
    
    all_users1 = set()
    all_items1 = set()
    for data in [analyzer1.train_data, analyzer1.valid_data, analyzer1.test_data]:
        for seq in data:
            all_users1.add(seq['user_id'])
            all_items1.update(seq['inter_history'])
            all_items1.add(seq['target_id'])
    
    all_users2 = set()
    all_items2 = set()
    for data in [analyzer2.train_data, analyzer2.valid_data, analyzer2.test_data]:
        for seq in data:
            all_users2.add(seq['user_id'])
            all_items2.update(seq['inter_history'])
            all_items2.add(seq['target_id'])
    
    print(f"  ç”¨æˆ·æ•°: {len(all_users1):,} vs {len(all_users2):,}")
    print(f"  ç‰©å“æ•°: {len(all_items1):,} vs {len(all_items2):,}")
    print(f"  å…±åŒç”¨æˆ·: {len(all_users1 & all_users2):,}")
    print(f"  å…±åŒç‰©å“: {len(all_items1 & all_items2):,}")
    
    # å¯¹æ¯”å†å²é•¿åº¦åˆ†å¸ƒ
    print(f"\nå†å²é•¿åº¦åˆ†å¸ƒå¯¹æ¯”:")
    
    dataset_pairs = [
        ('è®­ç»ƒé›†', analyzer1.train_data, analyzer2.train_data),
        ('éªŒè¯é›†', analyzer1.valid_data, analyzer2.valid_data),
        ('æµ‹è¯•é›†', analyzer1.test_data, analyzer2.test_data)
    ]
    
    for dataset_name, data1, data2 in dataset_pairs:
        if len(data1) > 0 and len(data2) > 0:
            lens1 = [len(seq['inter_history']) for seq in data1]
            lens2 = [len(seq['inter_history']) for seq in data2]
            
            print(f"\n  {dataset_name}:")
            print(f"    å¹³å‡é•¿åº¦: {np.mean(lens1):.2f} vs {np.mean(lens2):.2f}")
            print(f"    ä¸­ä½æ•°: {np.median(lens1):.2f} vs {np.median(lens2):.2f}")
            print(f"    æœ€å¤§é•¿åº¦: {np.max(lens1)} vs {np.max(lens2)}")


def main():
    """
    ä¸»å‡½æ•°
    """
    print(f"{'='*70}")
    print(f"ğŸµ æ•°æ®é›†åˆ†æå·¥å…·")
    print(f"{'='*70}")
    print(f"å½“å‰æ—¶é—´: 2025-11-14 08:57:57 UTC")
    print(f"å½“å‰ç”¨æˆ·: YYYYXL1004")
    print(f"{'='*70}")
    
    # ============ é…ç½® ============
    # ä½ è‡ªå·±ç”Ÿæˆçš„æ•°æ®é›†
    MY_DATASET_DIR = './dataset/Instruments2023'
    MY_DATASET_NAME = 'Instruments2023'
    
    # ä½œè€…æä¾›çš„æ•°æ®é›†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    AUTHOR_DATASET_DIR = '../dataset/instrument'  # ä¿®æ”¹ä¸ºä½œè€…æ•°æ®é›†è·¯å¾„
    AUTHOR_DATASET_NAME = 'instrument'
    
    # ============ ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆ†æè‡ªå·±çš„æ•°æ®é›† ============
    print(f"\n{'#'*70}")
    print(f"# ç¬¬ä¸€éƒ¨åˆ†: åˆ†æè‡ªå·±ç”Ÿæˆçš„æ•°æ®é›†")
    print(f"{'#'*70}")
    
    my_analyzer = DatasetAnalyzer(MY_DATASET_DIR, MY_DATASET_NAME)
    my_analyzer.load_data()
    
    # æ‰“å°åˆ†æç»“æœåˆ°æ§åˆ¶å°
    my_analyzer.run_full_analysis()
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    my_report_file = os.path.join(MY_DATASET_DIR, f'{MY_DATASET_NAME}_analysis_report.txt')
    my_analyzer.generate_report(my_report_file)
    
    # ============ ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ†æä½œè€…çš„æ•°æ®é›† ============
    if os.path.exists(AUTHOR_DATASET_DIR):
        print(f"\n{'#'*70}")
        print(f"# ç¬¬äºŒéƒ¨åˆ†: åˆ†æä½œè€…æä¾›çš„æ•°æ®é›†")
        print(f"{'#'*70}")
        
        author_analyzer = DatasetAnalyzer(AUTHOR_DATASET_DIR, AUTHOR_DATASET_NAME)
        author_analyzer.load_data()
        
        # æ‰“å°åˆ†æç»“æœåˆ°æ§åˆ¶å°
        author_analyzer.run_full_analysis()
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        author_report_file = os.path.join(AUTHOR_DATASET_DIR, f'{AUTHOR_DATASET_NAME}_analysis_report.txt')
        author_analyzer.generate_report(author_report_file)
        
        # ============ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›† ============
        print(f"\n{'#'*70}")
        print(f"# ç¬¬ä¸‰éƒ¨åˆ†: å¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›†")
        print(f"{'#'*70}")
        
        compare_datasets(
            MY_DATASET_DIR, MY_DATASET_NAME,
            AUTHOR_DATASET_DIR, AUTHOR_DATASET_NAME
        )
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        print(f"\n{'='*70}")
        print(f"ğŸ“„ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
        print(f"{'='*70}")
        
        import sys
        from io import StringIO
        
        old_stdout = sys.stdout
        sys.stdout = comparison_buffer = StringIO()
        
        compare_datasets(
            MY_DATASET_DIR, MY_DATASET_NAME,
            AUTHOR_DATASET_DIR, AUTHOR_DATASET_NAME
        )
        
        sys.stdout = old_stdout
        comparison_content = comparison_buffer.getvalue()
        
        comparison_file = os.path.join(MY_DATASET_DIR, 'comparison_report.txt')
        with open(comparison_file, 'w', encoding='utf-8') as f:
            f.write(f"æ•°æ®é›†å¯¹æ¯”æŠ¥å‘Š\n")
            f.write(f"æ—¶é—´: 2025-11-14 08:57:57 UTC\n")
            f.write(f"ç”¨æˆ·: YYYYXL1004\n")
            f.write(f"{'='*70}\n\n")
            f.write(comparison_content)
        
        print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {comparison_file}")
        
    else:
        print(f"\nâš ï¸  æœªæ‰¾åˆ°ä½œè€…æ•°æ®é›†ç›®å½•: {AUTHOR_DATASET_DIR}")
        print(f"   å¦‚æœä½ æœ‰ä½œè€…çš„æ•°æ®é›†ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ AUTHOR_DATASET_DIR å˜é‡")
    
    # ============ æ€»ç»“ ============
    print(f"\n{'='*70}")
    print(f"âœ… åˆ†æå®Œæˆï¼")
    print(f"{'='*70}")
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶:")
    if os.path.exists(my_report_file):
        print(f"   1. {my_report_file}")
    if os.path.exists(AUTHOR_DATASET_DIR):
        author_report_file = os.path.join(AUTHOR_DATASET_DIR, f'{AUTHOR_DATASET_NAME}_analysis_report.txt')
        if os.path.exists(author_report_file):
            print(f"   2. {author_report_file}")
        comparison_file = os.path.join(MY_DATASET_DIR, 'comparison_report.txt')
        if os.path.exists(comparison_file):
            print(f"   3. {comparison_file}")
    
    print(f"\n{'='*70}")


if __name__ == '__main__':
    main()