# ETEGRec: T5 â†’ LLaMA2-7B-HF è¿ç§»æ–¹æ¡ˆ (Final v3.2)

> **ç›®æ ‡è®¾å¤‡**: RTX 5090 32GB Ã— N å¡  
> **å‚è€ƒ**: Align3GR, MiniOneRec, OpenOneRec

---

## 0. æ ¸å¿ƒæ¶æ„å·®å¼‚è¯´æ˜

| ç‰¹æ€§ | T5 (åŸ) | LLaMA2 (ç›®æ ‡) |
|------|---------|---------------|
| æ¶æ„ | Encoder-Decoder | Decoder-only |
| SIAæå– | `encoder_last_hidden_state` Mean Pool | **Last Token** (seq_end_pos) |
| PSAæå– | `decoder_hidden_states[-1][:, 0]` | **Last Token** (seq_end_pos) |
| ç”ŸæˆLogits | decoderè¾“å‡º Ã— codebook.T | **Output Proj â†’ MatMul(Codebook.T)** |
| ç”Ÿæˆæ–¹å¼ | encoderç¼“å­˜ + decoderè‡ªå›å½’ | çº¯è‡ªå›å½’ç»­å†™ |

### â­ v3.1 å…³é”®ä¿®æ­£ (vs v3.0)

1. **ç§»é™¤ `code_heads`**ï¼šæ”¹å› Weight Tying (ç‚¹ç§¯ Codebook)ï¼Œä¿è¯ End-to-End æ¢¯åº¦æµ
2. **SIA æ”¹ç”¨ Last Token**ï¼šç¬¦åˆ Causal LM ç‰¹æ€§ï¼Œå»æ‰ Mean Pooling
3. **æ˜¾å¼ç®¡ç† Projector æ¢¯åº¦**ï¼šç¡®ä¿è‡ªå®šä¹‰å±‚å‚ä¸è®­ç»ƒ

### â­ v3.2 å·¥ç¨‹ä¼˜åŒ– (vs v3.1)

1. **Projector åˆå§‹åŒ–**ï¼šä½¿ç”¨å°æ–¹å·®åˆå§‹åŒ– (std=0.02)ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
2. **æ¾„æ¸… SEQ_END å®ç°**ï¼šä¸éœ€è¦æ˜¾å¼ç‰¹æ®Štokenï¼Œä»…é€šè¿‡ä½ç½®ç´¢å¼•åŒºåˆ†å†å²/ç›®æ ‡

---

## 1. æ ¸å¿ƒè®¾è®¡å†³ç­–

| è®¾è®¡ç‚¹ | å†³ç­– |
|-------|------|
| **Input Embedding** | SoftEmbedding + scid_projector (128 â†’ 4096)ï¼ŒæŒ‰é—´éš”æŸ¥è¡¨ |
| **Output Logits** | output_projector (4096 â†’ 128) + MatMul(Codebook.T) â­ Weight Tying |
| **SIA/PSA ä½ç½®** | Last Token (seq_end_pos)ï¼Œä¸ç”¨ Mean Pooling |
| **LoRA** | q/k/v/o_projï¼Œæ˜¾å¼è®­ç»ƒ Projectors |
| **æ˜¾å­˜ä¼˜åŒ–** | Gradient Checkpointing + bf16 |

---

## 2. Prompt æ ¼å¼è®¾è®¡

```
<c0_151> <c1_19> <c2_62> <c3_0> | <c0_74> <c1_44> ... | <c0_?> <c1_?> <c2_?> <c3_?>
|<--------- Item 1 --------->|   |<---- Item 2 ---->|   |<-- ç›®æ ‡ Item (é¢„æµ‹) -->|
                                                    â†‘
                                          seq_end_position (ä½ç½®ç´¢å¼•)
```

### â­ å…³é”®æ¾„æ¸…ï¼šä¸éœ€è¦æ˜¾å¼ `[SEQ_END]` Tokenï¼

- **å®ç°æ–¹å¼**ï¼šçº¯ codes åºåˆ—ï¼Œé€šè¿‡**ä½ç½®ç´¢å¼•**åŒºåˆ†å†å²/ç›®æ ‡
- **seq_end_position**ï¼šå†å²åºåˆ—æœ€åä¸€ä¸ª token çš„ä½ç½®ç´¢å¼•
- **æ— éœ€ resize è¯è¡¨**ï¼šä¸å¼•å…¥æ–° tokenï¼Œé›¶æ˜¾å­˜å¼€é”€
- **SIA/PSA æå–**ï¼šç›´æ¥ç”¨ `hidden_states[batch_idx, seq_end_position]`

```python
# DataLoader ä¸­çš„ä½ç½®è®¡ç®—
seq_end_position = len(history_codes) - 1  # æœ€åä¸€ä¸ªå†å² code çš„ä½ç½®
target_positions = range(len(history_codes), len(history_codes) + code_length)
```

---

## 3. å…³é”®ä»£ç å®ç°

### 3.1 æ¨¡å‹å®šä¹‰

```python
class LlamaRecModel(nn.Module):
    def __init__(self, config, llama_model, rqvae):
        super().__init__()
        
        # === LLaMA åŸºåº§ ===
        self.llama = llama_model
        self.llama.gradient_checkpointing_enable()
        
        # === RQ-VAE ===
        self.rqvae = rqvae
        self.code_length = config['code_length']  # 4 (3 from RQVAE + 1 collision)
        self.code_num = config['code_num']  # 256
        
        # === ç»´åº¦é…ç½® ===
        self.codebook_dim = config['e_dim']  # 128
        self.hidden_size = llama_model.config.hidden_size  # 4096
        self.semantic_dim = config['semantic_hidden_size']  # 256
        
        # === Input Projector: Codebook â†’ LLaMA ===
        self.scid_projector = nn.Linear(self.codebook_dim, self.hidden_size, bias=False)
        
        # === â­ Output Projector: LLaMA â†’ Codebook (ç”¨äºç‚¹ç§¯) ===
        # ä¸ç”¨ code_headsï¼ä¿æŒ Weight Tying è®©æ¢¯åº¦æµå‘ Codebook
        self.output_projector = nn.Linear(self.hidden_size, self.codebook_dim, bias=False)
        
        # === å¯¹é½å±‚ ===
        self.enc_adapter = MLPLayers([self.hidden_size, self.codebook_dim])  # SIA
        self.dec_adapter = MLPLayers([self.hidden_size, self.semantic_dim])  # PSA
        
        # === è¯­ä¹‰ Embedding (å†»ç»“) ===
        self.semantic_embedding = nn.Embedding(config['n_items'], self.semantic_dim)
        self.semantic_embedding.requires_grad_(False)
        
        # === LoRA ===
        lora_config = LoraConfig(
            r=64, lora_alpha=128,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            modules_to_save=[],  # Projectors ä¸åœ¨ llama å†…éƒ¨ï¼Œæ— éœ€åŠ å…¥
        )
        self.llama = get_peft_model(self.llama, lora_config)
        
        # === â­ æ˜¾å¼ç¡®ä¿è‡ªå®šä¹‰å±‚å‚ä¸è®­ç»ƒ ===
        self.scid_projector.requires_grad_(True)
        self.output_projector.requires_grad_(True)
        for param in self.enc_adapter.parameters():
            param.requires_grad_(True)
        for param in self.dec_adapter.parameters():
            param.requires_grad_(True)
        
        # === â­ v3.2: åˆå§‹åŒ–è‡ªå®šä¹‰å±‚ (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±) ===
        self._init_custom_weights()
    
    def _init_custom_weights(self):
        """
        ä½¿ç”¨å°æ–¹å·®åˆå§‹åŒ– Projector å±‚
        é˜²æ­¢è®­ç»ƒåˆæœŸ Logits è¿‡å¤§å¯¼è‡´ Softmax å˜ one-hot (æ¢¯åº¦æ¶ˆå¤±)
        æˆ– Logits è¿‡å°å¯¼è‡´å­¦ä¹ ç¼“æ…¢
        """
        # Projectors
        torch.nn.init.normal_(self.scid_projector.weight, mean=0.0, std=0.02)
        torch.nn.init.normal_(self.output_projector.weight, mean=0.0, std=0.02)
        
        # Adapters (å¦‚æœæ˜¯ nn.Linear)
        for module in [self.enc_adapter, self.dec_adapter]:
            for layer in module.modules():
                if isinstance(layer, nn.Linear):
                    torch.nn.init.normal_(layer.weight, mean=0.0, std=0.02)
                    if layer.bias is not None:
                        torch.nn.init.zeros_(layer.bias)

    def get_codebooks(self):
        """è·å– RQ-VAE çš„ç æœ¬ Embedding å±‚åˆ—è¡¨"""
        return self.rqvae.rq.vq_layers

    def get_input_embeddings(self, input_ids, attention_mask):
        """
        SoftEmbedding: æŒ‰é—´éš”æŸ¥è¡¨ï¼Œä¸åŸ T5 ç‰ˆæœ¬é€»è¾‘ä¸€è‡´
        
        input_ids å¸ƒå±€: [c0, c1, c2, c3, c0, c1, c2, c3, ...]
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        embeddings = torch.zeros(
            batch_size, seq_len, self.hidden_size,
            dtype=torch.bfloat16, device=device
        )
        
        # å¤„ç† padding (-1 â†’ 0)
        input_ids_safe = input_ids.clone()
        input_ids_safe[input_ids == -1] = 0
        
        # æŒ‰é—´éš”æŸ¥è¡¨
        codebooks = self.get_codebooks()
        for level in range(self.code_length):
            # å–æ¯éš” code_length çš„ä½ç½®
            codes_at_level = input_ids_safe[:, level::self.code_length]  # [B, seq_len/K]
            
            # ä»å¯¹åº”å±‚çš„ç æœ¬æŸ¥ embedding
            raw_embeds = codebooks[level].embedding(codes_at_level)  # [B, seq_len/K, 128]
            
            # æŠ•å½±åˆ° LLaMA ç»´åº¦
            proj_embeds = self.scid_projector(raw_embeds)  # [B, seq_len/K, 4096]
            
            # æ”¾å›å¯¹åº”ä½ç½®
            embeddings[:, level::self.code_length] = proj_embeds
        
        # Padding ä½ç½®ç½®é›¶
        padding_mask = ~attention_mask.bool()
        embeddings[padding_mask] = 0
        
        return embeddings

    def forward(self, input_ids, attention_mask, seq_end_positions, 
                target_positions, labels=None, targets=None):
        """
        Args:
            input_ids: [B, total_len] - å†å² + ç›®æ ‡çš„ codes
            attention_mask: [B, total_len]
            seq_end_positions: [B] - å†å²åºåˆ—ç»“æŸä½ç½® (ç”¨äº SIA/PSA)
            target_positions: [B, code_length] - ç›®æ ‡ code å„ä½ç½®çš„ç´¢å¼•
            labels: [B, code_length] - ç›®æ ‡ item çš„çœŸå® code
            targets: [B] - ç›®æ ‡ item ID (ç”¨äº SIA/PSA)
        """
        # === 1. è·å–è¾“å…¥åµŒå…¥ ===
        inputs_embeds = self.get_input_embeddings(input_ids, attention_mask)
        
        if self.training:
            inputs_embeds = inputs_embeds.requires_grad_(True)
        
        # === 2. LLaMA Forward ===
        outputs = self.llama(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        hidden_states = outputs.hidden_states[-1]  # [B, L, 4096]
        batch_size = hidden_states.size(0)
        batch_indices = torch.arange(batch_size, device=hidden_states.device)
        
        # === 3. â­ SIA/PSA: Last Token (ä¸ç”¨ Mean Pooling) ===
        # Causal LM ä¸­ï¼Œæœ€åä¸€ä¸ª token å·²ç»çœ‹è¿‡æ‰€æœ‰å†å²ï¼Œä¿¡æ¯æœ€ä¸°å¯Œ
        last_hidden = hidden_states[batch_indices, seq_end_positions]  # [B, 4096]
        
        seq_project_latents = self.enc_adapter(last_hidden)  # [B, 128] for SIA
        dec_latents = self.dec_adapter(last_hidden)  # [B, 256] for PSA
        
        # === 4. â­ ç”Ÿæˆ Logits: Weight Tying (ç‚¹ç§¯ Codebook) ===
        code_logits = []
        codebooks = self.get_codebooks()
        
        for i in range(self.code_length):
            pos_i = target_positions[:, i]  # [B]
            hidden_at_pos = hidden_states[batch_indices, pos_i]  # [B, 4096]
            
            # Step 1: æŠ•å½±å› Codebook ç»´åº¦
            query_emb = self.output_projector(hidden_at_pos)  # [B, 128]
            
            # Step 2: ä¸ç¬¬ i å±‚ Codebook åšç‚¹ç§¯ (Weight Tying!)
            # codebook.embedding.weight: [256, 128] â†’ è½¬ç½® â†’ [128, 256]
            codebook_weight = codebooks[i].embedding.weight.t()  # [128, 256]
        
            # Step 3: è®¡ç®—ç›¸ä¼¼åº¦ logits
            logits = torch.matmul(query_emb, codebook_weight)  # [B, 256]
            code_logits.append(logits)
        
        code_logits = torch.stack(code_logits, dim=1)  # [B, code_length, code_num]
        
        return QuantizeOutput(
            logits=code_logits,
            seq_latents=last_hidden,  # åŸå§‹ hiddenï¼Œä¾›è°ƒè¯•ç”¨
            seq_project_latents=seq_project_latents,
            dec_latents=dec_latents
        )

    @torch.no_grad()
    def generate(self, input_ids, attention_mask, seq_end_positions,
                 num_beams=20, num_return_sequences=10):
        """
        è‡ªå›å½’ç”Ÿæˆç›®æ ‡ item çš„ codes
        ä½¿ç”¨ Beam Searchï¼Œæ¯æ­¥ç”¨ output_projector + Codebook ç‚¹ç§¯
        """
        batch_size = input_ids.size(0)
        device = input_ids.device
        codebooks = self.get_codebooks()
        
        # Beam Search åˆå§‹åŒ–
        input_ids_expanded = input_ids.repeat_interleave(num_beams, dim=0)
        attention_mask_expanded = attention_mask.repeat_interleave(num_beams, dim=0)
        
        beam_scores = torch.zeros(batch_size, num_beams, device=device)
        beam_scores[:, 1:] = -1e9
        beam_scores = beam_scores.view(-1)
        
        generated_codes = []
        current_embeds = self.get_input_embeddings(input_ids_expanded, attention_mask_expanded)
        
        beam_idx_offset = torch.arange(batch_size, device=device).repeat_interleave(num_beams) * num_beams
        
        for code_idx in range(self.code_length):
            # Forward
            outputs = self.llama(
                inputs_embeds=current_embeds,
                attention_mask=attention_mask_expanded,
                output_hidden_states=True
            )
            
            last_hidden = outputs.hidden_states[-1][:, -1, :]  # [B*beams, 4096]
            
            # æŠ•å½± + ç‚¹ç§¯ Codebook
            query_emb = self.output_projector(last_hidden)  # [B*beams, 128]
            codebook_weight = codebooks[code_idx].embedding.weight.t()  # [128, 256]
            logits = torch.matmul(query_emb, codebook_weight)  # [B*beams, 256]
            
            # Beam Search æ›´æ–°
            log_probs = F.log_softmax(logits, dim=-1)
            next_scores = log_probs + beam_scores.unsqueeze(-1)
            
            vocab_size = log_probs.size(-1)
            next_scores = next_scores.view(batch_size, num_beams * vocab_size)
            next_scores, next_tokens = torch.topk(next_scores, num_beams, dim=-1)
            
            next_indices = torch.div(next_tokens, vocab_size, rounding_mode='floor')
            next_codes = next_tokens % vocab_size
            
            beam_scores = next_scores.view(-1)
            
            # è®°å½•ç”Ÿæˆçš„ code
            generated_codes.append(next_codes)
            
            # å‡†å¤‡ä¸‹ä¸€æ­¥çš„ embedding
            beam_idx = (next_indices + beam_idx_offset.view(batch_size, num_beams)).view(-1)
            current_embeds = current_embeds[beam_idx]
            attention_mask_expanded = attention_mask_expanded[beam_idx]
            
            # æ·»åŠ æ–°ç”Ÿæˆçš„ code embedding
            next_codes_flat = next_codes.view(-1)
            next_embeds = codebooks[code_idx].embedding(next_codes_flat)  # [B*beams, 128]
            next_embeds = self.scid_projector(next_embeds).unsqueeze(1)  # [B*beams, 1, 4096]
            
            current_embeds = torch.cat([current_embeds, next_embeds], dim=1)
            attention_mask_expanded = torch.cat([
                attention_mask_expanded,
                torch.ones(attention_mask_expanded.size(0), 1, device=device, dtype=attention_mask_expanded.dtype)
            ], dim=1)
        
        # æ•´ç†è¾“å‡º
        generated_codes = torch.stack(generated_codes, dim=-1)  # [B, beams, code_length]
        return generated_codes[:, :num_return_sequences, :]
```

### 3.2 DataLoader è®¾è®¡

```python
class LlamaRecDataset(Dataset):
    def __init__(self, data, all_item_code, code_length=4, max_seq_len=50):
        self.data = data
        self.all_item_code = all_item_code  # [n_items+1, code_length]
        self.code_length = code_length
        self.max_seq_len = max_seq_len
    
    def __getitem__(self, idx):
        user_seq = self.data[idx]['history']  # [L] item IDs
        target_item = self.data[idx]['target']  # item ID
        
        # 1. æ„é€ å†å²åºåˆ—çš„ codes
        history_codes = []
        for item_id in user_seq[-self.max_seq_len:]:
            item_codes = self.all_item_code[item_id]
            history_codes.extend(item_codes.tolist())
        
        # 2. æ„é€ ç›®æ ‡åºåˆ—çš„ codes
        target_codes = self.all_item_code[target_item].tolist()
        
        # 3. æ‹¼æ¥
        input_ids = history_codes + target_codes
        
        # 4. è®¡ç®—å…³é”®ä½ç½®
        seq_end_position = len(history_codes) - 1  # æœ€åä¸€ä¸ªå†å² token çš„ä½ç½®
        target_positions = list(range(len(history_codes), len(history_codes) + self.code_length))
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.ones(len(input_ids), dtype=torch.long),
            'seq_end_position': seq_end_position,
            'target_positions': torch.tensor(target_positions, dtype=torch.long),
            'labels': torch.tensor(target_codes, dtype=torch.long),
            'target_item': target_item,
        }


def collate_fn(batch):
    """åŠ¨æ€ Padding"""
    max_len = max(len(b['input_ids']) for b in batch)
    
    input_ids = []
    attention_mask = []
    seq_end_positions = []
    target_positions = []
    labels = []
    targets = []
    
    for b in batch:
        pad_len = max_len - len(b['input_ids'])
        # å·¦ Padding (LLaMA ä¹ æƒ¯)
        input_ids.append(F.pad(b['input_ids'], (pad_len, 0), value=-1))
        attention_mask.append(F.pad(b['attention_mask'], (pad_len, 0), value=0))
        
        # ä½ç½®ç´¢å¼•éœ€è¦åŠ ä¸Š padding åç§»
        seq_end_positions.append(b['seq_end_position'] + pad_len)
        target_positions.append(b['target_positions'] + pad_len)
        
        labels.append(b['labels'])
        targets.append(b['target_item'])
    
    return {
        'input_ids': torch.stack(input_ids),
        'attention_mask': torch.stack(attention_mask),
        'seq_end_positions': torch.tensor(seq_end_positions),
        'target_positions': torch.stack(target_positions),
        'labels': torch.stack(labels),
        'targets': torch.tensor(targets),
    }
```

### 3.3 Trainer æ ¸å¿ƒä¿®æ”¹

```python
def _train_epoch_rec(self, epoch_idx, loss_w, verbose=True):
    """è®­ç»ƒæ¨èå™¨ (å†»ç»“ Tokenizer)"""
    self.model_rec.train()
    self.model_id.eval()
    
    for batch in train_loader:
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        seq_end_positions = batch['seq_end_positions'].to(self.device)
        target_positions = batch['target_positions'].to(self.device)
        labels = batch['labels'].to(self.device)
        targets = batch['targets'].to(self.device)
        
        # ç›®æ ‡ item çš„è¯­ä¹‰ embedding
        target_semantic_embs = self.model_rec.semantic_embedding(targets)
        target_recon_embs, _, _, _, target_code_logits = self.model_id(target_semantic_embs)
        
        # Forward
        outputs = self.model_rec(
            input_ids=input_ids,
            attention_mask=attention_mask,
            seq_end_positions=seq_end_positions,
            target_positions=target_positions,
        )
        
        # === Loss è®¡ç®— ===
        
        # 1. Code Loss (ç”Ÿæˆä»»åŠ¡) - æ¢¯åº¦ä¼šæµå‘ Codebookï¼
        code_loss = F.cross_entropy(
            outputs.logits.view(-1, self.code_num),
            labels.view(-1)
        )
        
        # 2. SIA Loss (KL æ•£åº¦)
        _, _, _, _, seq_code_logits = self.model_id.rq(outputs.seq_project_latents)
        kl_loss = (
            self.compute_discrete_contrastive_loss_kl(seq_code_logits, target_code_logits) +
            self.compute_discrete_contrastive_loss_kl(target_code_logits, seq_code_logits)
        )
        
        # 3. PSA Loss (InfoNCE)
        dec_cl_loss = (
            self.compute_contrastive_loss(target_recon_embs, outputs.dec_latents) +
            self.compute_contrastive_loss(outputs.dec_latents, target_recon_embs)
        )
        
        # æ€» Loss
        loss = (loss_w['code_loss'] * code_loss + 
                loss_w['kl_loss'] * kl_loss + 
                loss_w['dec_cl_loss'] * dec_cl_loss)
        
        self.accelerator.backward(loss)
        self.rec_optimizer.step()
        self.rec_lr_scheduler.step()
```

---

## 4. 5090 32GB é…ç½®

```yaml
# config/llama_5090.yaml
model:
  base_model: "meta-llama/Llama-2-7b-hf"
precision: bf16
gradient_checkpointing: true

lora:
  r: 64
lora_alpha: 128
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

training:
  batch_size_per_gpu: 2
  gradient_accumulation: 8
learning_rate: 1e-4
epochs: 50
  warmup_steps: 500

data:
  max_seq_len: 50
  code_length: 4
  code_num: 256
```

---

## 5. å¯åŠ¨å‰ Checklist

- [ ] `scid_projector` (128 â†’ 4096) å·²æ·»åŠ 
- [ ] `output_projector` (4096 â†’ 128) å·²æ·»åŠ 
- [ ] **ç§»é™¤äº† `code_heads`**ï¼Œæ”¹ç”¨ Codebook ç‚¹ç§¯
- [ ] SIA/PSA ä½¿ç”¨ Last Tokenï¼Œç§»é™¤ Mean Pooling
- [ ] DataLoader è¿”å› `seq_end_positions` å’Œ `target_positions`
- [ ] **ä¸ä½¿ç”¨æ˜¾å¼ SEQ_END token**ï¼Œä»…ç”¨ä½ç½®ç´¢å¼•
- [ ] Gradient Checkpointing å·²å¼€å¯
- [ ] æ‰€æœ‰è‡ªå®šä¹‰å±‚ `requires_grad_(True)`
- [ ] **Projector åˆå§‹åŒ–** (`std=0.02`)
- [ ] è¿è¡Œæ¢¯åº¦å¥å…¨æ€§æ£€æŸ¥

---

## 6. æ¢¯åº¦æ£€æŸ¥è„šæœ¬

```python
def sanity_check(model, batch):
    """æ£€æŸ¥å…³é”®ç»„ä»¶çš„æ¢¯åº¦æµï¼Œç‰¹åˆ«æ˜¯ Codebook æ˜¯å¦æ”¶åˆ°æ¢¯åº¦"""
    model.train()
    
    outputs = model(
        input_ids=batch['input_ids'],
        attention_mask=batch['attention_mask'],
        seq_end_positions=batch['seq_end_positions'],
        target_positions=batch['target_positions'],
    )
    
    loss = F.cross_entropy(
        outputs.logits.view(-1, 256),
        batch['labels'].view(-1)
    )
    loss.backward()
    
    print("=== Gradient Check (v3.1) ===")
    
    # â­ å…³é”®ï¼šCodebook å¿…é¡»æœ‰æ¢¯åº¦ï¼
    codebooks = model.get_codebooks()
    for i, cb in enumerate(codebooks):
        grad = cb.embedding.weight.grad
        if grad is None:
            print(f"âŒ Codebook[{i}] grad is None - End-to-End æ–­è£‚!")
        elif grad.abs().sum() == 0:
            print(f"âš ï¸ Codebook[{i}] grad is zero")
        else:
            print(f"âœ… Codebook[{i}] grad norm: {grad.norm():.6f}")
    
    # Projectors
    for name, proj in [("scid_projector", model.scid_projector), 
                       ("output_projector", model.output_projector)]:
        grad = proj.weight.grad
        if grad is not None:
            print(f"âœ… {name} grad norm: {grad.norm():.6f}")
        else:
            print(f"âŒ {name} grad is None")
    
    # Adapters
    enc_grad = list(model.enc_adapter.parameters())[0].grad
    dec_grad = list(model.dec_adapter.parameters())[0].grad
    print(f"âœ… enc_adapter grad: {enc_grad.norm():.6f}" if enc_grad is not None else "âŒ enc_adapter None")
    print(f"âœ… dec_adapter grad: {dec_grad.norm():.6f}" if dec_grad is not None else "âŒ dec_adapter None")
```

---

## 7. v3.1 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           LlamaRecModel v3.1                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  [Input: History Codes]                                                 â”‚
â”‚         â”‚                                                               â”‚
â”‚         â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚  Codebook[i] â”‚ â”€â”€â”€â–º â”‚ scid_projectorâ”‚ â”€â”€â”€â–º [128 â†’ 4096]             â”‚
â”‚  â”‚  (RQ-VAE)    â”‚      â”‚   (å¯è®­ç»ƒ)     â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â”‚                     â”‚                                         â”‚
â”‚         â”‚                     â–¼                                         â”‚
â”‚         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚         â”‚              â”‚   LLaMA2-7B     â”‚                             â”‚
â”‚         â”‚              â”‚   (LoRA)        â”‚                             â”‚
â”‚         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚         â”‚                       â”‚                                       â”‚
â”‚         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚         â”‚           â”‚           â”‚           â”‚                          â”‚
â”‚         â”‚           â–¼           â–¼           â–¼                          â”‚
â”‚         â”‚     [seq_end_pos] [seq_end_pos] [target_pos]                 â”‚
â”‚         â”‚           â”‚           â”‚           â”‚                          â”‚
â”‚         â”‚           â–¼           â–¼           â–¼                          â”‚
â”‚         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚    â”‚enc_adapterâ”‚ â”‚dec_adapterâ”‚ â”‚output_projectorâ”‚             â”‚
â”‚         â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚         â”‚            â”‚               â”‚                        â”‚
â”‚         â”‚         â–¼            â–¼               â–¼                        â”‚
â”‚         â”‚    [B, 128]     [B, 256]        [B, 128]                      â”‚
â”‚         â”‚         â”‚            â”‚               â”‚                        â”‚
â”‚         â”‚         â”‚            â”‚               â”‚                        â”‚
â”‚         â”‚         â–¼            â–¼               â–¼                        â”‚
â”‚         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â”‚    â”‚SIA Loss â”‚ â”‚PSA Loss â”‚    â”‚ MatMul          â”‚            â”‚
â”‚         â”‚    â”‚(KL Div) â”‚ â”‚(InfoNCE)â”‚    â”‚ (Codebook.T)    â”‚â—„â”€â”€â”€â”€â”€â”     â”‚
â”‚         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚     â”‚
â”‚         â”‚                                        â”‚               â”‚     â”‚
â”‚         â”‚                                        â–¼               â”‚     â”‚
â”‚         â”‚                                   [B, 256]             â”‚     â”‚
â”‚         â”‚                                   (Logits)             â”‚     â”‚
â”‚         â”‚                                        â”‚               â”‚     â”‚
â”‚         â”‚                                        â–¼               â”‚     â”‚
â”‚         â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤Code Loss â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                  â­ Weight Tying:          â”‚(CE Loss) â”‚                â”‚
â”‚                  æ¢¯åº¦å›ä¼ åˆ° Codebook        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. å®æ–½ä¼˜å…ˆçº§

| Phase | ä»»åŠ¡ | å·¥æ—¶ |
|-------|------|------|
| ğŸ”´ P0 | LlamaRecModel å®ç° (å« Weight Tying) | 2å¤© |
| ğŸ”´ P0 | DataLoader é‡æ„ | 1.5å¤© |
| ğŸŸ¡ P1 | Trainer é€‚é… | 1.5å¤© |
| ğŸŸ¡ P1 | Generate (Beam Search) | 1.5å¤© |
| ğŸŸ¢ P2 | æ¢¯åº¦è°ƒè¯• + å½¢çŠ¶éªŒè¯ | 1å¤© |
| ğŸŸ¢ P2 | è¯„ä¼° + è°ƒä¼˜ | 2å¤© |

**æ€»è®¡: 9-10 å¤©**

---

## 9. é£é™©ç‚¹ä¸å¤‡é€‰æ–¹æ¡ˆ

| é£é™© | å½±å“ | å¤‡é€‰æ–¹æ¡ˆ |
|------|------|---------|
| æ˜¾å­˜æº¢å‡º | è®­ç»ƒå¤±è´¥ | é™ batchï¼ŒåŠ  grad_accumï¼Œç”¨ DeepSpeed ZeRO |
| Codebook æ¢¯åº¦æ¶ˆå¤± | End-to-End å¤±æ•ˆ | æ£€æŸ¥ output_projector åˆå§‹åŒ–ï¼ŒåŠ æ¢¯åº¦ç›‘æ§ |
| Logits çˆ†ç‚¸/NaN | è®­ç»ƒå´©æºƒ | å‡å°åˆå§‹åŒ– std (0.02â†’0.01)ï¼ŒåŠ  gradient clipping |
| ç”Ÿæˆè´¨é‡å·® | æ¨èæ•ˆæœä¸‹é™ | å¢åŠ  beam_sizeï¼Œæ·»åŠ  prefix constraint |

---

## 10. ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ ¸å¿ƒä¿®æ”¹ |
|------|---------|
| v3.0 | åŸºç¡€æ¶æ„è®¾è®¡ï¼ŒSoftEmbedding + code_heads |
| v3.1 | ç§»é™¤ code_headsï¼Œæ”¹ç”¨ Weight Tyingï¼›SIA æ”¹ç”¨ Last Token |
| v3.2 | æ·»åŠ  Projector åˆå§‹åŒ–ï¼›æ¾„æ¸…ä¸éœ€è¦æ˜¾å¼ SEQ_END token |
