# ETEGRec: T5 â†’ LLaMA2-7B-HF è¿ç§»æ–¹æ¡ˆ (Final v2.2)

> **ç›®æ ‡è®¾å¤‡**: RTX 5090 32GB Ã— N å¡  
> **å‚è€ƒ**: Align3GR, MiniOneRec, OpenOneRec

---

## 1. æ ¸å¿ƒè®¾è®¡å†³ç­–

| è®¾è®¡ç‚¹ | å†³ç­– |
|-------|------|
| **Embedding** | SoftEmbedding + Projector (128 â†’ 4096) |
| **SIA/PSA ä½ç½®** | `[SEP]`ï¼ˆDataLoader é¢„è®¡ç®—ï¼‰ |
| **LoRA** | q/k/v/o_projï¼Œä¸å« embed_tokens |
| **æ˜¾å­˜ä¼˜åŒ–** | Gradient Checkpointing + bf16 |

---

## 2. å…³é”®ä»£ç å®ç°

### 2.1 SoftEmbedding + Projector

```python
class LlamaRecModel(nn.Module):
    def __init__(self, config, rqvae):
        super().__init__()
        
        # LLaMA åŠ è½½
        self.llama = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        
        # Gradient Checkpointingï¼ˆ5090 å¿…é¡»å¼€å¯ï¼‰
        self.llama.gradient_checkpointing_enable()
        
        # RQ-VAE
        self.rqvae = rqvae
        for cb in self.rqvae.codebooks:
            cb.requires_grad_(True)
        
        # â­ Projector: Codebook dim â†’ LLaMA dim
        self.codebook_dim = self.rqvae.codebooks[0].embedding_dim  # 128
        self.hidden_size = self.llama.config.hidden_size  # 4096
        self.scid_projector = nn.Linear(self.codebook_dim, self.hidden_size, bias=False)
        self.scid_projector.to(torch.bfloat16)
        
        # LoRAï¼ˆä¸å« embed_tokensï¼‰
        lora_config = LoraConfig(
            r=64, lora_alpha=128,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            modules_to_save=[],
        )
        self.llama = get_peft_model(self.llama, lora_config)
        
        # å¯¹é½å±‚
        self.enc_adapter = MLPLayers([self.hidden_size, config['e_dim']])
        self.dec_adapter = MLPLayers([self.hidden_size, config['semantic_dim']])

    def get_input_embeddings(self, input_ids):
        """SoftEmbedding: SCID â†’ Codebook â†’ Projector â†’ LLaMA dim"""
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        embeddings = torch.zeros(
            batch_size, seq_len, self.hidden_size,
            dtype=torch.bfloat16, device=device
        )
        
        # 1. Text tokens â†’ Frozen LLaMA Embedding
        text_mask = input_ids < self.scid_token_start
        if text_mask.any():
            with torch.no_grad():
                embeddings[text_mask] = self.llama.model.model.embed_tokens(
                    input_ids[text_mask]
                )
        
        # 2. SCID tokens â†’ Codebook â†’ Projector
        scid_mask = ~text_mask
        if scid_mask.any():
            scid_values = input_ids.clone()
            scid_values[text_mask] = self.scid_token_start
            
            relative_ids = scid_values - self.scid_token_start
            level_idx = relative_ids // self.code_number
            code_idx = relative_ids % self.code_number
            
            for level in range(self.code_length):
                current_mask = scid_mask & (level_idx == level)
                if current_mask.any():
                    codes = code_idx[current_mask]
                    raw_embeds = self.rqvae.codebooks[level](codes)  # [N, 128]
                    proj_embeds = self.scid_projector(raw_embeds)    # [N, 4096]
                    embeddings[current_mask] = proj_embeds
        
        return embeddings

    def forward(self, input_ids, attention_mask, sep_indices, labels=None, target_item_emb=None):
        # Gradient Checkpointing å…¼å®¹æ€§
        inputs_embeds = self.get_input_embeddings(input_ids)
        if self.training:
            inputs_embeds.requires_grad_(True)
            inputs_embeds.register_hook(lambda x: x)  # Dummy hook
        
        # LLaMA Forward
        outputs = self.llama(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # æå– [SEP] ä½ç½®ï¼ˆç”± DataLoader ä¼ å…¥ï¼‰
        last_hidden = outputs.hidden_states[-1]
        batch_indices = torch.arange(last_hidden.size(0), device=last_hidden.device)
        seq_hidden = last_hidden[batch_indices, sep_indices]
        
        # å¯¹é½ Loss
        sia_loss = self.compute_sia(self.enc_adapter(seq_hidden), target_item_emb)
        psa_loss = self.compute_psa(self.dec_adapter(seq_hidden), target_item_emb)
        
        # ç”Ÿæˆ Loss
        gen_loss = self.compute_gen_loss(outputs.logits, labels) if labels is not None else 0
        
        return gen_loss + 0.0001 * sia_loss + 0.0003 * psa_loss
```

### 2.2 DataLoader è¿”å› sep_indices

```python
# data_llama.py
class LlamaRecDataset(Dataset):
    def __getitem__(self, idx):
        # ... æ„é€  prompt ...
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'sep_indices': len(history_tokens),  # â­ é¢„è®¡ç®—
            'labels': target_codes,
            'target_item_emb': item_embedding
        }
```

---

## 3. 5090 32GB é…ç½®

```yaml
# config/llama_5090.yaml
precision: bf16
batch_size_per_gpu: 4
gradient_accumulation: 4
gradient_checkpointing: true

lora_r: 64
lora_alpha: 128

learning_rate: 1e-4
epochs: 50
```

---

## 4. å¯åŠ¨å‰ Checklist

- [ ] Projector å±‚å·²æ·»åŠ  (`nn.Linear(128, 4096)`)
- [ ] DataLoader è¿”å› `sep_indices`
- [ ] Gradient Checkpointing å·²å¼€å¯
- [ ] æ‰€æœ‰å±‚ dtype ç»Ÿä¸€ä¸º `bfloat16`
- [ ] è¿è¡Œæ¢¯åº¦å¥å…¨æ€§æ£€æŸ¥

---

## 5. æ¢¯åº¦æ£€æŸ¥è„šæœ¬

```python
def sanity_check(model, batch):
    model.train()
    loss = model(**batch)
    loss.backward()
    
    print("=== Gradient Check ===")
    for i, cb in enumerate(model.rqvae.codebooks):
        grad = cb.weight.grad
        if grad is None:
            print(f"âŒ Codebook[{i}] grad is None")
        elif grad.abs().sum() == 0:
            print(f"âš ï¸ Codebook[{i}] grad is zero")
        else:
            print(f"âœ… Codebook[{i}] norm: {grad.norm():.6f}")
    
    proj_grad = model.scid_projector.weight.grad
    print(f"âœ… Projector grad norm: {proj_grad.norm():.6f}" if proj_grad is not None else "âŒ Projector grad None")
```

---

## 6. å®æ–½ä¼˜å…ˆçº§

| Phase | ä»»åŠ¡ | å·¥æ—¶ |
|-------|------|------|
| ğŸ”´ P0 | SoftEmbedding + Projector + æ¢¯åº¦æ£€æŸ¥ | 2å¤© |
| ğŸ”´ P0 | DataLoader (å« sep_indices) | 1å¤© |
| ğŸŸ¡ P1 | è®­ç»ƒå¾ªç¯ + å¤šå¡ | 1å¤© |
| ğŸŸ¡ P1 | SIA/PSA å¯¹é½ | 1å¤© |
| ğŸŸ¢ P2 | è¯„ä¼° | 1å¤© |

**æ€»è®¡: 6-7 å¤©**
