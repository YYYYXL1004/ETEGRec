# ETEGRec: T5 â†’ LLaMA2-7B-HF è¿ç§»æ–¹æ¡ˆ (Final v3.3)

> **ç›®æ ‡è®¾å¤‡**: RTX 5090 32GB Ã— N å¡  
> **å‚è€ƒ**: Align3GR, MiniOneRec, OpenOneRec

---

## 0. æ ¸å¿ƒæ¶æ„å·®å¼‚è¯´æ˜

| ç‰¹æ€§ | T5 (åŸ) | LLaMA2 (ç›®æ ‡) |
|------|---------|---------------|
| æ¶æ„ | Encoder-Decoder | Decoder-only |
| SIAæå– | `encoder_last_hidden_state` Mean Pool | **Last Token** (seq_end_pos) |
| PSAæå– | `decoder_hidden_states[-1][:, 0]` | **Last Token** (seq_end_pos) |
| ç”ŸæˆLogits | decoderè¾“å‡º Ã— codebook.T | **Output Proj â†’ MatMul(Codebook.T)** |
| ç”Ÿæˆæ–¹å¼ | encoderç¼“å­˜ + decoderè‡ªå›å½’ | çº¯è‡ªå›å½’ç»­å†™ |

### â­ ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ ¸å¿ƒä¿®æ”¹ |
|------|---------|
| v3.0 | åŸºç¡€æ¶æ„è®¾è®¡ï¼ŒSoftEmbedding + code_heads |
| v3.1 | ç§»é™¤ code_headsï¼Œæ”¹ç”¨ Weight Tyingï¼›SIA æ”¹ç”¨ Last Token |
| v3.2 | æ·»åŠ  Projector åˆå§‹åŒ–ï¼›æ¾„æ¸…ä¸éœ€è¦æ˜¾å¼ SEQ_END token |
| **v3.3** | **å®é™…å·¥ç¨‹å®ç°ï¼šDDPå…¼å®¹ã€ç‹¬ç«‹Codebookã€Suffixå±‚ã€8-bit Adamã€åˆ†æ‰¹Generate** |

### â­ v3.3 å…³é”®ä¿®æ­£ (vs v3.2)

1. **ç‹¬ç«‹ Codebook å‰¯æœ¬**ï¼šmodel_rec æŒæœ‰ä» rqvae å¤åˆ¶çš„ç‹¬ç«‹ codebookï¼Œé¿å… DDP å…±äº«å‚æ•°é—®é¢˜
2. **Suffix Embedding**ï¼šç¬¬ 4 å±‚ä½¿ç”¨ç‹¬ç«‹çš„ `suffix_embedding` å¤„ç†å†²çªè®¡æ•°
3. **8-bit AdamW**ï¼šLLaMA ä½¿ç”¨ `bitsandbytes` 8-bit ä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜
4. **åˆ†æ‰¹ Generate**ï¼šæ¨ç†æ—¶åˆ† chunk forwardï¼Œé¿å… OOM
5. **Gradient Checkpointing**ï¼šä½¿ç”¨ `use_reentrant=False` è§£å†³ DDP å…¼å®¹æ€§
6. **5090 è¡¥ä¸**ï¼šç¦ç”¨ TF32ï¼Œå¯ç”¨ç¡®å®šæ€§ç®—æ³•

---

## 1. æ ¸å¿ƒè®¾è®¡å†³ç­–

| è®¾è®¡ç‚¹ | å†³ç­– |
|-------|------|
| **Input Embedding** | SoftEmbedding + scid_projector (128 â†’ 4096)ï¼ŒæŒ‰é—´éš”æŸ¥è¡¨ |
| **Output Logits** | output_projector (4096 â†’ 128) + MatMul(Codebook.T) â­ Weight Tying |
| **SIA/PSA ä½ç½®** | Last Token (seq_end_pos)ï¼Œä¸ç”¨ Mean Pooling |
| **Codebook ç®¡ç†** | ç‹¬ç«‹å‰¯æœ¬ + Trainer æ‰‹åŠ¨åŒæ­¥ (DDP å…¼å®¹) |
| **Suffix å¤„ç†** | ç‹¬ç«‹çš„ suffix_embedding (ç¬¬4å±‚) |
| **LoRA** | q/k/v/o_projï¼Œæ˜¾å¼è®­ç»ƒ Projectors |
| **ä¼˜åŒ–å™¨** | 8-bit AdamW (LLaMA) + æ™®é€š AdamW (RQ-VAE) |
| **æ˜¾å­˜ä¼˜åŒ–** | Gradient Checkpointing (non-reentrant) + bf16 + åˆ†æ‰¹ Generate |

---

## 2. Prompt æ ¼å¼è®¾è®¡

```
<c0_151> <c1_19> <c2_62> <c3_0> | <c0_74> <c1_44> ... | <c0_?> <c1_?> <c2_?> <c3_?>
|<--------- Item 1 --------->|   |<---- Item 2 ---->|   |<-- ç›®æ ‡ Item (é¢„æµ‹) -->|
                                                    â†‘
                                          seq_end_position (ä½ç½®ç´¢å¼•)
```

### â­ å…³é”®è¯´æ˜

- **å®ç°æ–¹å¼**ï¼šçº¯ codes åºåˆ—ï¼Œé€šè¿‡**ä½ç½®ç´¢å¼•**åŒºåˆ†å†å²/ç›®æ ‡
- **seq_end_position**ï¼šå†å²åºåˆ—æœ€åä¸€ä¸ª token çš„ä½ç½®ç´¢å¼•
- **æ— éœ€ resize è¯è¡¨**ï¼šä¸å¼•å…¥æ–° tokenï¼Œé›¶æ˜¾å­˜å¼€é”€
- **SIA/PSA æå–**ï¼šç›´æ¥ç”¨ `hidden_states[batch_idx, seq_end_position]`

```python
# DataLoader ä¸­çš„ä½ç½®è®¡ç®—
seq_end_position = len(history_codes) - 1  # æœ€åä¸€ä¸ªå†å² code çš„ä½ç½®
target_positions = range(len(history_codes), len(history_codes) + code_length)
```

---

## 3. å…³é”®ä»£ç å®ç°

### 3.1 æ¨¡å‹å®šä¹‰ (model_llama.py)

```python
class LlamaRecModel(nn.Module):
    """
    åŸºäº LLaMA2-7B çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹
    
    å…³é”®ç‰¹æ€§ï¼š
    - SoftEmbedding: SCID codes â†’ Codebook â†’ Projector â†’ LLaMA
    - Weight Tying: ç”Ÿæˆ Logits é€šè¿‡ç‚¹ç§¯ Codebookï¼Œä¿è¯ End-to-End æ¢¯åº¦æµ
    - Last Token: SIA/PSA ä»å†å²åºåˆ—æœ€åä¸€ä¸ª token æå–
    """
    
    def __init__(self, config, rqvae, llama_path="models/Llama-2-7b-hf"):
        super().__init__()
        
        # === é…ç½®å‚æ•° ===
        self.code_length = config['code_length']  # 4
        self.code_num = config['code_num']  # 256
        self.codebook_dim = config['e_dim']  # 128
        self.semantic_dim = config['semantic_hidden_size']  # 256 or 1024 (DualSCID)
        self.n_items = config['n_items']
        self.num_beams = config.get('num_beams', 20)
        
        # === åŠ è½½ LLaMA åŸºåº§ ===
        # æ³¨æ„: å¤šå¡è®­ç»ƒæ—¶ä¸èƒ½ç”¨ device_map="auto"ï¼Œç”± accelerate ç®¡ç†è®¾å¤‡
        self.llama = AutoModelForCausalLM.from_pretrained(
            llama_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        self.hidden_size = self.llama.config.hidden_size  # 4096
        
        # å¯ç”¨ Gradient Checkpointing (èŠ‚çœæ˜¾å­˜)
        # ä½¿ç”¨ use_reentrant=False è§£å†³ DDP å…¼å®¹æ€§é—®é¢˜
        # (é¿å… "parameter marked ready twice" é”™è¯¯)
        self.llama.gradient_checkpointing_enable(
            gradient_checkpointing_kwargs={"use_reentrant": False}
        )
        
        # === â­ Codebook Embeddings (ç‹¬ç«‹å‰¯æœ¬ï¼Œé¿å… DDP å…±äº«å‚æ•°é—®é¢˜) ===
        # ä» rqvae å¤åˆ¶æƒé‡ï¼Œmodel_rec å’Œ model_id å„æŒæœ‰ç‹¬ç«‹çš„ codebook
        # è®­ç»ƒæ—¶éœ€è¦åœ¨ trainer ä¸­æ‰‹åŠ¨åŒæ­¥
        num_rqvae_layers = len(rqvae.rq.vq_layers)
        self.num_rqvae_layers = num_rqvae_layers  # ä¿å­˜å±‚æ•° (3)
        
        self.codebook_embeddings = nn.ModuleList([
            nn.Embedding(self.code_num, self.codebook_dim)
            for _ in range(num_rqvae_layers)
        ])
        # å¤åˆ¶æƒé‡
        for i, vq_layer in enumerate(rqvae.rq.vq_layers):
            self.codebook_embeddings[i].weight.data.copy_(vq_layer.embedding.weight.data)
        
        # === Input Projector: Codebook dim â†’ LLaMA dim ===
        self.scid_projector = nn.Linear(self.codebook_dim, self.hidden_size, bias=False)
        
        # === Output Projector: LLaMA dim â†’ Codebook dim (ç”¨äºç‚¹ç§¯) ===
        # â­ Weight Tying: ä¸ç”¨ç‹¬ç«‹çš„ code_headsï¼Œè®©æ¢¯åº¦æµå‘ Codebook
        self.output_projector = nn.Linear(self.hidden_size, self.codebook_dim, bias=False)
        
        # === å¯¹é½å±‚ ===
        self.enc_adapter = MLPLayers([self.hidden_size, self.codebook_dim])  # SIA
        self.dec_adapter = MLPLayers([self.hidden_size, self.semantic_dim])  # PSA
        
        # === è¯­ä¹‰ Embedding (å†»ç»“ï¼Œä¸åŸç‰ˆ T5 ä¸€è‡´) ===
        self.semantic_embedding = nn.Embedding(self.n_items, self.semantic_dim)
        self.semantic_embedding.requires_grad_(False)
        
        # === â­ Suffix Embedding (ç”¨äºç¬¬ 4 å±‚ï¼Œå¤„ç†å†²çªè®¡æ•°) ===
        # ä¸åŸç‰ˆ T5 ä¸€è‡´ï¼Œsuffix æœ‰ç‹¬ç«‹çš„ embedding å±‚
        self.suffix_embedding = nn.Embedding(self.code_num, self.codebook_dim)
        self.suffix_embedding.requires_grad_(True)
        
        # === LoRA å¾®è°ƒ ===
        lora_config = LoraConfig(
            r=config.get('lora_r', 64),
            lora_alpha=config.get('lora_alpha', 128),
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_dropout=config.get('lora_dropout', 0.05),
            bias="none",
            task_type="CAUSAL_LM"
        )
        self.llama = get_peft_model(self.llama, lora_config)
        
        # === æ˜¾å¼ç¡®ä¿è‡ªå®šä¹‰å±‚å‚ä¸è®­ç»ƒ ===
        self.scid_projector.requires_grad_(True)
        self.output_projector.requires_grad_(True)
        for param in self.enc_adapter.parameters():
            param.requires_grad_(True)
        for param in self.dec_adapter.parameters():
            param.requires_grad_(True)
        
        # === åˆå§‹åŒ–è‡ªå®šä¹‰å±‚ (v3.2: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±) ===
        self._init_custom_weights()
    
    def _init_custom_weights(self):
        """ä½¿ç”¨å°æ–¹å·®åˆå§‹åŒ– Projector å±‚"""
        torch.nn.init.normal_(self.scid_projector.weight, mean=0.0, std=0.02)
        torch.nn.init.normal_(self.output_projector.weight, mean=0.0, std=0.02)
        
        for module in [self.enc_adapter, self.dec_adapter]:
            for layer in module.modules():
                if isinstance(layer, nn.Linear):
                    torch.nn.init.normal_(layer.weight, mean=0.0, std=0.02)
                    if layer.bias is not None:
                        torch.nn.init.zeros_(layer.bias)
    
    def get_codebooks(self):
        """è·å–ç æœ¬ Embedding å±‚åˆ—è¡¨ (ç‹¬ç«‹å‰¯æœ¬)"""
        return self.codebook_embeddings
    
    def get_input_embeddings(self, input_ids, attention_mask):
        """
        SoftEmbedding: æŒ‰é—´éš”æŸ¥è¡¨
        
        input_ids å¸ƒå±€: [c0, c1, c2, c3, c0, c1, c2, c3, ...]
        æ¯éš” code_length å–ä¸€ä¸ªä½ç½®ï¼ŒæŸ¥å¯¹åº”å±‚çš„ç æœ¬
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        embeddings = torch.zeros(
            batch_size, seq_len, self.hidden_size,
            dtype=torch.bfloat16, device=device
        )
        
        # å¤„ç† padding (-1 â†’ 0)
        input_ids_safe = input_ids.clone()
        input_ids_safe[input_ids == -1] = 0
        
        # æŒ‰é—´éš”æŸ¥è¡¨ (Interval Slicing)
        codebooks = self.get_codebooks()
        
        for level in range(self.code_length):
            codes_at_level = input_ids_safe[:, level::self.code_length]
            
            if level < self.num_rqvae_layers:
                # å‰ 3 å±‚ï¼šä»å¯¹åº” codebook embedding æŸ¥
                raw_embeds = codebooks[level](codes_at_level)
            else:
                # â­ ç¬¬ 4 å±‚ (suffix)ï¼šç”¨ç‹¬ç«‹çš„ suffix_embedding
                raw_embeds = self.suffix_embedding(codes_at_level)
            
            proj_embeds = self.scid_projector(raw_embeds.to(self.scid_projector.weight.dtype))
            embeddings[:, level::self.code_length] = proj_embeds
        
        # Padding ä½ç½®ç½®é›¶
        padding_mask = ~attention_mask.bool()
        embeddings[padding_mask] = 0
        
        return embeddings
    
    def forward(self, input_ids, attention_mask, seq_end_positions, 
                target_positions, labels=None, targets=None):
        """
        Args:
            input_ids: [B, total_len] - å†å² + ç›®æ ‡çš„ codes
            attention_mask: [B, total_len]
            seq_end_positions: [B] - å†å²åºåˆ—æœ€åä¸€ä¸ª token çš„ä½ç½®ç´¢å¼•
            target_positions: [B, code_length] - ç›®æ ‡ code å„ä½ç½®çš„ç´¢å¼•
            labels: [B, code_length] - ç›®æ ‡ item çš„çœŸå® code
            targets: [B] - ç›®æ ‡ item ID
        """
        # 1. è·å–è¾“å…¥åµŒå…¥
        inputs_embeds = self.get_input_embeddings(input_ids, attention_mask)
        
        if self.training:
            inputs_embeds = inputs_embeds.requires_grad_(True)
        
        # 2. LLaMA Forward
        outputs = self.llama(
            inputs_embeds=inputs_embeds.to(torch.bfloat16),
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        )
        
        hidden_states = outputs.hidden_states[-1]  # [B, L, 4096]
        batch_size = hidden_states.size(0)
        device = hidden_states.device
        batch_indices = torch.arange(batch_size, device=device)
        
        # 3. SIA/PSA: Last Token
        last_hidden = hidden_states[batch_indices, seq_end_positions]  # [B, 4096]
        
        seq_project_latents = self.enc_adapter(last_hidden)  # [B, 128] for SIA
        dec_latents = self.dec_adapter(last_hidden)  # [B, 256] for PSA
        
        # 4. ç”Ÿæˆ Logits: Weight Tying (ç‚¹ç§¯ Codebook)
        code_logits = []
        codebooks = self.get_codebooks()
        
        for i in range(self.code_length):
            pos_i = target_positions[:, i]
            hidden_at_pos = hidden_states[batch_indices, pos_i]
            
            # Step 1: æŠ•å½±å› Codebook ç»´åº¦
            query_emb = self.output_projector(hidden_at_pos)
            
            # Step 2: ä¸å¯¹åº”å±‚çš„æƒé‡åšç‚¹ç§¯
            if i < self.num_rqvae_layers:
                codebook_weight = codebooks[i].weight.t()
            else:
                # â­ ç¬¬ 4 å±‚: suffix_embedding
                codebook_weight = self.suffix_embedding.weight.t()
            
            # Step 3: è®¡ç®—ç›¸ä¼¼åº¦ logits
            logits = torch.matmul(query_emb, codebook_weight.to(query_emb.dtype))
            code_logits.append(logits)
        
        code_logits = torch.stack(code_logits, dim=1)  # [B, code_length, code_num]
        
        return QuantizeOutput(
            logits=code_logits,
            seq_latents=last_hidden,
            seq_project_latents=seq_project_latents,
            dec_latents=dec_latents
        )
    
    @torch.no_grad()
    def generate(self, input_ids, attention_mask, num_return_sequences=10):
        """
        è‡ªå›å½’ç”Ÿæˆç›®æ ‡ item çš„ codes
        ä½¿ç”¨ Beam Search + åˆ†æ‰¹forwardï¼Œæ¯æ­¥ç”¨ output_projector + Codebook ç‚¹ç§¯
        
        â­ ä¼˜åŒ–ï¼šæ¯æ­¥forwardéƒ½åˆ†æ‰¹å¤„ç†ï¼Œé¿å…OOM
        """
        batch_size = input_ids.size(0)
        device = input_ids.device
        codebooks = self.get_codebooks()
        num_beams = self.num_beams
        
        # åˆ†æ‰¹forwardçš„chunkå¤§å°ï¼ˆæ¯æ¬¡æœ€å¤šforwardå¤šå°‘ä¸ªåºåˆ—ï¼‰
        chunk_size = 4  # å¯æ ¹æ®æ˜¾å­˜è°ƒæ•´
        
        # Beam Search åˆå§‹åŒ–
        input_ids_expanded = input_ids.repeat_interleave(num_beams, dim=0)
        attention_mask_expanded = attention_mask.repeat_interleave(num_beams, dim=0)
        
        beam_scores = torch.zeros(batch_size, num_beams, device=device)
        beam_scores[:, 1:] = -1e9
        beam_scores = beam_scores.view(-1)
        
        generated_codes = []
        current_embeds = self.get_input_embeddings(input_ids_expanded, attention_mask_expanded)
        beam_idx_offset = torch.arange(batch_size, device=device).repeat_interleave(num_beams) * num_beams
        
        for code_idx in range(self.code_length):
            # === â­ åˆ†æ‰¹forwardé¿å…OOM ===
            total_seqs = current_embeds.size(0)
            all_hidden_states = []
            
            for chunk_start in range(0, total_seqs, chunk_size):
                chunk_end = min(chunk_start + chunk_size, total_seqs)
                chunk_embeds = current_embeds[chunk_start:chunk_end]
                chunk_mask = attention_mask_expanded[chunk_start:chunk_end]
                
                outputs = self.llama(
                    inputs_embeds=chunk_embeds.to(torch.bfloat16),
                    attention_mask=chunk_mask,
                    use_cache=False,  # ä¸ç”¨KV Cacheï¼Œç®€åŒ–é€»è¾‘
                    output_hidden_states=True,
                    return_dict=True
                )
                
                all_hidden_states.append(outputs.hidden_states[-1][:, -1, :])
                del outputs
                torch.cuda.empty_cache()
            
            # åˆå¹¶ç»“æœ
            last_hidden = torch.cat(all_hidden_states, dim=0)
            del all_hidden_states
            
            # æŠ•å½± + ç‚¹ç§¯ Codebook
            last_hidden = last_hidden.to(self.output_projector.weight.dtype)
            query_emb = self.output_projector(last_hidden)
            if code_idx < self.num_rqvae_layers:
                codebook_weight = codebooks[code_idx].weight.t()
            else:
                codebook_weight = self.suffix_embedding.weight.t()
            logits = torch.matmul(query_emb, codebook_weight.to(query_emb.dtype))
            
            # Beam Search æ›´æ–°
            log_probs = F.log_softmax(logits, dim=-1)
            next_scores = log_probs + beam_scores.unsqueeze(-1)
            
            vocab_size = log_probs.size(-1)
            next_scores = next_scores.view(batch_size, num_beams * vocab_size)
            next_scores, next_tokens = torch.topk(next_scores, num_beams, dim=-1)
            
            next_indices = torch.div(next_tokens, vocab_size, rounding_mode='floor')
            next_codes = next_tokens % vocab_size
            
            beam_scores = next_scores.view(-1)
            generated_codes.append(next_codes)
            
            # å‡†å¤‡ä¸‹ä¸€æ­¥çš„ embedding
            beam_idx = (next_indices + beam_idx_offset.view(batch_size, num_beams)).view(-1)
            current_embeds = current_embeds[beam_idx]
            attention_mask_expanded = attention_mask_expanded[beam_idx]
            
            # æ·»åŠ æ–°ç”Ÿæˆçš„ code embedding
            next_codes_flat = next_codes.view(-1)
            if code_idx < self.num_rqvae_layers:
                next_embeds = codebooks[code_idx](next_codes_flat)
            else:
                next_embeds = self.suffix_embedding(next_codes_flat)
            next_embeds = self.scid_projector(next_embeds.to(self.scid_projector.weight.dtype))
            next_embeds = next_embeds.unsqueeze(1)
            
            current_embeds = torch.cat([current_embeds, next_embeds], dim=1)
            attention_mask_expanded = torch.cat([
                attention_mask_expanded,
                torch.ones(attention_mask_expanded.size(0), 1, device=device, dtype=attention_mask_expanded.dtype)
            ], dim=1)
        
        # æ•´ç†è¾“å‡º
        generated_codes = torch.stack(generated_codes, dim=-1)  # [B, beams, code_length]
        return generated_codes[:, :num_return_sequences, :]
```

### 3.2 DataLoader è®¾è®¡ (data_llama.py)

```python
class LlamaRecDataset(Dataset):
    """
    LLaMA æ¨èæ•°æ®é›†
    å°†åŸå§‹çš„ item ID åºåˆ—è½¬æ¢ä¸º codes åºåˆ—ï¼Œå¹¶è®¡ç®—å…³é”®ä½ç½®ç´¢å¼•
    """
    
    def __init__(self, inter_seq: List[List[int]], all_item_code: torch.Tensor,
                 code_length: int = 4, max_seq_len: int = 50):
        """
        Args:
            inter_seq: äº¤äº’åºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ [item_id1, item_id2, ..., target_id]
            all_item_code: [n_items+1, code_length] æ‰€æœ‰ item çš„ code è¡¨
            code_length: æ¯ä¸ª item çš„ code é•¿åº¦ (é»˜è®¤ 4)
            max_seq_len: æœ€å¤§å†å²åºåˆ—é•¿åº¦ (item æ•°é‡ï¼Œä¸æ˜¯ token æ•°é‡)
        """
        self.all_item_code = all_item_code
        self.code_length = code_length
        self.max_seq_len = max_seq_len
        self.data = self._preprocess(inter_seq)
    
    def _preprocess(self, inter_seq: List[List[int]]) -> List[Dict]:
        """é¢„å¤„ç†ï¼šåˆ†ç¦»å†å²å’Œç›®æ ‡"""
        data = []
        for seq in inter_seq:
            target = seq[-1]
            history = seq[:-1]
            data.append({'history': history, 'target': target})
        return data
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        item = self.data[idx]
        history = item['history'][-self.max_seq_len:]
        target_item = item['target']
        
        # æ„é€  codes
        history_codes = []
        for item_id in history:
            item_codes = self.all_item_code[item_id].tolist()
            history_codes.extend(item_codes)
        
        target_codes = self.all_item_code[target_item].tolist()
        input_ids = history_codes + target_codes
        
        # è®¡ç®—å…³é”®ä½ç½®
        seq_end_position = len(history_codes) - 1
        target_positions = list(range(len(history_codes), len(history_codes) + self.code_length))
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.ones(len(input_ids), dtype=torch.long),
            'seq_end_position': seq_end_position,
            'target_positions': torch.tensor(target_positions, dtype=torch.long),
            'labels': torch.tensor(target_codes, dtype=torch.long),
            'target_item': target_item,
        }


def collate_fn_llama(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    """åŠ¨æ€ Padding (å·¦ Paddingï¼ŒLLaMA ä¹ æƒ¯)"""
    max_len = max(len(b['input_ids']) for b in batch)
    
    input_ids = []
    attention_mask = []
    seq_end_positions = []
    target_positions = []
    labels = []
    targets = []
    
    for b in batch:
        pad_len = max_len - len(b['input_ids'])
        # â­ å·¦ Padding
        input_ids.append(F.pad(b['input_ids'], (pad_len, 0), value=-1))
        attention_mask.append(F.pad(b['attention_mask'], (pad_len, 0), value=0))
        
        # ä½ç½®ç´¢å¼•éœ€è¦åŠ ä¸Š padding åç§»
        seq_end_positions.append(b['seq_end_position'] + pad_len)
        target_positions.append(b['target_positions'] + pad_len)
        
        labels.append(b['labels'])
        targets.append(b['target_item'])
    
    return {
        'input_ids': torch.stack(input_ids),
        'attention_mask': torch.stack(attention_mask),
        'seq_end_positions': torch.tensor(seq_end_positions, dtype=torch.long),
        'target_positions': torch.stack(target_positions),
        'labels': torch.stack(labels),
        'targets': torch.tensor(targets, dtype=torch.long),
    }


class LlamaCollator:
    """Collator ç±»å°è£…ï¼Œæ–¹ä¾¿ä¼ å…¥ DataLoader"""
    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        return collate_fn_llama(batch)
```

### 3.3 Trainer æ ¸å¿ƒé€»è¾‘ (trainer_llama.py)

```python
class LlamaTrainer:
    def __init__(self, config, model_rec, model_id, accelerator,
                 train_data=None, valid_data=None, test_data=None):
        # ... é…ç½®åˆå§‹åŒ– ...
        
        # === â­ ä¼˜åŒ–å™¨ï¼šLLaMA ä½¿ç”¨ 8-bit Adam èŠ‚çœæ˜¾å­˜ ===
        self.rec_optimizer = self._build_optimizer(model_rec, self.lr_rec, self.weight_decay, use_8bit=True)
        self.id_optimizer = self._build_optimizer(model_id, self.lr_id, self.weight_decay, use_8bit=False)
        
        # ... Accelerate å‡†å¤‡ ...
    
    def _build_optimizer(self, model, lr, weight_decay, use_8bit=False):
        """æ„å»ºä¼˜åŒ–å™¨"""
        # åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼Œé¿å…ä¸ºå†»ç»“çš„ 7B å‚æ•°åˆ†é… optimizer state
        params = [p for p in model.parameters() if p.requires_grad]
        
        if use_8bit:
            # â­ 8-bit AdamW: æ˜¾å­˜å‡åŠï¼Œç”¨äº LLaMA ç­‰å¤§æ¨¡å‹
            import bitsandbytes as bnb
            optimizer = bnb.optim.AdamW8bit(params, lr=lr, weight_decay=weight_decay)
        else:
            optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)
        
        return optimizer
    
    # === â­ Codebook åŒæ­¥ (DDP å…¼å®¹) ===
    
    def _sync_codebook_to_model_id(self):
        """model_rec.codebook_embeddings â†’ model_id.rq.vq_layers"""
        with torch.no_grad():
            rec_model = self.model_rec.module if dist.is_initialized() else self.model_rec
            id_model = self.model_id.module if dist.is_initialized() else self.model_id
            
            for i, vq_layer in enumerate(id_model.rq.vq_layers):
                vq_layer.embedding.weight.data.copy_(rec_model.codebook_embeddings[i].weight.data)
    
    def _sync_codebook_to_model_rec(self):
        """model_id.rq.vq_layers â†’ model_rec.codebook_embeddings"""
        with torch.no_grad():
            rec_model = self.model_rec.module if dist.is_initialized() else self.model_rec
            id_model = self.model_id.module if dist.is_initialized() else self.model_id
            
            for i, vq_layer in enumerate(id_model.rq.vq_layers):
                rec_model.codebook_embeddings[i].weight.data.copy_(vq_layer.embedding.weight.data)
    
    def _train_epoch_rec(self, epoch_idx, loss_w, verbose=True):
        """è®­ç»ƒæ¨èå™¨ (å†»ç»“ Tokenizer)"""
        self.model_rec.train()
        self.model_id.eval()
        
        for batch in train_loader:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            seq_end_positions = batch['seq_end_positions'].to(self.device)
            target_positions = batch['target_positions'].to(self.device)
            labels = batch['labels'].to(self.device)
            targets = batch['targets'].to(self.device)
            
            # ç›®æ ‡ item çš„è¯­ä¹‰ embedding
            if dist.is_initialized():
                target_semantic_embs = self.model_rec.module.semantic_embedding(targets)
            else:
                target_semantic_embs = self.model_rec.semantic_embedding(targets)
            
            # â­ model_id åœ¨ train_rec é˜¶æ®µè¢«å†»ç»“ï¼Œä½¿ç”¨ no_grad + ç›´æ¥è®¿é—® module
            with torch.no_grad():
                if dist.is_initialized():
                    target_recon_embs, _, _, _, target_code_logits = self.model_id.module(target_semantic_embs)
                else:
                    target_recon_embs, _, _, _, target_code_logits = self.model_id(target_semantic_embs)
            
            # Forward
            outputs = self.model_rec(
                input_ids=input_ids,
                attention_mask=attention_mask,
                seq_end_positions=seq_end_positions,
                target_positions=target_positions,
            )
            
            # === Loss è®¡ç®— ===
            
            # 1. Code Loss (ç”Ÿæˆä»»åŠ¡) - æ¢¯åº¦ä¼šæµå‘ Codebook!
            code_loss = F.cross_entropy(
                outputs.logits.view(-1, self.code_num),
                labels.view(-1)
            )
            
            # 2. SIA Loss (KL æ•£åº¦)
            if dist.is_initialized():
                _, _, _, _, seq_code_logits = self.model_id.module.rq(outputs.seq_project_latents)
            else:
                _, _, _, _, seq_code_logits = self.model_id.rq(outputs.seq_project_latents)
            
            kl_loss = (
                self.compute_discrete_contrastive_loss_kl(seq_code_logits, target_code_logits) +
                self.compute_discrete_contrastive_loss_kl(target_code_logits, seq_code_logits)
            )
            
            # 3. PSA Loss (InfoNCE)
            dec_cl_loss = (
                self.compute_contrastive_loss(target_recon_embs, outputs.dec_latents) +
                self.compute_contrastive_loss(outputs.dec_latents, target_recon_embs)
            )
            
            # æ€» Loss
            loss = (loss_w['code_loss'] * code_loss + 
                    loss_w['kl_loss'] * kl_loss + 
                    loss_w['dec_cl_loss'] * dec_cl_loss)
            
            self.accelerator.backward(loss)
            self.accelerator.clip_grad_norm_(self.model_rec.parameters(), 1.0)  # â­ æ¢¯åº¦è£å‰ª
            self.rec_optimizer.step()
            self.rec_lr_scheduler.step()
        
        # â­ åŒæ­¥ codebook: model_rec â†’ model_id
        self._sync_codebook_to_model_id()
    
    def _train_epoch_id(self, epoch_idx, loss_w, verbose=True):
        """è®­ç»ƒ Tokenizer (å†»ç»“ Recommender)"""
        # ... ç±»ä¼¼ _train_epoch_rec ...
        
        # â­ åŒæ­¥ codebook: model_id â†’ model_rec
        self._sync_codebook_to_model_rec()
```

---

## 4. 5090 32GB é…ç½® (llama_instrument2018.yaml)

```yaml
# === æ•°æ®é›†é…ç½® ===
dataset: Instrument2018_5090
seed: 2020
reproducibility: True

# DualSCID: collab + text embedding æ‹¼æ¥ (256 + 768 = 1024)
collab_emb_path: Instrument2018_5090_emb_256.npy
text_emb_path: Instrument2018_5090_sentence-transformer_text_768.npy
normalize: false

# === LLaMA æ¨¡å‹é…ç½® ===
llama_path: models/Llama-2-7b-hf
precision: bf16
gradient_checkpointing: true

# LoRA é…ç½®
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05

# === æ¨èæ¨¡å‹é…ç½® ===
semantic_hidden_size: 1024  # DualSCID = 256 + 768
code_num: 256
code_length: 4  # RQ-VAE 3å±‚ + 1å±‚ suffix
e_dim: 128
num_beams: 20

# === è®­ç»ƒé…ç½® ===
epochs: 50
lr_rec: 0.0001          # LLaMA å­¦ä¹ ç‡è¾ƒä½
lr_id: 0.0001
weight_decay: 0.05

# Loss æƒé‡ (è®­ç»ƒ Tokenizer)
id_vq_loss: 1
id_code_loss: 0
id_kl_loss: 0.0001
id_dec_cl_loss: 0.0003

# Loss æƒé‡ (è®­ç»ƒ Recommender)
rec_vq_loss: 0
rec_code_loss: 1
rec_kl_loss: 0.0001
rec_dec_cl_loss: 0.0003

# è®­ç»ƒå‘¨æœŸ
cycle: 2
sim: cos
warm_epoch: 5

# ä¼˜åŒ–å™¨
learner: AdamW
lr_scheduler_type: cosine
warmup_steps: 500

# Batch é…ç½® (5090 32GB)
batch_size: 2               # æ¯å¡ batch size
gradient_accumulation_steps: 8  # ç­‰æ•ˆ batch_size = 16
eval_batch_size: 4
num_workers: 4

# åºåˆ—é…ç½®
max_seq_len: 50

# è¯„ä¼°é…ç½®
eval_step: 2
early_stop: 10
metrics: recall@1,recall@5,ndcg@5,recall@10,ndcg@10
valid_metric: ndcg@10
```

---

## 5. 5090 è¿ç§»ä¸“ç”¨è¡¥ä¸ (main_llama.py)

```python
# === 5090 è¿ç§»ä¸“ç”¨è¡¥ä¸ ===
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
torch.use_deterministic_algorithms(True, warn_only=True)
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"

# DDP é…ç½®ï¼š
# - find_unused_parameters: LoRA åœºæ™¯ä¸‹éƒ¨åˆ†å‚æ•°å¯èƒ½ä¸å‚ä¸ loss
# - æ³¨æ„ï¼šä¸èƒ½ç”¨ static_graph=Trueï¼Œå› ä¸º train_id/train_rec é˜¶æ®µå›¾ç»“æ„ä¸åŒ
ddp_kwargs = DistributedDataParallelKwargs(
    find_unused_parameters=True
)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])
```

---

## 6. å¯åŠ¨å‰ Checklist

- [x] `scid_projector` (128 â†’ 4096) å·²æ·»åŠ 
- [x] `output_projector` (4096 â†’ 128) å·²æ·»åŠ 
- [x] **ç§»é™¤äº† `code_heads`**ï¼Œæ”¹ç”¨ Codebook ç‚¹ç§¯
- [x] SIA/PSA ä½¿ç”¨ Last Tokenï¼Œç§»é™¤ Mean Pooling
- [x] DataLoader è¿”å› `seq_end_positions` å’Œ `target_positions`
- [x] **ä¸ä½¿ç”¨æ˜¾å¼ SEQ_END token**ï¼Œä»…ç”¨ä½ç½®ç´¢å¼•
- [x] Gradient Checkpointing å·²å¼€å¯ (**use_reentrant=False**)
- [x] æ‰€æœ‰è‡ªå®šä¹‰å±‚ `requires_grad_(True)`
- [x] **Projector åˆå§‹åŒ–** (`std=0.02`)
- [x] **ç‹¬ç«‹ Codebook å‰¯æœ¬** (é¿å… DDP å…±äº«å‚æ•°é—®é¢˜)
- [x] **Codebook åŒæ­¥é€»è¾‘** (_sync_codebook_to_model_id/rec)
- [x] **Suffix Embedding** (ç¬¬4å±‚ç‹¬ç«‹å¤„ç†)
- [x] **8-bit AdamW** (èŠ‚çœæ˜¾å­˜)
- [x] **åˆ†æ‰¹ Generate** (é¿å… OOM)
- [x] **5090 è¡¥ä¸** (ç¦ç”¨ TF32)
- [x] **DDP é…ç½®** (find_unused_parameters=True)

---

## 7. æ¢¯åº¦æ£€æŸ¥è„šæœ¬

```python
def sanity_check(model, batch):
    """æ£€æŸ¥å…³é”®ç»„ä»¶çš„æ¢¯åº¦æµï¼Œç‰¹åˆ«æ˜¯ Codebook æ˜¯å¦æ”¶åˆ°æ¢¯åº¦"""
    model.train()
    
    outputs = model(
        input_ids=batch['input_ids'],
        attention_mask=batch['attention_mask'],
        seq_end_positions=batch['seq_end_positions'],
        target_positions=batch['target_positions'],
    )
    
    loss = F.cross_entropy(
        outputs.logits.view(-1, 256),
        batch['labels'].view(-1)
    )
    loss.backward()
    
    print("=== Gradient Check (v3.3) ===")
    
    # â­ å…³é”®ï¼šCodebook å¿…é¡»æœ‰æ¢¯åº¦ï¼
    codebooks = model.get_codebooks()
    for i, cb in enumerate(codebooks):
        grad = cb.weight.grad
        if grad is None:
            print(f"âŒ Codebook[{i}] grad is None - End-to-End æ–­è£‚!")
        elif grad.abs().sum() == 0:
            print(f"âš ï¸ Codebook[{i}] grad is zero")
        else:
            print(f"âœ… Codebook[{i}] grad norm: {grad.norm():.6f}")
    
    # â­ Suffix Embedding
    suffix_grad = model.suffix_embedding.weight.grad
    if suffix_grad is not None:
        print(f"âœ… suffix_embedding grad norm: {suffix_grad.norm():.6f}")
    else:
        print(f"âŒ suffix_embedding grad is None")
    
    # Projectors
    for name, proj in [("scid_projector", model.scid_projector), 
                       ("output_projector", model.output_projector)]:
        grad = proj.weight.grad
        if grad is not None:
            print(f"âœ… {name} grad norm: {grad.norm():.6f}")
        else:
            print(f"âŒ {name} grad is None")
    
    # Adapters
    enc_grad = list(model.enc_adapter.parameters())[0].grad
    dec_grad = list(model.dec_adapter.parameters())[0].grad
    print(f"âœ… enc_adapter grad: {enc_grad.norm():.6f}" if enc_grad is not None else "âŒ enc_adapter None")
    print(f"âœ… dec_adapter grad: {dec_grad.norm():.6f}" if dec_grad is not None else "âŒ dec_adapter None")
```

---

## 8. v3.3 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           LlamaRecModel v3.3                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  [Input: History Codes]                                                          â”‚
â”‚         â”‚                                                                        â”‚
â”‚         â–¼                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ get_input_embeddings (Interval Slicing) â”‚                                    â”‚
â”‚  â”‚   level 0-2: codebook_embeddings[i]     â”‚                                    â”‚
â”‚  â”‚   level 3:   suffix_embedding (ç‹¬ç«‹)     â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                   â–¼                                                              â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚           â”‚ scid_projectorâ”‚ [128 â†’ 4096]                                        â”‚
â”‚           â”‚   (å¯è®­ç»ƒ)     â”‚                                                     â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚                   â–¼                                                              â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚           â”‚       LLaMA2-7B             â”‚                                       â”‚
â”‚           â”‚  (LoRA + Gradient Ckpt)     â”‚                                       â”‚
â”‚           â”‚  use_reentrant=False        â”‚                                       â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                       â”‚                                                          â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚           â”‚           â”‚           â”‚                                             â”‚
â”‚           â–¼           â–¼           â–¼                                             â”‚
â”‚     [seq_end_pos] [seq_end_pos] [target_pos]                                    â”‚
â”‚           â”‚           â”‚           â”‚                                             â”‚
â”‚           â–¼           â–¼           â–¼                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚    â”‚enc_adapterâ”‚ â”‚dec_adapterâ”‚ â”‚output_projectorâ”‚                                â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚         â”‚            â”‚               â”‚                                           â”‚
â”‚         â–¼            â–¼               â–¼                                           â”‚
â”‚    [B, 128]     [B, semantic]   [B, 128]                                        â”‚
â”‚         â”‚            â”‚               â”‚                                           â”‚
â”‚         â”‚            â”‚               â”‚                                           â”‚
â”‚         â–¼            â–¼               â–¼                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚SIA Loss â”‚ â”‚PSA Loss â”‚    â”‚ MatMul (Weight Tying)       â”‚                   â”‚
â”‚    â”‚(KL Div) â”‚ â”‚(InfoNCE)â”‚    â”‚ level 0-2: codebook[i].T    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ level 3:   suffix_emb.T     â”‚         â”‚        â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚        â”‚
â”‚                                            â”‚                          â”‚        â”‚
â”‚                                            â–¼                          â”‚        â”‚
â”‚                                       [B, 256]                        â”‚        â”‚
â”‚                                       (Logits)                        â”‚        â”‚
â”‚                                            â”‚                          â”‚        â”‚
â”‚                                            â–¼                          â”‚        â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚        â”‚
â”‚                                      â”‚Code Loss â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                      â”‚(CE Loss) â”‚                              â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                                 â”‚
â”‚  â­ ç‹¬ç«‹ codebook_embeddings + Trainer æ‰‹åŠ¨åŒæ­¥                                  â”‚
â”‚  â­ 8-bit AdamW (LLaMA) + æ™®é€š AdamW (RQ-VAE)                                   â”‚
â”‚  â­ åˆ†æ‰¹ Generate (é¿å… OOM)                                                     â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. å®æ–½ä¼˜å…ˆçº§

| Phase | ä»»åŠ¡ | çŠ¶æ€ |
|-------|------|------|
| âœ… P0 | LlamaRecModel å®ç° (å« Weight Tying + Suffix) | å®Œæˆ |
| âœ… P0 | DataLoader é‡æ„ | å®Œæˆ |
| âœ… P1 | Trainer é€‚é… (å« Codebook åŒæ­¥) | å®Œæˆ |
| âœ… P1 | Generate (Beam Search + åˆ†æ‰¹) | å®Œæˆ |
| ğŸŸ¡ P2 | æ¢¯åº¦è°ƒè¯• + å½¢çŠ¶éªŒè¯ | è¿›è¡Œä¸­ |
| ğŸŸ¡ P2 | è¯„ä¼° + è°ƒä¼˜ | è¿›è¡Œä¸­ |

---

## 10. é£é™©ç‚¹ä¸å¤‡é€‰æ–¹æ¡ˆ

| é£é™© | å½±å“ | å¤‡é€‰æ–¹æ¡ˆ |
|------|------|---------|
| æ˜¾å­˜æº¢å‡º | è®­ç»ƒå¤±è´¥ | é™ batchï¼ŒåŠ  grad_accumï¼Œç”¨ DeepSpeed ZeRO |
| Codebook æ¢¯åº¦æ¶ˆå¤± | End-to-End å¤±æ•ˆ | æ£€æŸ¥ output_projector åˆå§‹åŒ–ï¼ŒåŠ æ¢¯åº¦ç›‘æ§ |
| Logits çˆ†ç‚¸/NaN | è®­ç»ƒå´©æºƒ | å‡å°åˆå§‹åŒ– std (0.02â†’0.01)ï¼ŒåŠ  gradient clipping |
| ç”Ÿæˆè´¨é‡å·® | æ¨èæ•ˆæœä¸‹é™ | å¢åŠ  beam_sizeï¼Œæ·»åŠ  prefix constraint |
| DDP å‚æ•°å…±äº«å†²çª | è®­ç»ƒæŠ¥é”™ | âœ… å·²è§£å†³ï¼šç‹¬ç«‹ codebook + æ‰‹åŠ¨åŒæ­¥ |
| Generate OOM | è¯„ä¼°å¤±è´¥ | âœ… å·²è§£å†³ï¼šåˆ†æ‰¹ forward |

---

## 11. å¯åŠ¨å‘½ä»¤

```bash
# å•å¡
python main_llama.py --config ./config/llama_instrument2018.yaml

# å¤šå¡ (Accelerate)
accelerate launch --config_file accelerate_config_llama.yaml main_llama.py --config ./config/llama_instrument2018.yaml

# è°ƒè¯•æ¨¡å¼ (è·³è¿‡ train_id)
accelerate launch main_llama.py --config ./config/llama_instrument2018.yaml --skip_id
```
