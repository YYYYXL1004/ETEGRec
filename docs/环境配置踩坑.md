1. 使用`conda install -c pytorch -c nvidia faiss-gpu`而不是pip，因为numpy版本会冲突，需要2.0以下版本的numpy
   使用conda install 可以自行解决冲突，降级numpy
2. 50系列的必须安装cu12.8版本的pytorch
   `pip3 install torch torchvision`
3. 从源码编译安装 Flash Attention

- 安装编译依赖 (通常系统里可能缺 ninja)
  `pip install packaging ninja`

- 安装匹配的编译器 (在 conda 环境中)
  这会下载一个独立的 nvcc 编译器，版本强制指定为 12.8，只在当前环境生效。

  // 确保你在 ETEGRec 环境下

  `conda install -c nvidia/label/cuda-12.8.0 cuda-nvcc`

- 设置环境变量并安装
  安装完编译器后，指定环境变量 CUDA_HOME 指向 Conda 环境，然后再次安装 flash-attn。

    - 设置 CUDA 路径为当前 Conda 环境
      `export CUDA_HOME=$CONDA_PREFIX`
    - 强制源码编译（跳过 GitHub 下载步骤，解决网络报错）
      `export FLASH_ATTENTION_FORCE_BUILD=TRUE`

- 再次尝试编译安装 (这次应该能通过版本检查了)
  // --no-build-isolation 很重要，保证它使用我们刚修好的环境
  `pip install flash-attn --no-build-isolation`
  预期结果： 这次编译过程会开始跑（可能会跑几分钟，屏幕上会刷很多 Building ...），只要最后出现 Successfully installed flash-attn，就算大功告成。