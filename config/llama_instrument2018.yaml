# ETEGRec LLaMA2-7B 配置文件
# 基于 T5_to_LLaMA2_Migration_Plan v3.2

# === 数据集配置 ===
dataset: Instrument2018_5090
seed: 2020
reproducibility: True
log_dir: ./logs

# 数据路径
data_path: ./RecBole/dataset
map_path: .emb_map.json
semantic_emb_path: Instrument2018_5090_emb_256.npy

# DualSCID: collab + text embedding 拼接 (256 + 768 = 1024)
collab_emb_path: Instrument2018_5090_emb_256.npy
text_emb_path: Instrument2018_5090_sentence-transformer_text_768.npy
normalize: false

# === LLaMA 模型配置 ===
llama_path: models/Llama-2-7b-hf
precision: bf16
gradient_checkpointing: true

# LoRA 配置
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05

# === 推荐模型配置 ===
semantic_hidden_size: 1024  # DualSCID checkpoint 使用 1024
code_num: 256
code_length: 4  # RQ-VAE 3层 + 1层 suffix (处理冲突)
e_dim: 128
num_beams: 20

# === 训练配置 ===
epochs: 50
lr_rec: 0.0001          # LLaMA 学习率较低
lr_id: 0.0001
weight_decay: 0.05

# Loss 权重 (训练 Tokenizer)
id_vq_loss: 1
id_code_loss: 0
id_kl_loss: 0.0001
id_dec_cl_loss: 0.0003

# Loss 权重 (训练 Recommender)
rec_vq_loss: 0
rec_code_loss: 1
rec_kl_loss: 0.0001
rec_dec_cl_loss: 0.0003

# 训练周期
cycle: 2
sim: cos
warm_epoch: 5

# 优化器
learner: AdamW
lr_scheduler_type: cosine
warmup_steps: 500

# Batch 配置 (5090 32GB)
batch_size: 2               # 每卡 batch size
gradient_accumulation_steps: 8  # 等效 batch_size = 16
eval_batch_size: 4
num_workers: 4

# 序列配置
max_seq_len: 50
max_his_len: 50

# 评估配置
eval_step: 2
early_stop: 10
metrics: recall@1,recall@5,ndcg@5,recall@10,ndcg@10
valid_metric: ndcg@10

# === RQ-VAE 配置 ===
rqvae_path: ./RQVAE/rqvae_ckpt/DualSCID/Dec-23-2025_13-49-09/best_collision_0.0056_gini_0.2534.pth
num_emb_list: [256,256,256]
beta: 0.25
layers: [1024, 512, 256]
vq_type: vq
dist: l2
tau: 0.07
kmeans_init: False
kmeans_iters: 100
dropout_prob: 0.0
bn: False
loss_type: mse
alpha: 1

# === 保存路径 ===
save_path: ./myckpt/llama_instrument2018

